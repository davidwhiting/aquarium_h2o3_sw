{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents of this notebook are shamelessly stolen from Megan Kurka.  \n",
    "\n",
    "The goal of this demo is to showcase how easy it is to use H2OXGBoost in Gridsearch just like our native GBM algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(h2o)\n",
    "h2o.init(strict_version_check=F) # do not do this, you are not ME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the titanic dataset and try to predict who lived and who died...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df <- h2o.importFile(path = \"http://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df$survived <- as.factor(df$survived)\n",
    "df$ticket <- as.factor(df$ticket)\n",
    "# Set predictors and response variable\n",
    "response <- \"survived\"\n",
    "predictors <- colnames(df)[!(colnames(df) %in% c(\"survived\", \"name\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data for machine learning\n",
    "splits <- h2o.splitFrame(\n",
    "  data = df, \n",
    "  ratios = c(0.6,0.2),   ## only need to specify 2 fractions, the 3rd is implied\n",
    "  destination_frames = c(\"train.hex\", \"valid.hex\", \"test.hex\"), seed = 1234\n",
    ")\n",
    "train <- splits[[1]]\n",
    "valid <- splits[[2]]\n",
    "test  <- splits[[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build two models: gbm and H2OXGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model <- h2o.glm(x = predictors, y = response, training_frame = train, validation_frame = valid,\n",
    "                     family = \"binomial\", model_id = \"glm_default.hex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model <- h2o.xgboost(x = predictors, y = response, training_frame = train, validation_frame = valid,\n",
    "                     model_id = \"glm_default.hex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the baseline results (us being lazy and only use default parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results <- data.frame('model' = c(\"GBM\", \"XGBoost\"),\n",
    "                               'training auc' = c(h2o.auc(gbm_model, train = T), h2o.auc(xgboost_model, train = T)),\n",
    "                               'validation_auc' = c(h2o.auc(gbm_model, valid = T), h2o.auc(xgboost_model, valid = T)))\n",
    "print(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like H2OXGBoost default gives us a better result.  Let's use gridsearch with early stopping on both models to see if we can improve their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = list( max_depth = seq(1, 25, 2))\n",
    "\n",
    "grid <- h2o.grid(\n",
    "  ## hyper parameters\n",
    "  hyper_params = hyper_params,\n",
    "  \n",
    "  ## full Cartesian hyper-parameter search\n",
    "  search_criteria = list(strategy = \"Cartesian\"),\n",
    "  \n",
    "  ## which algorithm to run\n",
    "  algorithm=\"gbm\",\n",
    "  \n",
    "  ## identifier for the grid, to later retrieve it\n",
    "  grid_id=\"depth_grid\",\n",
    "  \n",
    "  ## standard model parameters\n",
    "  x = predictors, \n",
    "  y = response, \n",
    "  training_frame = train, \n",
    "  validation_frame = valid,\n",
    "  \n",
    "  ## more trees is better if the learning rate is small enough \n",
    "  ## here, use \"more than enough\" trees - we have early stopping\n",
    "  ntrees = 5000,                                                            \n",
    "  \n",
    "  ## smaller learning rate is better\n",
    "  ## since we have learning_rate_annealing, we can afford to start with a bigger learning rate\n",
    "  learn_rate = 0.05,                                                         \n",
    "  \n",
    "  ## learning rate annealing: learning_rate shrinks by 1% after every tree \n",
    "  ## (use 1.00 to disable, but then lower the learning_rate)\n",
    "  learn_rate_annealing = 0.99,                                               \n",
    "  \n",
    "  ## sample 80% of rows per tree\n",
    "  sample_rate = 0.8,                                                       \n",
    "  \n",
    "  ## sample 80% of columns per split\n",
    "  col_sample_rate = 0.8, \n",
    "  \n",
    "  ## fix a random number generator seed for reproducibility\n",
    "#  seed = 1234,                                                             \n",
    "  \n",
    "  ## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events\n",
    "  stopping_rounds = 5,\n",
    "  stopping_tolerance = 0.001,\n",
    "  stopping_metric = \"AUC\", \n",
    "  \n",
    "  ## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)\n",
    "  score_tree_interval = 10                                                \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid                                                                       \n",
    "\n",
    "## sort the grid models by decreasing AUC\n",
    "sorted_grid <- h2o.getGrid(\"depth_grid\", sort_by=\"auc\", decreasing = TRUE)    \n",
    "sorted_grid@summary_table[c(1:5), ]\n",
    "\n",
    "## find the range of max_depth for the top 5 models\n",
    "topDepths = sorted_grid@summary_table$max_depth[1:5]                       \n",
    "minDepth = min(as.numeric(topDepths))\n",
    "maxDepth = max(as.numeric(topDepths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params_xgboost = list(max_depth = seq(1, 25, 2), ntrees=5000, learn_rate=c(1,0.1,0.01,0.001))\n",
    "xgboost_grid <- h2o.grid(\n",
    "  ## hyper parameters\n",
    "  hyper_params = hyper_params_xgboost,\n",
    "  \n",
    "  ## full Cartesian hyper-parameter search\n",
    "  search_criteria = list(strategy = \"Cartesian\"),\n",
    "  \n",
    "  ## which algorithm to run\n",
    "  algorithm=\"xgboost\",\n",
    "  \n",
    "  ## identifier for the grid, to later retrieve it\n",
    "  grid_id=\"depth_grid_xgboost\",\n",
    "  \n",
    "  ## standard model parameters\n",
    "  x = predictors, \n",
    "  y = response, \n",
    "  training_frame = train, \n",
    "  validation_frame = valid,                                                          \n",
    "    \n",
    "  ## learning rate annealing: learning_rate shrinks by 1% after every tree \n",
    "  ## (use 1.00 to disable, but then lower the learning_rate)\n",
    " # learn_rate_annealing = 0.99,                                               \n",
    "  \n",
    "  ## sample 80% of rows per tree\n",
    "  sample_rate = 0.8,                                                       \n",
    "  \n",
    "  ## sample 80% of columns per split\n",
    "  col_sample_rate = 0.8, \n",
    "  \n",
    "  ## fix a random number generator seed for reproducibility\n",
    "#  seed = 1234,                                                             \n",
    "  \n",
    "  ## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events\n",
    "  stopping_rounds = 5,\n",
    "  stopping_tolerance = 0.001,\n",
    "  stopping_metric = \"AUC\", \n",
    "  \n",
    "  ## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)\n",
    "  score_tree_interval = 10                                                \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_grid                                                                       \n",
    "\n",
    "## sort the grid models by decreasing AUC\n",
    "sorted_xgboost_grid  <- h2o.getGrid(\"depth_grid_xgboost\", sort_by=\"auc\", decreasing = TRUE)    \n",
    "sorted_xgboost_grid@summary_table[c(1:5), ]\n",
    "\n",
    "## find the range of max_depth for the top 5 models\n",
    "topDepthsXGboost = sorted_xgboost_grid@summary_table$max_depth[1:5]                       \n",
    "minDepth = min(as.numeric(topDepthsXGboost))\n",
    "maxDepth = max(as.numeric(topDepthsXGboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid                                                                       \n",
    "\n",
    "## sort the grid models by decreasing AUC\n",
    "sorted_grid <- h2o.getGrid(\"depth_grid\", sort_by=\"auc\", decreasing = TRUE)    \n",
    "sorted_grid@summary_table[c(1:5), ]\n",
    "\n",
    "## find the range of max_depth for the top 5 models\n",
    "topDepths = sorted_grid@summary_table$max_depth[1:5]                       \n",
    "minDepth = min(as.numeric(topDepths))\n",
    "maxDepth = max(as.numeric(topDepths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With gridsearch, we were able to narrow the performance gap between GBM and H2OXGBoost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
