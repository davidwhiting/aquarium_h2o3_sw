{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML with Feature Engineering: A Lending Club Analysis\n",
    "\n",
    "In this tutorial, we will go through a step-by-step workflow to build models using H2O's AutoML to determine loan deliquency. The data for this exercise come from the public Lending Club data set, a description can be found [here](https://www.kaggle.com/pragyanbo/a-hitchhiker-s-guide-to-lending-club-loan-data/notebook).\n",
    "\n",
    "We will highlight feature engineering methods to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "1. Start H2O-3 cluster\n",
    "2. Import data\n",
    "3. Train default models using H2O's AutoML\n",
    "4. Feature engineering to improve model performance\n",
    "5. Train models using transformed data with AutoML\n",
    "6. (Optional) Assignment: Transform test data\n",
    "7. Stop H2O-3 cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 (of 7). Start H2O Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "h2o.init(max_mem_size = \"6g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 (of 7). Import Data\n",
    "\n",
    "This exploration of H2O will use a version of the Lending Club Loan Data that can be found on [Kaggle](https://www.kaggle.com/wendykan/lending-club-loan-data). This data consists of 15 variables:\n",
    "\n",
    "|  Id   | Column Name | Description |\n",
    "| --- | ----------- | ----------- |\n",
    "|   1 | loan_amnt   | Requested loan amount (US dollars) |\n",
    "|   2 | term        | Loan term length (months) |\n",
    "|   3 | int_rate    | Recommended interest rate |\n",
    "|   4 | emp_length  | Employment length (years) |\n",
    "|   5 | home_ownership| Housing status |\n",
    "|   6 | annual_inc  | Annual income (US dollars) |\n",
    "|   7 | purpose     | Purpose for the loan |\n",
    "|   8 | addr_state  | State of residence |\n",
    "|   9 | dti         | Debt to income ratio |\n",
    "|  10 | delinq_2yrs | Number of delinquencies in the past 2 years |\n",
    "|  11 | revol_util  | Percent of revolving credit line utilized |\n",
    "|  12 | total_acc   | Number of active accounts |\n",
    "|  13 | bad_loan    | Bad loan indicator |\n",
    "|  14 | longest_credit_length | Age of oldest active account |\n",
    "|  15 | verification_status | Income verification status |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://s3-us-west-2.amazonaws.com/h2o-tutorials/data/topics/data/automl/loan.csv\n",
    "loans = h2o.import_file(\"../../data/automl/loan.csv\")\n",
    "loans[\"bad_loan\"] = loans[\"bad_loan\"].asfactor()\n",
    "\n",
    "loans.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 (of 7). Train Default Models with AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by splitting the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = loans.split_frame(seed=1234, ratios=[0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and train 6 models excluding the `DeepLearning` algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target and predictor variables\n",
    "y = \"bad_loan\"\n",
    "x = train.col_names\n",
    "x.remove(y)\n",
    "x.remove(\"int_rate\")\n",
    "\n",
    "# Use Auto ML to train models\n",
    "from h2o.automl import H2OAutoML\n",
    "aml = H2OAutoML(max_models = 6, exclude_algos = ['DeepLearning'])\n",
    "aml.train(x = x, y = y, training_frame = train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out a leaderboard with the 6 best models and two ensembles: a stacked ensemble of all models, and a best-of-family stacked ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aml.leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is frequently the case, the stacked ensemble models are among the best performers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Top Model\n",
    "\n",
    "The `aml.leader` object contains the top model from the leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = aml.leader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details for both training and cross-validated data are contained in `aml.leader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml.leader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get details for the holdout test data, use `model_performance` as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perf = aml.leader.model_performance(test_data=test)\n",
    "perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at overall performance of the top model by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC:     train = {:.4f}, xval = {:.4f}, test = {:.4f}\" \\\n",
    "      .format(aml.leader.auc(train = True), aml.leader.auc(xval = True), perf.auc()))\n",
    "print(\"Logloss: train = {:.4f}, xval = {:.4f}, test = {:.4f}\" \\\n",
    "      .format(aml.leader.logloss(train = True), aml.leader.logloss(xval = True), perf.logloss()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the cross-validation and test data metrics are very similar, but there is a gap between training and test metrics. Those for training are overly optimistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine a Selected Model\n",
    "\n",
    "In practice, the top performing model ranked by a leaderboard may not necessarily be the model you want to put into production. Other considerations, such as modeling type, regulatory or internal business preferences, likelihood of model approval, etc., may play a role in determining which model to use.  \n",
    "\n",
    "Here we demonstrate how to select and evaluate any model from the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aml.leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select an specified model from the leaderboard by indicating its position. The following code will select the best model that is not a stacked ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = aml.leaderboard['model_id'].grep(\"StackedEnsemble\", invert=True).min()\n",
    "select_model = h2o.get_model(aml.leaderboard[int(m),'model_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at performance with the selected model just as we did with the top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "select_perf = select_model.model_performance(test_data=test)\n",
    "select_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary is then given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC:     train = {:.4f}, xval = {:.4f}, test = {:.4f}\" \\\n",
    "      .format(select_model.auc(train = True), select_model.auc(xval = True), select_perf.auc()))\n",
    "print(\"Logloss: train = {:.4f}, xval = {:.4f}, test = {:.4f}\" \\\n",
    "      .format(select_model.logloss(train = True), select_model.logloss(xval = True), select_perf.logloss()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next look at variable importance plots for our selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "select_model.varimp_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `addr_state`, a high-cardinality categorical variable, is the most important variable. We will next try some feature engineering to see whether we can improve this model's overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 (of 7). Feature Engineering to Improve Model Performance\n",
    "\n",
    "The goal of this section is to improve upon these predictors through a number of feature engineering steps. In particular, we will perform four feature engineering tasks on select variables:\n",
    "\n",
    "**Creating Target Encoding** \n",
    "   - Loan Purpose\n",
    "   - State of Residence\n",
    "\n",
    "**Separating Typical from Extreme**\n",
    "   - Loan Amount\n",
    "   - Annual Income\n",
    "   - Debt to Income Ratio\n",
    "   - Number of Delinquencies in the Past 2 Years\n",
    "   - Revolving Credit Line Utilized\n",
    "   - Number of Credit Lines\n",
    "   - Longest Credit Length\n",
    "\n",
    "**Creating Indicator Functions**\n",
    "   - Term\n",
    "   - Income Verification Status\n",
    "   - Employment Length (Missing Value)\n",
    "\n",
    "**Combining Categories** \n",
    "   - Home Ownership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Target Encoding\n",
    "\n",
    "### Cross Validation with Target Encoding\n",
    "\n",
    "Some of the engineered features will use [cross-validated mean target encoding](https://github.com/h2oai/h2o-tutorials/blob/master/best-practices/categorical-predictors/target_encoding.md) of categorical predictors since one-hot encodings can lead to overfitting of infrequent categories.\n",
    "\n",
    "To achieve this goal, we will first create soft partitions using H2OFrame's [`kfold_column`](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/frame.html?highlight=kfold_column#h2o.frame.H2OFrame.kfold_column) function, then calculate summary statistics using H2O's [`group_by`](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/frame.html#groupby) function, and finally join these engineered features using H2OFrame's [`merge`](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/frame.html?highlight=merge#h2o.frame.H2OFrame.merge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"bad_loan\"\n",
    "x_orig = train.col_names\n",
    "x_orig.remove(y)\n",
    "x_orig.remove(\"int_rate\")\n",
    "\n",
    "x_trans = x_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_nfolds = 5\n",
    "cv_seed = 1234\n",
    "train[\"cv_fold\"] = train.kfold_column(n_folds = cv_nfolds, seed = cv_seed)\n",
    "\n",
    "train[\"cv_fold\"].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p):\n",
    "    return np.log(p) - np.log(1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_target(data, x, y = \"bad_loan\"):\n",
    "    grouped_data = data[[x, y]].group_by([x])\n",
    "    stats = grouped_data.count(na = \"ignore\").mean(na = \"ignore\")\n",
    "    return stats.get_frame().as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_target_encoding(data, x, y = \"bad_loan\", fold_column = \"cv_fold\", prior_mean = 0.183, prior_count = 1):\n",
    "    \"\"\"\n",
    "    Creates target encoding for binary target\n",
    "    data (H2OFrame) : data set\n",
    "    x (string) : categorical predictor column name\n",
    "    y (string) : binary target column name\n",
    "    fold_column (string) : cross-validation fold column name\n",
    "    prior_mean (float) : proportion of 1s in the target column\n",
    "    prior_count (positive number) : weight to give to prior_mean\n",
    "    \"\"\" \n",
    "    grouped_data = data[[x, fold_column, y]].group_by([x, fold_column])\n",
    "    grouped_data.sum(na = \"ignore\").count(na = \"ignore\")\n",
    "    df = grouped_data.get_frame().as_data_frame()\n",
    "    df_list = []\n",
    "    nfold = int(data[fold_column].max()) + 1\n",
    "    for j in range(0, nfold):\n",
    "        te_x = \"te_{}\".format(x)\n",
    "        sum_y = \"sum_{}\".format(y)\n",
    "        oof = df.loc[df[fold_column] != j, [x, sum_y, \"nrow\"]]\n",
    "        stats = oof.groupby([x]).sum()\n",
    "        stats[x] = stats.index\n",
    "        stats[fold_column] = j\n",
    "        p = (stats[sum_y] + (prior_count * prior_mean)) / (stats[\"nrow\"] + prior_count)\n",
    "        stats[te_x] = logit(p)\n",
    "        df_list.append(stats[[x, fold_column, te_x]])\n",
    "    return h2o.H2OFrame(pd.concat(df_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Creating Target Encoding for Loan Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = train[\"purpose\"].table().as_data_frame()\n",
    "tbl[\"Percent\"] = np.round((100 * tbl[\"Count\"]/train.nrows), 2)\n",
    "tbl = tbl.sort_values(by = \"Count\", ascending = 0)\n",
    "tbl = tbl.reset_index(drop = True)\n",
    "print(tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there is a high concentration of loans for debt consolidation (56.87%), a sizable number for credit card (18.78%), and the remaining 24.35% loans are spread amongst 12 other purposes, we will use mean target encoding to avoid overfitting the later group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = mean_target(train, \"purpose\")\n",
    "\n",
    "df = df.sort_values(by = \"mean_bad_loan\", ascending = 0)\n",
    "df = df.reset_index(drop = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation = 90)\n",
    "plt.yscale(\"logit\")\n",
    "plt.plot(df[\"purpose\"], df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_purpose = mean_target_encoding(train, \"purpose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(te_purpose, all_x = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace `purpose` with target encoded `te_purpose`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"purpose\")\n",
    "x_trans.append(\"te_purpose\")\n",
    "\n",
    "train[\"te_purpose\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Target Encoding for State of Residence\n",
    "\n",
    "We will also use a mean target encoding for *state of residence* for a reason similar to that for *purpose*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = train[\"addr_state\"].table().as_data_frame()\n",
    "tbl[\"Percent\"] = np.round((100 * tbl[\"Count\"]/train.nrows), 2)\n",
    "tbl = tbl.sort_values(by = \"Count\", ascending = 0)\n",
    "tbl = tbl.reset_index(drop = True)\n",
    "print(tbl[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = mean_target(train, \"addr_state\")\n",
    "\n",
    "df = df.sort_values(by = \"mean_bad_loan\", ascending = 0)\n",
    "df = df.reset_index(drop = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"logit\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.plot(df[\"addr_state\"], df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_addr_state = mean_target_encoding(train, \"addr_state\", prior_count = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = te_addr_state[[\"addr_state\", \"te_addr_state\"]].group_by([\"addr_state\"])\n",
    "df = grouped_data.count(na = \"ignore\").mean(na = \"ignore\").get_frame().as_data_frame()\n",
    "\n",
    "df = df.sort_values(by = \"mean_te_addr_state\", ascending = 0)\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "plt.xticks(rotation = 90)\n",
    "plt.plot(df[\"addr_state\"], df[\"mean_te_addr_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(te_addr_state, all_x = True)\n",
    "\n",
    "x_trans.remove(\"addr_state\")\n",
    "x_trans.append(\"te_addr_state\")\n",
    "\n",
    "train[\"te_addr_state\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Separating Typical from Extreme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Separating Typical from Extreme Loan Amount\n",
    "\n",
    "After binning `loan_amt` using H2OFrame's [`cut`](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/frame.html?highlight=cut#h2o.frame.H2OFrame.cut) function and looking at the fraction of bad loans on a logit scale, we see that the chance of a bad loan roughly increases linearly in loan amount from \\\\$5,000 to \\\\$30,000 and is relatively flat below \\\\$5,000 and above \\\\$30,000. To reflect this finding in the modeling, we will replace the original `loan_amnt` measure with two derived measures:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\text{loan_amnt_core} & = & \\max(5000, \\min(\\text{loan_amnt}, 30000)) \\\\\n",
    "\\text{loan_amnt_diff} & = & \\text{loan_amnt} - \\text{loan_amnt_core}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"loan_amnt\"].quantile([0, 0.05, 0.25, 0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = np.linspace(0, 35000, 8).tolist()\n",
    "train[\"loan_amnt_cat\"] = train[\"loan_amnt\"].cut(breaks = breaks)\n",
    "\n",
    "df = mean_target(train, \"loan_amnt_cat\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation = 90)\n",
    "plt.yscale(\"logit\")\n",
    "plt.plot(df[\"loan_amnt_cat\"], df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"loan_amnt\")\n",
    "x_trans.append(\"loan_amnt_core\")\n",
    "x_trans.append(\"loan_amnt_delta\")\n",
    "\n",
    "train[\"loan_amnt_core\"] = h2o.H2OFrame.ifelse(train[\"loan_amnt\"] <= 5000, 5000, train[\"loan_amnt\"])\n",
    "train[\"loan_amnt_core\"] = h2o.H2OFrame.ifelse(train[\"loan_amnt_core\"] <= 30000, train[\"loan_amnt_core\"], 30000)\n",
    "\n",
    "train[\"loan_amnt_delta\"] = train[\"loan_amnt\"] - train[\"loan_amnt_core\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Separating Typical from Extreme Annual Income\n",
    "\n",
    "Looking at the occurrence of bad loans on a logit scale reveal that the chance of a bad loan roughly decreases linearly in annual income from \\\\$10,000 to \\\\$105,000 and is relatively flat above \\\\$105,000. To reflect this finding in the modeling, we will replace the original `annual_inc` measure with two derived measures:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\text{annual_inc_core} & = & \\max(10000, \\min(\\text{annual_inc}, 105000)) \\\\\n",
    "\\text{annual_inc_diff} & = & \\text{annual_inc} - \\text{annual_inc_core}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"annual_inc\"].quantile([0, 0.05, 0.25, 0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "breaks = np.linspace(0, 150000, 31).tolist()\n",
    "train[\"annual_inc_cat\"] = train[\"annual_inc\"].cut(breaks = breaks)\n",
    "\n",
    "df = mean_target(train, \"annual_inc_cat\")\n",
    "df.drop(df.index[:1], inplace=True) # remove NaN\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"logit\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.plot(df[\"annual_inc_cat\"], df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"annual_inc\")\n",
    "x_trans.append(\"annual_inc_core\")\n",
    "x_trans.append(\"annual_inc_delta\")\n",
    "\n",
    "train[\"annual_inc_core\"] = h2o.H2OFrame.ifelse(train[\"annual_inc\"] <= 10000, 10000, train[\"annual_inc\"])\n",
    "train[\"annual_inc_core\"] = h2o.H2OFrame.ifelse(train[\"annual_inc_core\"] <= 105000,\n",
    "                                               train[\"annual_inc_core\"], 105000)\n",
    "\n",
    "train[\"annual_inc_delta\"] = train[\"annual_inc\"] - train[\"annual_inc_core\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Separating Typical from Extreme Debt to Income Ratio\n",
    "\n",
    "Looking at the occurance of bad loans on a logit scale reveal that the chance of a bad loan roughly increases linearly in debt-to-income from 5% to 30% and is highly volatile outside of that range due to small numbers of observations. To reflect this finding in the modeling, we will replace the original `dti` measure with two derived measures:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\text{dti_core} & = & \\max(5, \\min(\\text{dti}, 30)) \\\\\n",
    "\\text{dti_diff} & = & \\text{dti} - \\text{dti_core}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"dti\"].quantile([0, 0.05, 0.25, 0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = np.linspace(0, 40, 41).tolist()\n",
    "train[\"dti_cat\"] = train[\"dti\"].cut(breaks = breaks)\n",
    "\n",
    "df = mean_target(train, \"dti_cat\")\n",
    "df.drop(df.index[:1], inplace=True) # remove NaN\n",
    "\n",
    "plt.yscale(\"logit\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.plot(df[\"dti_cat\"], df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"dti\")\n",
    "x_trans.append(\"dti_core\")\n",
    "x_trans.append(\"dti_delta\")\n",
    "\n",
    "train[\"dti_core\"] = h2o.H2OFrame.ifelse(train[\"dti\"] <= 5, 5, train[\"dti\"])\n",
    "train[\"dti_core\"] = h2o.H2OFrame.ifelse(train[\"dti_core\"] <= 30, train[\"dti_core\"], 30)\n",
    "\n",
    "train[\"dti_delta\"] = train[\"dti\"] - train[\"dti_core\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Separating Typical from Extreme Number of Delinquencies in the Past 2 Years\n",
    "\n",
    "The chance of a bad loan seems to max out at 3 delinquent payments in the past two years. To reflect this finding in the modeling, we will replace the original `delinq_2yrs` measure with two derived measures:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\text{delinq_2yrs_core} & = & \\min(\\text{delinq_2yrs}, 3) \\\\\n",
    "\\text{delinq_2yrs_diff} & = & \\text{delinq_2yrs} - \\text{delinq_2yrs_core}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"delinq_2yrs\"].quantile([0, 0.05, 0.25, 0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = np.linspace(0, 5, 6).tolist()\n",
    "train[\"delinq_2yrs_cat\"] = train[\"delinq_2yrs\"].cut(breaks = breaks)\n",
    "\n",
    "df = mean_target(train, \"delinq_2yrs_cat\")\n",
    "df.drop(df.index[:1], inplace=True) # remove NaN\n",
    "\n",
    "plt.yscale(\"logit\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.plot(df[\"delinq_2yrs_cat\"], df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"delinq_2yrs\")\n",
    "x_trans.append(\"delinq_2yrs_core\")\n",
    "x_trans.append(\"delinq_2yrs_delta\")\n",
    "\n",
    "train[\"delinq_2yrs_core\"] = h2o.H2OFrame.ifelse(train[\"delinq_2yrs\"] <= 3, train[\"delinq_2yrs\"], 3)\n",
    "train[\"delinq_2yrs_delta\"] = train[\"delinq_2yrs\"] - train[\"delinq_2yrs_core\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Separating Typical from Extreme Revolving Credit Line Utilized\n",
    "\n",
    "The relationship between credit line utilized is somewhat interesting. There appears to be a higher rate for a bad loan when 0% of the credit lines are utilized, then it drops down slightly and roughly increases linearly in credit line utilized up to 100%. To reflect this finding in the modeling, we will replace the original `revol_util` measure with three derived measures:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\text{revol_util_0} & = & I(\\text{revol_util} == 0) \\\\\n",
    "\\text{revol_util_core} & = & \\max(5, \\min(\\text{revol_util}, 30)) \\\\\n",
    "\\text{revol_util_diff} & = & \\text{revol_util} - \\text{revol_util_core}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"revol_util\"].quantile([0, 0.05, 0.25, 0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = np.linspace(0, 120, 25).tolist()\n",
    "train[\"revol_util_cat\"] = train[\"revol_util\"].cut(breaks = breaks)\n",
    "\n",
    "df = mean_target(train, \"revol_util_cat\")\n",
    "df.drop(df.index[:1], inplace=True) # remove NaN\n",
    "\n",
    "plt.yscale(\"logit\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.plot(df[\"revol_util_cat\"], df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"revol_util\")\n",
    "x_trans.append(\"revol_util_0\")\n",
    "x_trans.append(\"revol_util_core\")\n",
    "x_trans.append(\"revol_util_delta\")\n",
    "\n",
    "train[\"revol_util_0\"] = train[\"revol_util\"] == 0\n",
    "train[\"revol_util_core\"] = h2o.H2OFrame.ifelse(train[\"revol_util\"] <= 100, train[\"revol_util\"], 100)\n",
    "train[\"revol_util_delta\"] = train[\"revol_util\"] - train[\"revol_util_core\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Separating Typical from Extreme Number of Credit Lines\n",
    "\n",
    "Looking at the occurance of bad loans on a logit scale reveal that the chance of a bad loan roughly decreases linearly in number of lines of credit up to about 50. To reflect this finding in the modeling, we will replace the original `total_acc` measure with two derived measures:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\text{total_acc_core} & = & \\min(\\text{total_acc}, 50) \\\\\n",
    "\\text{total_acc_diff} & = & \\text{total_acc} - \\text{total_acc_core}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"total_acc\"].quantile([0, 0.05, 0.25, 0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = np.linspace(0, 60, 13).tolist()\n",
    "train[\"total_acc_cat\"] = train[\"total_acc\"].cut(breaks = breaks)\n",
    "\n",
    "df = mean_target(train, \"total_acc_cat\")\n",
    "df.drop(df.index[:1], inplace=True) # remove NaN\n",
    "\n",
    "plt.yscale(\"logit\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.plot(df[\"total_acc_cat\"], df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train[\"total_acc\"] == None).table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"total_acc\")\n",
    "x_trans.append(\"total_acc_core\")\n",
    "x_trans.append(\"total_acc_delta\")\n",
    "\n",
    "train[\"total_acc_core\"] = h2o.H2OFrame.ifelse(train[\"total_acc\"] <= 50, train[\"total_acc\"], 50)\n",
    "train[\"total_acc_delta\"] = train[\"total_acc\"] - train[\"total_acc_core\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Separating Typical from Extreme Longest Credit Length\n",
    "\n",
    "Looking at the occurance of bad loans on a logit scale reveal that the chance of a bad loan roughly decreases linearly in longest credit length from 3 to 20 years and is highly volatile outside of that range due to small numbers of observations. To reflect this finding in the modeling, we will replace the original `longest_credit_length` measure with two derived measures:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\text{longest_credit_length_core} & = & \\max(3, \\min(\\text{longest_credit_length}, 20)) \\\\\n",
    "\\text{longest_credit_length_diff} & = & \\text{longest_credit_length} - \\text{longest_credit_length_core}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"longest_credit_length\"].quantile([0, 0.05, 0.25, 0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = np.linspace(0, 25, 26).tolist()\n",
    "train[\"longest_credit_length_cat\"] = train[\"longest_credit_length\"].cut(breaks = breaks)\n",
    "\n",
    "df = mean_target(train, \"longest_credit_length_cat\")\n",
    "df.drop(df.index[:1], inplace=True) # remove NaN\n",
    "\n",
    "plt.yscale(\"logit\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.plot(df[\"longest_credit_length_cat\"], df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"longest_credit_length\")\n",
    "x_trans.append(\"longest_credit_length_core\")\n",
    "x_trans.append(\"longest_credit_length_delta\")\n",
    "\n",
    "train[\"longest_credit_length_core\"] = h2o.H2OFrame.ifelse(train[\"longest_credit_length\"] <= 3,\n",
    "                                                          3, train[\"longest_credit_length\"])\n",
    "train[\"longest_credit_length_core\"] = h2o.H2OFrame.ifelse(train[\"longest_credit_length_core\"] <= 20,\n",
    "                                                          train[\"longest_credit_length_core\"], 20)\n",
    "train[\"longest_credit_length_delta\"] = train[\"longest_credit_length\"] - train[\"longest_credit_length_core\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Indicator Functions\n",
    "\n",
    "### 3.1. Converting Term to a 0/1 Indicator\n",
    "\n",
    "Given that term of the loans are either 3 or 5 years, we will create a simplifed `term_36month` binary indicator that is 1 when the terms of the loan is for 5 years and 0 for loans with a term of 3 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"term\"].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"term\")\n",
    "x_trans.append(\"term_60months\")\n",
    "\n",
    "train[\"term_60months\"] = train[\"term\"] == \"60 months\"\n",
    "\n",
    "train[\"term_60months\"].table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Converting Income Verification Status to a 0/1 Indicator\n",
    "\n",
    "Given that incomes are either verified or not verified, we will create a simplifed `verified` binary indicator that is 1 when income has been verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"verification_status\"].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"verification_status\")\n",
    "x_trans.append(\"verified\")\n",
    "\n",
    "train[\"verified\"] = train[\"verification_status\"] == \"verified\"\n",
    "train[\"verified\"].table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Creating Missing Value Indicator for Employment Length\n",
    "\n",
    "The most interesting characteristic about employment length is whether or not it is missing. The divide between those with missing values for employment length to those who have a recorded employment length is 26.3% bad loans to 18.0% bad loans respectively. Interestingly, there doesn't appear to be any differences in bad loans across employment lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train[\"emp_length\"].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.append(\"emp_length_missing\")\n",
    "train[\"emp_length_missing\"] = train[\"emp_length\"] == None\n",
    "\n",
    "mean_target_encoding(train, \"emp_length_missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mean_target(train, \"emp_length\")\n",
    "df.drop(df.index[:1], inplace=True) # remove NaN\n",
    "\n",
    "plt.yscale(\"logit\")\n",
    "plt.plot(df[\"emp_length\"], df[\"mean_bad_loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combining Categories\n",
    "### 4.1.  Combining Categories in Home Ownership\n",
    "\n",
    "Although there are 6 recorded categories within home ownership, only three had over 200 observations: OWN, MORTGAGE, and RENT. The remaining three are so infrequent we will combine them {ANY, NONE, OTHER} with RENT to form an enlarged OTHER category. This new `home_ownership_3cat` variable will have values in {MORTGAGE, OTHER, OWN}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_target(train, \"home_ownership\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvls = [\"OTHER\", \"MORTGAGE\", \"OTHER\", \"OTHER\", \"OWN\", \"OTHER\"]\n",
    "train[\"home_ownership_3cat\"] = train[\"home_ownership\"].set_levels(lvls).ascharacter().asfactor()\n",
    "train[[\"home_ownership\", \"home_ownership_3cat\"]].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_target(train, \"home_ownership_3cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.remove(\"home_ownership\")\n",
    "x_trans.append(\"home_ownership_3cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 (of 7). Train Models using Transformed Data\n",
    "\n",
    "The complete variable set, including transformed variables, are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit a new set of models using AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_trans = H2OAutoML(max_models = 6, exclude_algos = ['DeepLearning'])\n",
    "aml_trans.train(x = x_trans, y = y, training_frame = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aml_trans.leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_trans = h2o.get_model(aml_trans.leaderboard[2,'model_id'])\n",
    "best_model_trans.varimp_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC     (transformed): train = {:.4f}, xval = {:.4f}\" \\\n",
    "      .format(best_model_trans.auc(train = True),\n",
    "              best_model_trans.auc(xval = True)))\n",
    "print(\"Logloss (transformed): train = {:.4f}, xval = {:.4f}\" \\\n",
    "      .format(best_model_trans.logloss(train = True),\n",
    "              best_model_trans.logloss(xval = True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the cross-validated model performance is now much closer to that of the training model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 6 (of 7). Assignment: Transform Test Data (Optional)\n",
    "\n",
    "In order to use the `model_performance` command to evaluate the model on the holdout training data set, you will need to transform the training data using the same feature engineering transformations created above. In other words, transform the data from\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"bad_loan\"\n",
    "x_test = test.col_names\n",
    "x_test.remove(y)\n",
    "x_test.remove(\"int_rate\")\n",
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to the variables below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 (of 7). Shutdown H2O Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
