{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Lending Club Loan Default*\n",
    "\n",
    "In this tutorial, we will go through a step-by-step workflow to determine loan deliquency. Predictions will be made based only on the information available at the time the loan was issued.  Our data is a portion of the public Lending Club data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "1. Start H2O-3 cluster\n",
    "2. Import data\n",
    "3. Clean data\n",
    "4. Feature engineering\n",
    "5. Model training\n",
    "6. Examine model accuracy\n",
    "7. Interpret model\n",
    "8. Save and reuse model\n",
    "9. AutoML (optional)\n",
    "10. Stop H2O-3 cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 (of 10). Start H2O-3 cluster\n",
    "\n",
    "The method you use for starting and stopping an H2O-3 cluster will depend on how H2O is set up on your system. The method we show here is the simplest and most straightforward. It is critical, however, that you properly shut down any H2O cluster that you start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "h2o.init(bind_to_localhost=False)  # run and expose the cluster to the Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 (of 10). Import data\n",
    "\n",
    "A full description of the complete Lending Club data is available at kaggle.com [here](https://www.kaggle.com/pragyanbo/a-hitchhiker-s-guide-to-lending-club-loan-data/notebook). \n",
    "\n",
    "The data set we use below is a local copy of https://s3-us-west-2.amazonaws.com/h2o-tutorials/data/topics/lending/lending_club/LoanStats3a.csv.\n",
    "\n",
    "### View and Inspect the Data\n",
    "\n",
    "The loans data is loaded directly into H2O using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = h2o.import_file(\"../../data/lending_club/LoanStats3a.csv\",\n",
    "                        col_types = {\"int_rate\":\"string\", \n",
    "                                     \"revol_util\":\"string\", \n",
    "                                     \"emp_length\":\"string\", \n",
    "                                     \"verification_status\":\"string\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `h2o.import_file` command completely bypassed python and loaded the data directly into H2O memory. Alternatively, one could load a dataset into a python object, for instance, then pass it to H2O. The approach we use above is far more efficient and generally recommended, especially as data size increases.\n",
    "\n",
    "### Inspect the Data with H2O Flow\n",
    "\n",
    "Now is a good time to enable H2O Flow. Although H2O Flow can be used for everything from loading data to building models to creating production code, we use it here for data investigation and H2O system monitoring.\n",
    "\n",
    "> Note: the reported IP above \n",
    ">\n",
    ">```\n",
    "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n",
    ">```\n",
    ">\n",
    "> is the local IP within your particular cloud instance. \n",
    ">\n",
    "> **To open H2O Flow in your own browser, copy your browser URL and replace the port with 54321.**\n",
    ">\n",
    ">> For example, my Jupyter notebook's URL is \n",
    ">>\n",
    ">>`http://52.202.98.125:8888`. \n",
    ">>\n",
    ">> After opening a new browser tab or window, I copy the address and replace port `8888` with `54321`:\n",
    ">>\n",
    ">>`http://52.202.98.125:54321`.\n",
    "\n",
    "A quick summary of the data size and data fields are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    ">## Editorial comment: Data preparation for modeling\n",
    ">\n",
    ">In this tutorial, we either omit or rush through some steps that a modeler normally would spend a considerable amount of time on. For example, we completely skip exploratory data analysis used in conjunction with modeling. Additionally, the process of **_defining the problem_** is often iterative and takes a lot of thought and effort. We make the assumption here that this work has already been done by the modeler.\n",
    ">\n",
    ">In reality, the majority of a modeler's time is spent on problem definition and data cleaning/wrangling/munging. Our speed at going through these steps to demonstrate the use of H2O-3 in no way minimizes the importance of careful and thoughtful data preparation for model building. \n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 (of 10).  Clean data\n",
    "\n",
    "\n",
    "## Part 1. Defining the problem and creating the response variable\n",
    "\n",
    "The total number of loans in our data is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unfiltered_loans = loans.dim[0]\n",
    "num_unfiltered_loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are interested in loan default, we need to look at the `loan_status` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"loan_status\"].table().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like many real data sources, `loan_status` is messy and contains multiple (somewhat overlapping) categories. Before modeling, we will need to clean this up by (1) removing loans that are still ongoing, and (2) simplifying the response column.\n",
    "\n",
    "### (1) Filter Loans\n",
    "\n",
    "In order to build a valid model, we have to remove loans that are not yet completely \"good\" or \"bad\". These ongoing loans have `loan_status` like \"Current\" and \"In Grace Period\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ongoing_status = [\"Current\",\n",
    "                  \"In Grace Period\",\n",
    "                  \"Late (16-30 days)\",\n",
    "                  \"Late (31-120 days)\",\n",
    "                  \"Does not meet the credit policy.  Status:Current\",\n",
    "                  \"Does not meet the credit policy.  Status:In Grace Period\"\n",
    "                 ]\n",
    "loans = loans[~loans[\"loan_status\"].isin(ongoing_status)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering out ongoing loans, we now have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filtered_loans = loans.dim[0]\n",
    "num_filtered_loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loans whose final state is known, which means we filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_loans_filtered_out = num_unfiltered_loans - num_filtered_loans\n",
    "num_loans_filtered_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loans. \n",
    "\n",
    "These loans are now summarized by `loan_status` as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"loan_status\"].table().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Create Response Column\n",
    "\n",
    "Let's name our response column `bad_loan`, which will be equal to one if the loan was not completely paid off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_paid = [\"Fully Paid\",\n",
    "              \"Does not meet the credit policy.  Status:Fully Paid\"\n",
    "             ]\n",
    "loans[\"bad_loan\"] = ~(loans[\"loan_status\"].isin(fully_paid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the `bad_loan` column a factor so we can build a classification model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"bad_loan\"] = loans[\"bad_loan\"].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of bad loans is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_loan_dist = loans[\"bad_loan\"].table()\n",
    "bad_loan_dist[\"Percentage\"] = (100 * bad_loan_dist[\"Count\"] / loans.nrow).round()\n",
    "bad_loan_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Convert strings to numeric\n",
    "\n",
    "Consider the data columns `int_rate`, `revol_util`, and `emp_length`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[[\"int_rate\", \"revol_util\", \"emp_length\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `int_rate` and `revol_util` are inherently numeric but entered as percentages. Since they include a \"%\" sign, they are read in as strings. The solution for both of these columns is simple: strip the \"%\" sign and convert the strings to numeric.\n",
    "\n",
    "The `emp_length` column is only slightly more complex. Besides removing the \"year\" or \"years\" word, we have to deal with `< 1` and `10+`, which aren't directly numeric. If we define `< 1` as 0 and `10+` as 10, then `emp_length` can also be cast as numeric.\n",
    "\n",
    "We demonstrate the steps for converting these string variables into numeric values below.\n",
    "\n",
    "### Convert `int_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"int_rate\"] = loans[\"int_rate\"].gsub(pattern = \"%\", replacement = \"\") # strip %\n",
    "loans[\"int_rate\"] = loans[\"int_rate\"].trim() # trim whitespace\n",
    "loans[\"int_rate\"] = loans[\"int_rate\"].asnumeric() # change to numeric "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert `revol_util`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"revol_util\"] = loans[\"revol_util\"].gsub(pattern=\"%\", replacement=\"\") # strip %\n",
    "loans[\"revol_util\"] = loans[\"revol_util\"].trim() # trim whitespace\n",
    "loans[\"revol_util\"] = loans[\"revol_util\"].asnumeric() # change to numeric "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert `emp_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gsub to remove \" year\" and \" years\"; also translate n/a to \"\" \n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].gsub(pattern=\"([ ]*+[a-zA-Z].*)|(n/a)\", replacement=\"\") \n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].trim() # trim whitespace\n",
    "\n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].gsub(pattern=\"< 1\", replacement=\"0\") # convert \"< 1\" to 0\n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].gsub(pattern=\"10\\\\+\", replacement=\"10\") # convert \"10+\" to 10\n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].asnumeric() # trim whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The converted results for the three former string variables are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[[\"int_rate\", \"revol_util\", \"emp_length\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Interest rate distributions\n",
    "\n",
    "Now that we have converted interest rate to numeric, we can use the `hist` function to compare the interest rate distributions for good and bad loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "print(\"Bad Loans\")\n",
    "loans[loans[\"bad_loan\"] == \"1\", \"int_rate\"].hist()\n",
    "\n",
    "print(\"Good Loans\")\n",
    "loans[loans[\"bad_loan\"] == \"0\", \"int_rate\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the bad loan distribution contains a higher proportion of high interest rates than the distribution for good loans. Likewise, the good loan distribution contains a higher proportion of low interest rates than that for bad loans. It is a truism to say that interest rate should be a good predictor of default.\n",
    "\n",
    ">Financial institutions typically determine a loan's interest rate based largely on risk and customer demand. If the underwriting rules are any good at all, then we would expect that interest rate would be a strong predictor of default. In fact, we should be surprised if `int_rate` is not one of the top two or three variables in our model regardless of algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Clean up messy categorical columns\n",
    "\n",
    "Much as we did with the `loan_status` column to create our response variable, the `verification_status` column needs cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"verification_status\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are multiple values that mean verified: `VERIFIED - income` and `VERIFIED - income source`.  We will replace these values with `verified`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"verification_status\"] = loans[\"verification_status\"].sub(pattern=\"VERIFIED - income source\", \n",
    "                                                                replacement=\"verified\")\n",
    "loans[\"verification_status\"] = loans[\"verification_status\"].sub(pattern=\"VERIFIED - income\", \n",
    "                                                                replacement=\"verified\")\n",
    "loans[\"verification_status\"] = loans[\"verification_status\"].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resulting in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loans[\"verification_status\"].table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 (of 10).  Feature engineering\n",
    "\n",
    "Now that we have cleaned our data, we can extract information from our current columns to create new features. This process is referred to as _feature engineering_. The general idea is to express information found in our data in a manner that is most understandable to the algorithms we employ, with the goal of improving the performance of our supervised learning models.\n",
    "\n",
    "Feature engineering can be considered the \"secret sauce\" in building a superior predictive model: it is often (although not always) more important than the choice of machine learning algorithm. A very good summary of feature engineering recipes can be found in the online [Driverless AI Documentation](http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/transformations.html). \n",
    "\n",
    "We will do some basic feature engineering using the date fields in our data, and then use NLP (natural language processing) to create word embedding features from the loan description text field in our data.\n",
    "\n",
    "The new columns we will create are: \n",
    "* `credit_length`: the number of years someone has had a credit history\n",
    "* `issue_d_year` and `issue_d_month`: the year and month from the loan issue date\n",
    "* word embeddings from the loan description\n",
    "\n",
    "### Credit Length\n",
    "\n",
    "We can extract the credit length by subtracting the year of their earliest credit line from the year they were issued the loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"credit_length\"] = loans[\"issue_d\"].year() - loans[\"earliest_cr_line\"].year()\n",
    "loans[\"credit_length\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue Date Expansion\n",
    "\n",
    "We next extract the year and month from the issue date.  We may find that the month or the year when the loan was issued will impact the probability of a bad loan. We will treat `issue_d_month` as a factor, since months are cyclical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"issue_d_year\"] = loans[\"issue_d\"].year()\n",
    "loans[\"issue_d_month\"] = loans[\"issue_d\"].month().asfactor()\n",
    "\n",
    "loans[[\"issue_d_year\", \"issue_d_month\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "One of the columns in our dataset is a user-provided description of why the loan was requested. The first few descriptions in the dataset are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"desc\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptions above may contain information that would assist in predicting default, but supervised learning algorithms in general have a hard time understanding text. We need to convert these strings into a numeric representation of the text in order for our algorithms to utilize it. There are multiple choices for doing so, in this example we will use the Word2Vec algorithm.\n",
    "\n",
    "We start by defining stop words (terms that are considered too frequent to carry much information) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = [\"ax\",\"i\",\"you\",\"edu\",\"s\",\"t\",\"m\",\"subject\",\"can\",\"lines\",\"re\",\"what\",\n",
    "              \"there\",\"all\",\"we\",\"one\",\"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\n",
    "              \"but\",\"is\",\"in\",\"a\",\"not\",\"with\",\"as\",\"was\",\"if\",\"they\",\"are\",\"this\",\"and\",\"it\",\"have\",\n",
    "              \"from\",\"at\",\"my\",\"be\",\"by\",\"not\",\"that\",\"to\",\"from\",\"com\",\"org\",\"like\",\"likes\",\"so\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next _tokenize_ the descriptions by breaking the text into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences, stop_word = STOP_WORDS):\n",
    "    tokenized = sentences.tokenize(\"\\\\W+\")\n",
    "    tokenized_lower = tokenized.tolower()\n",
    "    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n",
    "    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n",
    "    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n",
    "    return tokenized_words\n",
    "\n",
    "words = tokenize(loans[\"desc\"].ascharacter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next train our Word2Vec model on the words extracted from our descriptions. We choose an output vector size of 100.\n",
    "\n",
    ">What does Word2Vec do? At a high level, it is a dimensionality reduction method for numerical representations of text. But it reduces dimensionality while preserving (and discovering?) relationships between words in the text.\n",
    ">\n",
    ">Suppose we were to create a dictionary of all the words in our descriptions, and suppose that dictionary contained 2500 unique words. At one extreme, we could create an indicator variable for each word (i.e., one-hot encoding). This would yield 2500 new features that would likely lead to massive overfitting of models.\n",
    ">\n",
    ">At the other extreme, suppose we had someone classify those words into different groups and create indicator variables for each: e.g., `risky_words` (\"bankruptcy\", \"default\", \"forfeit\", \"lien\", etc.), `angry_words` (profanity, \"complaint\", etc.), and so on. This reduces dimensionality by manually grouping words, but it is extremely labor intensive.\n",
    ">\n",
    ">Word2Vec starts with the entire dictionary size $K$ as inputs and the selected vector size $k$ as the target number of outputs. In the in-between layer(s) of the Word2Vec neural net, a $k$-dimensional numeric representation of each word is derived. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "\n",
    "w2v_model = H2OWord2vecEstimator(vec_size=100, model_id=\"w2v\")\n",
    "w2v_model.train(training_frame=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check the Word2Vec model by finding synonyms for the word \"car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.find_synonyms(\"car\", count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next calculate a vector for each description by averaging over all of the words in that description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "desc_vecs = w2v_model.transform(words, aggregate_method=\"AVERAGE\")\n",
    "desc_vecs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the aggregated word embeddings from the Word2Vec model to the loans data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = loans.cbind(desc_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 (of 10). Model training\n",
    "\n",
    "Now that we have cleaned our data and added new columns, we will train a model to predict bad loans. First split our data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = loans.split_frame(seed=25, ratios=[0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = [\"initial_list_status\",\n",
    "                  \"out_prncp\",\n",
    "                  \"out_prncp_inv\",\n",
    "                  \"total_pymnt\",\n",
    "                  \"total_pymnt_inv\",\n",
    "                  \"total_rec_prncp\", \n",
    "                  \"total_rec_int\",\n",
    "                  \"total_rec_late_fee\",\n",
    "                  \"recoveries\",\n",
    "                  \"collection_recovery_fee\",\n",
    "                  \"last_pymnt_d\", \n",
    "                  \"last_pymnt_amnt\",\n",
    "                  \"next_pymnt_d\",\n",
    "                  \"last_credit_pull_d\",\n",
    "                  \"collections_12_mths_ex_med\" , \n",
    "                  \"mths_since_last_major_derog\",\n",
    "                  \"policy_code\",\n",
    "                  \"loan_status\",\n",
    "                  \"funded_amnt\",\n",
    "                  \"funded_amnt_inv\",\n",
    "                  \"mths_since_last_delinq\",\n",
    "                  \"mths_since_last_record\",\n",
    "                  \"id\",\n",
    "                  \"member_id\",\n",
    "                  \"desc\",\n",
    "                  \"zip_code\"]\n",
    "\n",
    "predictors = list(set(loans.col_names) - set(cols_to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create a list of predictors as a subset of the columns of the `loans` H2O Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an XGBoost model for predicting loan default. This model is being run with almost all of the values at their defaults. Later we may want to optimize the hyperparameters using a grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OXGBoostEstimator\n",
    "\n",
    "param = {\n",
    "      \"ntrees\" : 20\n",
    "    , \"nfolds\" : 5\n",
    "    , \"seed\": 25\n",
    "}\n",
    "xgboost_model = H2OXGBoostEstimator(**param)\n",
    "xgboost_model.train(x = predictors,\n",
    "                    y = \"bad_loan\",\n",
    "                    training_frame=train,\n",
    "                    validation_frame=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 (of 10).  Examine model accuracy\n",
    "\n",
    "The plot below shows the performance of the model as more trees are built.  This graph can help us see at what point our model begins overfitting.  Our test data error rate stops improving at around 8-10 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "xgboost_model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve of the training and testing data are shown below.  The area under the ROC curve is much higher for the training data than the test data, indicating that the model may be beginning to memorize the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data\")\n",
    "xgboost_model.model_performance(train = True).plot()\n",
    "print(\"Testing Data\")\n",
    "xgboost_model.model_performance(valid = True).plot()\n",
    "print(\"X-Val\")\n",
    "xgboost_model.model_performance(xval=True).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 (of 10). Interpret model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable importance plot shows us which variables are most important to predicting `bad_loan`.  We can use partial dependency plots to learn more about how these variables affect the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model.varimp_plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, interest rate appears to be the most important feature in predicting loan default. The partial dependency plot of the `int_rate` predictor shows us that as the interest rate increases, the likelihood of the loan defaulting also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pdp = xgboost_model.partial_plot(cols=[\"int_rate\"], data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 (of 10). Save and reuse model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can either be embedded into a self-contained Java MOJO package\n",
    "or it can be saved and later loaded directly into an H2O-3 cluster. For production\n",
    "use, we recommend using MOJO as it is optimized for speed. See the [guide](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html) for further information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading MOJO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model.download_mojo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and reuse the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the model to disk for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = h2o.save_model(model=xgboost_model, force=True)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a later date, we can load the model for batch scoring in the H2O cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = h2o.load_model(path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also score new data using the predict function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bad_loan_hat = loaded_model.predict(test)\n",
    "bad_loan_hat.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9 (of 10). AutoML (optional)\n",
    "\n",
    "AutoML can be used for automating the machine learning workflow, which includes automatic training and tuning of many models within a user-specified time-limit or user specified model build limit. \n",
    "\n",
    "Stacked Ensembles will be automatically trained on collections of individual models to produce highly predictive ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "aml = H2OAutoML(max_models=5, \n",
    "                max_runtime_secs_per_model=60, \n",
    "                include_algos = [\"GLM\", \"DRF\", \"XGBoost\", \"StackedEnsemble\"],\n",
    "                seed=25)\n",
    "aml.train(x=predictors, y='bad_loan', training_frame=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the AutoML job is running, this is a good time to open H2O Flow and monitor the model building process.\n",
    "\n",
    "Once complete, the leaderboard contains the performance metrics of the models generated by AutoML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aml.leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we provided only the training H2O Frame during training, the models are sorted by their cross-validated performance metrics (AUC by default for classification). We can evaluate the best model (`leader`) on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml.leader.model_performance(test_data=test).plot()\n",
    "aml.leader.model_performance(test_data=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another convenient use of H2O Flow is to explore the various models built by AutoML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10 (of 10). Stop H2O-3 cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: H2O-3 documentation\n",
    "\n",
    "* http://docs.h2o.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
