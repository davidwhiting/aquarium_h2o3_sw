{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM and XGBoost Gridsearch Demo\n",
    "\n",
    "In this tutorial, we will go through a step-by-step workflow to demonstrate how easy it is to use H2OXGBoost with Gridsearch.\n",
    "\n",
    "Begin by starting H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "h2o.init(bind_to_localhost=False)  # run and expose the cluster to the Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic Data Set\n",
    "\n",
    "We will look at the famous Titanic passenger data set and try to predict who lived and who died...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"http://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv\"\n",
    "filename = \"../../data/titanic/titanic.csv\"\n",
    "titanic = h2o.import_file(path = filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `survived` as a factor so that H2O can build a classification model. Also cast `ticket` as a factor rather than numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"survived\"] = titanic[\"survived\"].asfactor()\n",
    "titanic[\"ticket\"] = titanic[\"ticket\"].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the predictors and response variables. Note that we exclude `name` because it is a text variable. We also exclude `boat` and `body`, because those variables would not have been known at the time of setting sail. Including those is a classic example of *data leakage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set predictors and response variable\n",
    "response = \"survived\"\n",
    "exclude = [\"name\", \"survived\", \"boat\", \"body\"]\n",
    "# not including boat or body due to data leakage\n",
    "\n",
    "predictors = list(set(titanic.col_names) - set(exclude))\n",
    "predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create training and test data sets. Rather than creating a validation data set, we will use k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = titanic.split_frame(seed = 1234, \n",
    "                                  ratios = [0.75], \n",
    "                                  destination_frames = [\"train.hex\", \"test.hex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default GBM Model\n",
    "\n",
    "Build a GBM model with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "\n",
    "#We only provide the required parameters, everything else is default\n",
    "gbm_model = H2OGradientBoostingEstimator(seed = 1234, nfolds = 5)\n",
    "gbm_model.train(x = predictors\n",
    "                , y = response\n",
    "                , training_frame = train\n",
    "                , validation_frame = test\n",
    "                , model_id = \"gbm_default.hex\"\n",
    "               )\n",
    "\n",
    "## Show a detailed model summary\n",
    "print(gbm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "gbm_model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data\")\n",
    "gbm_model.model_performance(train = True).plot()\n",
    "print(\"Cross-Validation\")\n",
    "gbm_model.model_performance(xval = True).plot()\n",
    "print(\"Testing Data\")\n",
    "gbm_model.model_performance(valid = True).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default GBM model overtrained pretty severely.\n",
    "\n",
    "## Default XGBoost Models\n",
    "\n",
    "Build an XGBoost default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OXGBoostEstimator\n",
    "\n",
    "param = {\"seed\": 1234,\n",
    "         \"nfolds\": 5\n",
    "        }\n",
    "\n",
    "xgboost_model = H2OXGBoostEstimator(**param)\n",
    "xgboost_model.train(x = predictors\n",
    "                    , y = response\n",
    "                    , training_frame = train\n",
    "                    , validation_frame = test\n",
    "                    , model_id = \"xgb_default.hex\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgboost_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data\")\n",
    "xgboost_model.model_performance(train = True).plot()\n",
    "print(\"Cross-Validation\")\n",
    "xgboost_model.model_performance(xval = True).plot()\n",
    "print(\"Testing Data\")\n",
    "xgboost_model.model_performance(valid = True).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default XGBoost model gives us a better result. Let's use gridsearch with early stopping on both models to see if we can improve their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM Gridsearch \n",
    "\n",
    "### Notes on parameter values\n",
    "\n",
    "Our strategy is to start with a large number of trees and a small learning rate in combination with early stopping.\n",
    "\n",
    "- Early stopping kicks in if the AUC doesn't improve by 0.001 for 5 consecutive scoring intervals. \n",
    "- We begin with a not-so-small 0.05 learning rate, but use `learn_rate_annealing` to decrease the learning rate by 1% after each tree. (Alternately, we could set annealing to 1 and make the learning rate smaller.)\n",
    "- We sample 80% of rows per tree (`sample_rate`)\n",
    "- We sample 80% of columns per split (`col_sample_rate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "gbm_params = {'max_depth': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25]\n",
    "              , 'ntrees': [5000]\n",
    "              , 'learn_rate': [0.05]\n",
    "              , 'learn_rate_annealing': [0.99]\n",
    "              , 'sample_rate': [0.8]\n",
    "              , 'col_sample_rate': [0.8]\n",
    "              , 'stopping_metric': 'AUC'\n",
    "              , 'stopping_rounds': [5]\n",
    "              , 'stopping_tolerance': [0.001]\n",
    "             }\n",
    "\n",
    "gbm_grid = H2OGridSearch(model = H2OGradientBoostingEstimator,\n",
    "                         hyper_params = gbm_params\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping is only reproducible if we use `score_tree_interval`; here we set it to score every 10 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_grid.train(x = predictors, y = response\n",
    "               , training_frame = train\n",
    "               , validation_frame = test\n",
    "               , score_tree_interval = 10\n",
    "               , seed = 1234\n",
    "               , grid_id = \"gbm_grid\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Gridsearch\n",
    "\n",
    "Let's do the same with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_params = {'max_depth': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25]\n",
    "                  , 'ntrees': [5000]\n",
    "                  , 'learn_rate': [1, 0.1, 0.01, 0.001]\n",
    "                  , 'sample_rate': [0.8]\n",
    "                  , 'col_sample_rate': [0.8]\n",
    "                  , 'stopping_metric': 'AUC'\n",
    "                  , 'stopping_rounds': [5]\n",
    "                  , 'stopping_tolerance': [0.001]\n",
    "                 }\n",
    "\n",
    "xgboost_grid = H2OGridSearch(model = H2OXGBoostEstimator\n",
    "                             , hyper_params = xgboost_params\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_grid.train(x = predictors, y = response\n",
    "                   , training_frame = train               \n",
    "                   , validation_frame = test               \n",
    "                   , score_tree_interval = 10              \n",
    "                   , seed = 1234\n",
    "                   , grid_id = \"xgboost_grid\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid summary\n",
    "\n",
    "### GBM Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort the grid models by decreasing AUC\n",
    "sorted_gbm_grid = gbm_grid.get_grid(sort_by=\"auc\", decreasing = True)\n",
    "sorted_gbm_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gbm = sorted_gbm_grid.models[0]\n",
    "best_gbm_perf = best_gbm.model_performance(test)\n",
    "best_gbm_perf.auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort the grid models by decreasing AUC\n",
    "sorted_xgboost_grid = xgboost_grid.get_grid(sort_by=\"auc\", decreasing = True)\n",
    "sorted_xgboost_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgboost = sorted_xgboost_grid.models[0]\n",
    "best_xgboost_perf = best_xgboost.model_performance(test)\n",
    "best_xgboost_perf.auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with gridsearch, XGBoost does a better job than GBM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
