{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following demo is taken from Anqi (the original GLRM author at H2O).  \n",
    "\n",
    "I want to illustrate several things from this demo:\n",
    "- analyzing archetype (Y); \n",
    "- analyzing coefficients (X).\n",
    "- advantages of using GLRM to build your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(h2o)\n",
    "h2o.init(strict_version_check=F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dataset contains information from various human subjects walking on a treadmill.  In particular, each subject is attached with sensors at the various joints.  Data from the sensors are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gait.hex <- h2o.importFile(path=\"http://s3.amazonaws.com/h2o-public-test-data/smalldata/glrm_test/subject01_walk1.csv\", destination_frame = \"gait.hex\")\n",
    "dim(gait.hex)\n",
    "summary(gait.hex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a GLRM model using quadratic loss and no regularization since the dataset contains only numeric features.  Skip the first column (time) and set k=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gait.glrm <- h2o.glrm(training_frame = gait.hex, cols = 2:ncol(gait.hex), k = 10, loss = \"Quadratic\", \n",
    "                      regularization_x = \"None\", regularization_y = \"None\", max_iterations = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset basically contains the spatial information of a user's head, temple, toes, wrists, elbows, biceps, sternum, acromium (shoulder above arm joint), midfoots, heels, rear/upper shank, thigh, ....  What do you think the archetypes will look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gait.y <- gait.glrm@model$archetypes\n",
    "gait.y.mat <- as.matrix(gait.y)\n",
    "x_coords <- seq(1, ncol(gait.y), by = 3)\n",
    "y_coords <- seq(2, ncol(gait.y), by = 3)\n",
    "feat_nams <- sapply(colnames(gait.y), function(nam) { substr(nam, 1, nchar(nam)-1) })\n",
    "feat_nams <- as.character(feat_nams[x_coords])\n",
    "for(k in 1:10) {\n",
    "    plot(gait.y.mat[k,x_coords], gait.y.mat[k,y_coords], xlab = \"X-Coordinate Weight\", ylab = \"Y-Coordinate Weight\", main = paste(\"Feature Weights of Archetype\", k), col = \"blue\", pch = 19, lty = \"solid\")\n",
    "    text(gait.y.mat[k,x_coords], gait.y.mat[k,y_coords], labels = feat_nams, cex = 0.7, pos = 3)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to understand if we break our data into A=XY, for a given set of Y, what can we use X for?  What can X show us?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we want to predict whether a firm will repeat an offense or not after a compliance action has been carried out on a firm.  The dataset collected here includes information on each investigation, including zip code (ZCTA) where the firm is located, number of violations found, civil penalities assessed.  The zipcode data by itself is not meaningful.  In fact, it is a categorical data with high cardinality (42000).  If we use one hot encoding to expand the zip code column into 42000 columns, our model will run slowly and probably overfit.  \n",
    "\n",
    "Instead, we choose to user the American Community Survey (ACS) 5-year estimates of household characteristics dataset to expand our zip code column.  Each row of ACS contains information for a unique zip code, other information like household size, income, education level, number of children, etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_orig <- h2o.importFile(path=\"http://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/census/ACS_13_5YR_DP02_cleaned.zip\", col.types = c(\"enum\", rep(\"numeric\", 149)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(acs_orig)\n",
    "summary(acs_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_zcta_col <- acs_orig$ZCTA5\n",
    "acs_full <- acs_orig[,-which(colnames(acs_orig) == \"ZCTA5\")]\n",
    "dim(acs_full)\n",
    "summary(acs_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the zip code column, we build a GLRM model out of the ACS data with k=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_model <- h2o.glrm(training_frame = acs_full, k = 10, transform = \"STANDARDIZE\", \n",
    "                      loss = \"Quadratic\", regularization_x = \"Quadratic\", \n",
    "                      regularization_y = \"L1\", max_iterations = 100, gamma_x = 0.25, gamma_y = 0.5)\n",
    "plot(acs_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of X represents the coefficients needed represent a row of ACS dataset using the archetypes in Y.  For cities that are similar, you will expect them to have similar X values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcta_arch_x <- h2o.getFrame(acs_model@model$representation_name)\n",
    "head(zcta_arch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx <- ((acs_zcta_col == \"10065\") |   # Manhattan, NY (Upper East Side)\n",
    "        (acs_zcta_col == \"11219\") |   # Manhattan, NY (East Harlem)\n",
    "        (acs_zcta_col == \"66753\") |   # McCune, KS\n",
    "        (acs_zcta_col == \"84104\") |   # Salt Lake City, UT\n",
    "        (acs_zcta_col == \"94086\") |   # Sunnyvale, CA\n",
    "        (acs_zcta_col == \"95014\"))    # Cupertino, CA\n",
    "\n",
    "city_arch <- as.data.frame(zcta_arch_x[idx,1:2])\n",
    "xeps <- (max(city_arch[,1]) - min(city_arch[,1])) / 10\n",
    "yeps <- (max(city_arch[,2]) - min(city_arch[,2])) / 10\n",
    "xlims <- c(min(city_arch[,1]) - xeps, max(city_arch[,1]) + xeps)\n",
    "ylims <- c(min(city_arch[,2]) - yeps, max(city_arch[,2]) + yeps)\n",
    "plot(city_arch[,1], city_arch[,2], xlim = xlims, ylim = ylims, xlab = \"First Archetype\", ylab = \"Second Archetype\", main = \"Archetype Representation of Zip Code Tabulation Areas\")\n",
    "text(city_arch[,1], city_arch[,2], labels = c(\"Upper East Side\", \"East Harlem\", \"McCune\", \"Salt Lake City\", \"Sunnyvale\", \"Cupertino\"), pos = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cities like Sunnyvale and Cupertino, they are more similar than with East Harlem.  Note that we are able to cluster coefficients of archetypes of similar cities today, we have no idea what each archetype actually represent.  This is a general problem with machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build a deeplearning model on the WHD dataset to predict repeat and/or willful violators.  For comparison purposes, we will train a model using the original dataset, the original dataset with the zip code column replaced by the compressed GLRM representation (the X matrix) and the data with the zip code column replaced with all the demographic features in the ACS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whd_zcta <- h2o.importFile(path = \"http://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/census/whd_zcta_cleaned.zip\", col.types = c(rep(\"enum\", 7), rep(\"numeric\", 97)))\n",
    "split <- h2o.runif(whd_zcta)\n",
    "train <- whd_zcta[split <= 0.8,]\n",
    "test <- whd_zcta[split > 0.8,]\n",
    "myY <- \"flsa_repeat_violator\"\n",
    "myX <- setdiff(5:ncol(train), which(colnames(train) == myY))\n",
    "orig_time <- system.time(dl_orig <- h2o.deeplearning(x = myX, y = myY, training_frame = train, \n",
    "                                                     validation_frame = test, distribution = \"multinomial\",\n",
    "                                                     epochs = 0.1, hidden = c(50,50,50)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcta_arch_x$zcta5_cd <- acs_zcta_col\n",
    "whd_arch <- h2o.merge(whd_zcta, zcta_arch_x, all.x = TRUE, all.y = FALSE)\n",
    "whd_arch$zcta5_cd <- NULL\n",
    "train_mod <- whd_arch[split <= 0.8,]\n",
    "test_mod  <- whd_arch[split > 0.8,]\n",
    "myX <- setdiff(5:ncol(train_mod), which(colnames(train_mod) == myY))\n",
    "mod_time <- system.time(dl_mod <- h2o.deeplearning(x = myX, y = myY, training_frame = train_mod, \n",
    "                                                   validation_frame = test_mod, distribution = \"multinomial\",\n",
    "                                                   epochs = 0.1, hidden = c(50,50,50)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames(acs_orig)[1] <- \"zcta5_cd\"\n",
    "whd_acs <- h2o.merge(whd_zcta, acs_orig, all.x = TRUE, all.y = FALSE)\n",
    "train_comb <- whd_acs[split <= 0.8,]\n",
    "test_comb <- whd_acs[split > 0.8,]\n",
    "myX <- setdiff(5:ncol(train_comb), which(colnames(train_comb) == myY))\n",
    "comb_time <- system.time(dl_comb <- h2o.deeplearning(x = myX, y = myY, training_frame = train_comb,\n",
    "                                                     validation_frame = test_comb, distribution = \"multinomial\",\n",
    "                                                     epochs = 0.1, hidden = c(50,50,50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.frame(original = c(orig_time[3], h2o.logloss(dl_orig, train = TRUE), h2o.logloss(dl_orig, valid = TRUE)),\n",
    "              reduced  = c(mod_time[3], h2o.logloss(dl_mod, train = TRUE), h2o.logloss(dl_mod, valid = TRUE)),\n",
    "           combined = c(comb_time[3], h2o.logloss(dl_comb, train = TRUE), h2o.logloss(dl_comb, valid = TRUE)),\n",
    "           row.names = c(\"runtime\", \"train_logloss\", \"test_logloss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance between the three models. We see that the model built on the reduced WHD data set finishes almost 10 times faster than the model using the original data set, and it yields a lower log-loss error. The model with the combined WHD-ACS data set does not improve significantly on this error. We can conclude that our GLRM compressed the ZCTA demographics with little informational loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
