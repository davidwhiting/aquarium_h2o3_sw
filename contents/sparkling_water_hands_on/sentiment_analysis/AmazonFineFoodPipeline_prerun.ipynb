{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkling Water Pipeline Productionalization\n",
    "\n",
    "## Background\n",
    "\n",
    "Sparkling Water provides access to H2O algorithms and publishes an API to integrate them as part of regular Spark pipelines. This feature allows for seamless training and deployment of H2O algorithms in the Spark environment. Furthermore, thanks to MOJO (java binary) representations of trained H2O models, production pipelines do not require access to H2O runtime. This enables a wide variety of deployment scenarios. Similarly, Sparkling Water can be used for deploying MOJOs from Driverless AI models.\n",
    "\n",
    "Moreover, by supporting Python and Scala environments, we enable a simple transfer of modeling results between data scientists (\"Python land\") and production (\"JVM land\").\n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goals of this hands-on are two-fold:\n",
    "  - Show integration of H2O models into Spark pipelines using PySpark and PySparkling,\n",
    "  - Demonstrate deployment of the trained pipeline in the context of JVM and Spark streaming.\n",
    "  \n",
    "Our modeling goal is to predict sentiment of Amazon food reviews. For this purpose, we use a pre-processed dataset from [SNAP repository](https://snap.stanford.edu/data/web-FineFoods.html). The dataset contains multiple columns but for simplicity, we will use only `date`, `summary` and overall `score`. The score helps us to approximate sentiment.\n",
    "\n",
    "![Scenario](./img/scenario.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment preparation\n",
    "\n",
    "First, let's verify that `SparkSession` is available in the notebook environment. We do not need to explicitly create a `SparkSession` as it is automatically created for us\n",
    "during startup of the Jupyter notebook. This works because Jupyter is configured with a Spark kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a8dc4da183fe:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3d08104710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare `H2OContext`\n",
    "\n",
    "We will start `H2OContext` in the so-called _internal backend_ mode. The means H2O is sharing the JVM with Spark (see details in [Sparkling Water documentation](https://github.com/h2oai/sparkling-water/blob/rel-2.2/doc/tutorials/backends.rst)).\n",
    "\n",
    "The following call initializes H2O on each Spark executor in the Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://172.17.0.2:54321/h2o ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>06 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.24.0.5</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>2 months and 10 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>sparkling-water-h2o_local-1567066369972</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>7.076 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://172.17.0.2:54321/h2o</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, Amazon S3, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>2.7.16 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------\n",
       "H2O cluster uptime:         06 secs\n",
       "H2O cluster timezone:       Etc/UTC\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.24.0.5\n",
       "H2O cluster version age:    2 months and 10 days\n",
       "H2O cluster name:           sparkling-water-h2o_local-1567066369972\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    7.076 Gb\n",
       "H2O cluster total cores:    16\n",
       "H2O cluster allowed cores:  16\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://172.17.0.2:54321/h2o\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, Amazon S3, AutoML, Core V3, Core V4\n",
       "Python version:             2.7.16 final\n",
       "--------------------------  ---------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparkling Water Context:\n",
      " * Sparkling Water Version: 2.4.13\n",
      " * H2O name: sparkling-water-h2o_local-1567066369972\n",
      " * cluster size: 1\n",
      " * list of used nodes:\n",
      "  (executorId, host, port)\n",
      "  ------------------------\n",
      "  (driver,a8dc4da183fe,54321)\n",
      "  ------------------------\n",
      "\n",
      "  Open H2O Flow in browser: http://172.17.0.2:54321/h2o (CMD + click in Mac OSX)\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from pysparkling import *\n",
    "hc = H2OContext.getOrCreate(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: the reported IP is the private IP of the docker container where the demo is running. To open H2O Flow in your own browser, copy your browser URL and replace the port with 54321.\n",
    ">\n",
    "> For example, my Jupyter notebook's URL is `http://52.202.98.125:8888` (`http://52.202.98.125/jupyter`). After opening a new browser tab or window, copy the address and replace port `8888` with `54321` (`/jupyter` with `/h2o`):\n",
    ">`http://52.202.98.125:54321` (`http://52.202.98.125/h2o)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "We are going to use H2O to load the data since it does a pretty good job of guessing all the nuances of input formats. We will then pass the data to Spark.\n",
    "\n",
    "_(Alternatively, one could load the data directly into a Spark dataframe, bypassing H2O completely. In this case, leveraging H2O for data input is considerably easier.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "import h2o\n",
    "reviews_h2o = h2o.upload_file(\"../../data/amazon_reviews/AmazonReviews_Train.csv\", \"reviews.hex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data table in H2O Flow\n",
    "\n",
    "At this point, we can access H2O Flow and explore data and its properties directly there.\n",
    "\n",
    "### Convert H2O frame to Spark frame so we can pass it as the input to the pipeline\n",
    "\n",
    "After data exploration, we can start with data munging. Since we are going to use Spark for these steps, we will pass the dataframe from H2O to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_spark = hc.as_spark_frame(reviews_h2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick #1: Save the original Spark schema\n",
    "\n",
    "At this point, we will save the input data schema to be used later in the deployed Spark streaming application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = false)\n",
      " |-- ProductId: string (nullable = false)\n",
      " |-- UserId: string (nullable = false)\n",
      " |-- ProfileName: string (nullable = false)\n",
      " |-- HelpfulnessNumerator: short (nullable = false)\n",
      " |-- HelpfulnessDenominator: short (nullable = false)\n",
      " |-- Score: byte (nullable = false)\n",
      " |-- Time: integer (nullable = false)\n",
      " |-- Summary: string (nullable = false)\n",
      " |-- Text: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_spark.printSchema()\n",
    "\n",
    "with open('schema.json','w') as f:\n",
    "    f.write(str(reviews_spark.schema.json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's define all the stages for the pipeline\n",
    "\n",
    "The Spark pipelines are composed of various transformers. In our example, we combine a few Spark transformers to clean up textual data and transform it into numerical format. The pipeline is finalized by training an H2O XGBoost binomial model.\n",
    "\n",
    "> Note: The pipeline stages are not executed right away, they are executed during each fit and transform call.\n",
    "\n",
    "### Define transformer to drop unnecessary columns\n",
    "The Spark `SQLTransformer` allows for using SQL to munge data.\n",
    "\n",
    "As part of this transformer, we convert timestamp to a human readable date string.\n",
    "\n",
    "For this example, we are selecting just the `Score`, `Time` and `Summary` columns. The goal of this analysis is to predict sentiment, i.e., whether the review is positive or negative. The review can be influenced by several aspects. The `Summary` is of course the mostly important information, but `Time` can influence the model as well. For example, people may tend to give higher reviews on Friday evenings because there's a weekend in front of them. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "colSelect = SQLTransformer(\n",
    "    statement=\"SELECT Score, from_unixtime(Time) as Time, Summary FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick #2: Explore intermediate results\n",
    "To explore intermediate results, we can invoke the defined transformer directly. Note that this will cause Spark to execute the transformer as well as all unevaluated upstream code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+--------------------+\n",
      "|Score|               Time|             Summary|\n",
      "+-----+-------------------+--------------------+\n",
      "|    5|2011-04-27 00:00:00|Good Quality Dog ...|\n",
      "|    1|2012-09-07 00:00:00|   Not as Advertised|\n",
      "|    4|2008-08-18 00:00:00|\"Delight\" says it...|\n",
      "|    2|2011-06-13 00:00:00|      Cough Medicine|\n",
      "|    5|2012-10-21 00:00:00|         Great taffy|\n",
      "|    4|2012-07-12 00:00:00|          Nice Taffy|\n",
      "|    5|2012-06-20 00:00:00|Great!  Just as g...|\n",
      "|    5|2012-05-03 00:00:00|Wonderful, tasty ...|\n",
      "|    5|2011-11-23 00:00:00|          Yay Barley|\n",
      "|    5|2012-10-26 00:00:00|    Healthy Dog Food|\n",
      "|    5|2005-02-08 00:00:00|The Best Hot Sauc...|\n",
      "|    5|2010-08-27 00:00:00|My cats LOVE this...|\n",
      "|    1|2012-06-13 00:00:00|My Cats Are Not F...|\n",
      "|    4|2010-11-05 00:00:00|   fresh and greasy!|\n",
      "|    5|2010-03-12 00:00:00|Strawberry Twizzl...|\n",
      "|    5|2009-12-29 00:00:00|Lots of twizzlers...|\n",
      "|    2|2012-09-20 00:00:00|          poor taste|\n",
      "|    5|2012-08-16 00:00:00|            Love it!|\n",
      "|    5|2011-12-23 00:00:00|  GREAT SWEET CANDY!|\n",
      "|    5|2011-10-08 00:00:00|Home delivered tw...|\n",
      "+-----+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = colSelect.transform(reviews_spark)\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transformer to create multiple time features based on the `Time` column\n",
    "\n",
    "The `Time` column is stored internally as a timestamp. To be useful in modeling, we need to extract the time information in a format that is understandable by the predictive algorithms we employ. We can use SparkSQL data methods such as `month`, `dayofmonth`, etc. to engineer multiple new features from the timestamp information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "refineTime = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT  Score,\n",
    "            Summary, \n",
    "            dayofmonth(Time) as Day, \n",
    "            month(Time) as Month, \n",
    "            year(Time) as Year, \n",
    "            weekofyear(Time) as WeekNum, \n",
    "            date_format(Time, 'EEE') as Weekday, \n",
    "            hour(Time) as HourOfDay, \n",
    "            IF(date_format(Time, 'EEE')='Sat' OR date_format(Time, 'EEE')='Sun', 1, 0) as Weekend, \n",
    "            CASE \n",
    "                WHEN month(TIME)=12 OR month(Time)<=2 THEN 'Winter' \n",
    "                WHEN month(TIME)>=3 OR month(Time)<=5 THEN 'Spring' \n",
    "                WHEN month(TIME)>=6 AND month(Time)<=9 THEN 'Summer' \n",
    "                ELSE 'Fall' END as Season \n",
    "    FROM __THIS__\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now inspect the updated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+-----+----+-------+-------+---------+-------+------+\n",
      "|Score|             Summary|Day|Month|Year|WeekNum|Weekday|HourOfDay|Weekend|Season|\n",
      "+-----+--------------------+---+-----+----+-------+-------+---------+-------+------+\n",
      "|    5|Good Quality Dog ...| 27|    4|2011|     17|    Wed|        0|      0|Spring|\n",
      "|    1|   Not as Advertised|  7|    9|2012|     36|    Fri|        0|      0|Spring|\n",
      "|    4|\"Delight\" says it...| 18|    8|2008|     34|    Mon|        0|      0|Spring|\n",
      "|    2|      Cough Medicine| 13|    6|2011|     24|    Mon|        0|      0|Spring|\n",
      "|    5|         Great taffy| 21|   10|2012|     42|    Sun|        0|      1|Spring|\n",
      "|    4|          Nice Taffy| 12|    7|2012|     28|    Thu|        0|      0|Spring|\n",
      "|    5|Great!  Just as g...| 20|    6|2012|     25|    Wed|        0|      0|Spring|\n",
      "|    5|Wonderful, tasty ...|  3|    5|2012|     18|    Thu|        0|      0|Spring|\n",
      "|    5|          Yay Barley| 23|   11|2011|     47|    Wed|        0|      0|Spring|\n",
      "|    5|    Healthy Dog Food| 26|   10|2012|     43|    Fri|        0|      0|Spring|\n",
      "|    5|The Best Hot Sauc...|  8|    2|2005|      6|    Tue|        0|      0|Winter|\n",
      "|    5|My cats LOVE this...| 27|    8|2010|     34|    Fri|        0|      0|Spring|\n",
      "|    1|My Cats Are Not F...| 13|    6|2012|     24|    Wed|        0|      0|Spring|\n",
      "|    4|   fresh and greasy!|  5|   11|2010|     44|    Fri|        0|      0|Spring|\n",
      "|    5|Strawberry Twizzl...| 12|    3|2010|     10|    Fri|        0|      0|Spring|\n",
      "|    5|Lots of twizzlers...| 29|   12|2009|     53|    Tue|        0|      0|Winter|\n",
      "|    2|          poor taste| 20|    9|2012|     38|    Thu|        0|      0|Spring|\n",
      "|    5|            Love it!| 16|    8|2012|     33|    Thu|        0|      0|Spring|\n",
      "|    5|  GREAT SWEET CANDY!| 23|   12|2011|     51|    Fri|        0|      0|Winter|\n",
      "|    5|Home delivered tw...|  8|   10|2011|     40|    Sat|        0|      1|Spring|\n",
      "+-----+--------------------+---+-----+----+-------+-------+---------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined = refineTime.transform(selected)\n",
    "refined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove neutral reviews and classify the Scores\n",
    "\n",
    "We are not interested in the neutral reviews (reviews with the `Score=3`) as they would not add much information to the model. This is a fairly standard approach in NPS (net promoter score) type analyses, and common in particular in sentiment analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, IDF, CountVectorizer\n",
    "\n",
    "filterScore = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT  IF(Score<3,'NEGATIVE', 'POSITIVE') as Sentiment, Summary, Day, Month, Year,\n",
    "            WeekNum, Weekday, HourOfDay, Weekend, Season \n",
    "    FROM __THIS__ WHERE Score !=3 \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-----+----+-------+-------+---------+-------+------+\n",
      "|Sentiment|             Summary|Day|Month|Year|WeekNum|Weekday|HourOfDay|Weekend|Season|\n",
      "+---------+--------------------+---+-----+----+-------+-------+---------+-------+------+\n",
      "| POSITIVE|Good Quality Dog ...| 27|    4|2011|     17|    Wed|        0|      0|Spring|\n",
      "| NEGATIVE|   Not as Advertised|  7|    9|2012|     36|    Fri|        0|      0|Spring|\n",
      "| POSITIVE|\"Delight\" says it...| 18|    8|2008|     34|    Mon|        0|      0|Spring|\n",
      "| NEGATIVE|      Cough Medicine| 13|    6|2011|     24|    Mon|        0|      0|Spring|\n",
      "| POSITIVE|         Great taffy| 21|   10|2012|     42|    Sun|        0|      1|Spring|\n",
      "| POSITIVE|          Nice Taffy| 12|    7|2012|     28|    Thu|        0|      0|Spring|\n",
      "| POSITIVE|Great!  Just as g...| 20|    6|2012|     25|    Wed|        0|      0|Spring|\n",
      "| POSITIVE|Wonderful, tasty ...|  3|    5|2012|     18|    Thu|        0|      0|Spring|\n",
      "| POSITIVE|          Yay Barley| 23|   11|2011|     47|    Wed|        0|      0|Spring|\n",
      "| POSITIVE|    Healthy Dog Food| 26|   10|2012|     43|    Fri|        0|      0|Spring|\n",
      "| POSITIVE|The Best Hot Sauc...|  8|    2|2005|      6|    Tue|        0|      0|Winter|\n",
      "| POSITIVE|My cats LOVE this...| 27|    8|2010|     34|    Fri|        0|      0|Spring|\n",
      "| NEGATIVE|My Cats Are Not F...| 13|    6|2012|     24|    Wed|        0|      0|Spring|\n",
      "| POSITIVE|   fresh and greasy!|  5|   11|2010|     44|    Fri|        0|      0|Spring|\n",
      "| POSITIVE|Strawberry Twizzl...| 12|    3|2010|     10|    Fri|        0|      0|Spring|\n",
      "| POSITIVE|Lots of twizzlers...| 29|   12|2009|     53|    Tue|        0|      0|Winter|\n",
      "| NEGATIVE|          poor taste| 20|    9|2012|     38|    Thu|        0|      0|Spring|\n",
      "| POSITIVE|            Love it!| 16|    8|2012|     33|    Thu|        0|      0|Spring|\n",
      "| POSITIVE|  GREAT SWEET CANDY!| 23|   12|2011|     51|    Fri|        0|      0|Winter|\n",
      "| POSITIVE|Home delivered tw...|  8|   10|2011|     40|    Sat|        0|      1|Spring|\n",
      "+---------+--------------------+---+-----+----+-------+-------+---------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered = filterScore.transform(refined)\n",
    "filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the message\n",
    "\n",
    "Here we use Spark's [RegexTokenizer](https://spark.apache.org/docs/2.1.0/ml-features.html#tokenizer) to tokenize the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"Summary\",\n",
    "                                outputCol=\"tokenized_summary\",\n",
    "                                pattern=\"[, ]\",\n",
    "                                toLowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-----+----+-------+-------+---------+-------+------+--------------------+\n",
      "|Sentiment|             Summary|Day|Month|Year|WeekNum|Weekday|HourOfDay|Weekend|Season|   tokenized_summary|\n",
      "+---------+--------------------+---+-----+----+-------+-------+---------+-------+------+--------------------+\n",
      "| POSITIVE|Good Quality Dog ...| 27|    4|2011|     17|    Wed|        0|      0|Spring|[good, quality, d...|\n",
      "| NEGATIVE|   Not as Advertised|  7|    9|2012|     36|    Fri|        0|      0|Spring|[not, as, adverti...|\n",
      "| POSITIVE|\"Delight\" says it...| 18|    8|2008|     34|    Mon|        0|      0|Spring|[\"delight\", says,...|\n",
      "| NEGATIVE|      Cough Medicine| 13|    6|2011|     24|    Mon|        0|      0|Spring|   [cough, medicine]|\n",
      "| POSITIVE|         Great taffy| 21|   10|2012|     42|    Sun|        0|      1|Spring|      [great, taffy]|\n",
      "| POSITIVE|          Nice Taffy| 12|    7|2012|     28|    Thu|        0|      0|Spring|       [nice, taffy]|\n",
      "| POSITIVE|Great!  Just as g...| 20|    6|2012|     25|    Wed|        0|      0|Spring|[great!, just, as...|\n",
      "| POSITIVE|Wonderful, tasty ...|  3|    5|2012|     18|    Thu|        0|      0|Spring|[wonderful, tasty...|\n",
      "| POSITIVE|          Yay Barley| 23|   11|2011|     47|    Wed|        0|      0|Spring|       [yay, barley]|\n",
      "| POSITIVE|    Healthy Dog Food| 26|   10|2012|     43|    Fri|        0|      0|Spring|[healthy, dog, food]|\n",
      "| POSITIVE|The Best Hot Sauc...|  8|    2|2005|      6|    Tue|        0|      0|Winter|[the, best, hot, ...|\n",
      "| POSITIVE|My cats LOVE this...| 27|    8|2010|     34|    Fri|        0|      0|Spring|[my, cats, love, ...|\n",
      "| NEGATIVE|My Cats Are Not F...| 13|    6|2012|     24|    Wed|        0|      0|Spring|[my, cats, are, n...|\n",
      "| POSITIVE|   fresh and greasy!|  5|   11|2010|     44|    Fri|        0|      0|Spring|[fresh, and, grea...|\n",
      "| POSITIVE|Strawberry Twizzl...| 12|    3|2010|     10|    Fri|        0|      0|Spring|[strawberry, twiz...|\n",
      "| POSITIVE|Lots of twizzlers...| 29|   12|2009|     53|    Tue|        0|      0|Winter|[lots, of, twizzl...|\n",
      "| NEGATIVE|          poor taste| 20|    9|2012|     38|    Thu|        0|      0|Spring|       [poor, taste]|\n",
      "| POSITIVE|            Love it!| 16|    8|2012|     33|    Thu|        0|      0|Spring|         [love, it!]|\n",
      "| POSITIVE|  GREAT SWEET CANDY!| 23|   12|2011|     51|    Fri|        0|      0|Winter|[great, sweet, ca...|\n",
      "| POSITIVE|Home delivered tw...|  8|   10|2011|     40|    Sat|        0|      1|Spring|[home, delivered,...|\n",
      "+---------+--------------------+---+-----+----+-------+-------+---------+-------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized = regexTokenizer.transform(filtered)\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary words\n",
    "\n",
    "Some words do not bring much information for the resulting model. For this, we use Spark's [StopWordsRemover](https://spark.apache.org/docs/2.1.0/ml-features.html#stopwordsremover) to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWordsRemover = StopWordsRemover(inputCol=regexTokenizer.getOutputCol(),\n",
    "                                    outputCol=\"CleanedSummary\",\n",
    "                                    caseSensitive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|Sentiment|             Summary|      CleanedSummary|\n",
      "+---------+--------------------+--------------------+\n",
      "| POSITIVE|Good Quality Dog ...|[good, quality, d...|\n",
      "| NEGATIVE|   Not as Advertised|        [advertised]|\n",
      "| POSITIVE|\"Delight\" says it...|   [\"delight\", says]|\n",
      "| NEGATIVE|      Cough Medicine|   [cough, medicine]|\n",
      "| POSITIVE|         Great taffy|      [great, taffy]|\n",
      "| POSITIVE|          Nice Taffy|       [nice, taffy]|\n",
      "| POSITIVE|Great!  Just as g...|[great!, good, ex...|\n",
      "| POSITIVE|Wonderful, tasty ...|[wonderful, tasty...|\n",
      "| POSITIVE|          Yay Barley|       [yay, barley]|\n",
      "| POSITIVE|    Healthy Dog Food|[healthy, dog, food]|\n",
      "| POSITIVE|The Best Hot Sauc...|[best, hot, sauce...|\n",
      "| POSITIVE|My cats LOVE this...|[cats, love, \"die...|\n",
      "| NEGATIVE|My Cats Are Not F...|[cats, fans, new,...|\n",
      "| POSITIVE|   fresh and greasy!|    [fresh, greasy!]|\n",
      "| POSITIVE|Strawberry Twizzl...|[strawberry, twiz...|\n",
      "| POSITIVE|Lots of twizzlers...|[lots, twizzlers,...|\n",
      "| NEGATIVE|          poor taste|       [poor, taste]|\n",
      "| POSITIVE|            Love it!|         [love, it!]|\n",
      "| POSITIVE|  GREAT SWEET CANDY!|[great, sweet, ca...|\n",
      "| POSITIVE|Home delivered tw...|[home, delivered,...|\n",
      "+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopWordsRemoved = stopWordsRemover.transform(tokenized)\n",
    "stopWordsRemoved.select([\"Sentiment\", \"Summary\", \"CleanedSummary\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash the words\n",
    "\n",
    "NLP (natural language processing) for predictive modeling is based on the idea that text can be represented as numeric values. These values are then fed into any algorithm the user chooses. One choice of numeric representation uses [CountVectorizer](https://spark.apache.org/docs/2.1.0/ml-features.html#countvectorizer).\n",
    "\n",
    "`CountVectorizer` is very similar to the [HashingTF](https://spark.apache.org/docs/2.1.0/ml-features.html#tf-idf) function, except that it preserves the mapping from the index back to the word using an internal vocabulary.\n",
    "\n",
    "For example, if the word `Dog` is stored in the hash at the index `100`, we can get the word back as `countVectorizerModel.vocabulary[100]`.\n",
    "\n",
    "#### Trick #3: Set minDF parameter to limit number of words\n",
    "\n",
    "The `minDF` parameter ensures that only words which occur more the `minDF` times in our case are included. This both speeds the process of modeling and ensures that outliers (infrequent words) do not affect our model that much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer = CountVectorizer(inputCol=stopWordsRemover.getOutputCol(),\n",
    "                                  outputCol=\"frequencies\",\n",
    "                                  minDF=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick #4: Manually train the count vectorizer so we can see how it behaves before we execute the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVecModel = countVectorizer.fit(stopWordsRemoved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is 1528\n",
      "[u'great', u'good', u'best', u'love', u'coffee', u'tea', u'product', u'taste', u'delicious', u'excellent']\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size is \" + str(len(countVecModel.vocabulary)))\n",
    "print(countVecModel.vocabulary[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|Sentiment|      CleanedSummary|         frequencies|\n",
      "+---------+--------------------+--------------------+\n",
      "| POSITIVE|[good, quality, d...|(1528,[1,10,12,35...|\n",
      "| NEGATIVE|        [advertised]|  (1528,[619],[1.0])|\n",
      "| POSITIVE|   [\"delight\", says]|  (1528,[402],[1.0])|\n",
      "| NEGATIVE|   [cough, medicine]|        (1528,[],[])|\n",
      "| POSITIVE|      [great, taffy]|(1528,[0,1434],[1...|\n",
      "| POSITIVE|       [nice, taffy]|(1528,[29,1434],[...|\n",
      "| POSITIVE|[great!, good, ex...|(1528,[1,59,126],...|\n",
      "| POSITIVE|[wonderful, tasty...|(1528,[15,37,1434...|\n",
      "| POSITIVE|       [yay, barley]|        (1528,[],[])|\n",
      "| POSITIVE|[healthy, dog, food]|(1528,[10,12,21],...|\n",
      "| POSITIVE|[best, hot, sauce...|(1528,[2,44,86,45...|\n",
      "| POSITIVE|[cats, love, \"die...|(1528,[3,12,23,41...|\n",
      "| NEGATIVE|[cats, fans, new,...|(1528,[12,41,79],...|\n",
      "| POSITIVE|    [fresh, greasy!]|   (1528,[83],[1.0])|\n",
      "| POSITIVE|[strawberry, twiz...|(1528,[13,19,667]...|\n",
      "| POSITIVE|[lots, twizzlers,...|  (1528,[408],[1.0])|\n",
      "| NEGATIVE|       [poor, taste]|(1528,[7,178],[1....|\n",
      "| POSITIVE|         [love, it!]|(1528,[3,32],[1.0...|\n",
      "| POSITIVE|[great, sweet, ca...|(1528,[0,43,927],...|\n",
      "| POSITIVE|[home, delivered,...|(1528,[352,1077],...|\n",
      "+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorized = countVecModel.transform(stopWordsRemoved)\n",
    "vectorized.select([\"Sentiment\", \"CleanedSummary\", \"frequencies\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create an Inverse Document Frequency (IDF) model\n",
    "\n",
    "Here we use Spark's [tf-idf](https://spark.apache.org/docs/2.1.0/ml-features.html#tf-idf) method to model the importance of a term in a document to the given set of data. Please see the [Spark documentation](https://spark.apache.org/docs/2.1.0/ml-features.html#tf-idf) for more information on TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=countVectorizer.getOutputCol(),\n",
    "          outputCol=\"tf_idf_frequencies\",\n",
    "          minDocFreq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually train the IDF model to see the results before we execute the pipeline,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfModel = idf.fit(vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+\n",
      "|Sentiment|      CleanedSummary|         frequencies|  tf_idf_frequencies|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "| POSITIVE|[good, quality, d...|(1528,[1,10,12,35...|(1528,[1,10,12,35...|\n",
      "| NEGATIVE|        [advertised]|  (1528,[619],[1.0])|(1528,[619],[7.32...|\n",
      "| POSITIVE|   [\"delight\", says]|  (1528,[402],[1.0])|(1528,[402],[6.86...|\n",
      "| NEGATIVE|   [cough, medicine]|        (1528,[],[])|        (1528,[],[])|\n",
      "| POSITIVE|      [great, taffy]|(1528,[0,1434],[1...|(1528,[0,1434],[2...|\n",
      "| POSITIVE|       [nice, taffy]|(1528,[29,1434],[...|(1528,[29,1434],[...|\n",
      "| POSITIVE|[great!, good, ex...|(1528,[1,59,126],...|(1528,[1,59,126],...|\n",
      "| POSITIVE|[wonderful, tasty...|(1528,[15,37,1434...|(1528,[15,37,1434...|\n",
      "| POSITIVE|       [yay, barley]|        (1528,[],[])|        (1528,[],[])|\n",
      "| POSITIVE|[healthy, dog, food]|(1528,[10,12,21],...|(1528,[10,12,21],...|\n",
      "| POSITIVE|[best, hot, sauce...|(1528,[2,44,86,45...|(1528,[2,44,86,45...|\n",
      "| POSITIVE|[cats, love, \"die...|(1528,[3,12,23,41...|(1528,[3,12,23,41...|\n",
      "| NEGATIVE|[cats, fans, new,...|(1528,[12,41,79],...|(1528,[12,41,79],...|\n",
      "| POSITIVE|    [fresh, greasy!]|   (1528,[83],[1.0])|(1528,[83],[5.507...|\n",
      "| POSITIVE|[strawberry, twiz...|(1528,[13,19,667]...|(1528,[13,19,667]...|\n",
      "| POSITIVE|[lots, twizzlers,...|  (1528,[408],[1.0])|(1528,[408],[6.88...|\n",
      "| NEGATIVE|       [poor, taste]|(1528,[7,178],[1....|(1528,[7,178],[3....|\n",
      "| POSITIVE|         [love, it!]|(1528,[3,32],[1.0...|(1528,[3,32],[3.0...|\n",
      "| POSITIVE|[great, sweet, ca...|(1528,[0,43,927],...|(1528,[0,43,927],...|\n",
      "| POSITIVE|[home, delivered,...|(1528,[352,1077],...|(1528,[352,1077],...|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "afterIdf = idfModel.transform(vectorized)\n",
    "afterIdf.select([\"Sentiment\", \"CleanedSummary\", \"frequencies\", \"tf_idf_frequencies\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Remove Summary Column\n",
    "\n",
    "Recall from above that predictive algorithms do not understand string values very well. This is why we transformed the text data of the `Summary` column using TF-IDF. We will keep the numeric representations of `Summary` and drop the original text so that we do not confuse the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeSummary = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT Sentiment, Day, Month, Year, WeekNum, Weekday, HourOfDay, Weekend, Season, tf_idf_frequencies\n",
    "    FROM __THIS__ \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|Weekday|HourOfDay|Weekend|Season|  tf_idf_frequencies|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+\n",
      "| POSITIVE| 27|    4|2011|     17|    Wed|        0|      0|Spring|(1528,[1,10,12,35...|\n",
      "| NEGATIVE|  7|    9|2012|     36|    Fri|        0|      0|Spring|(1528,[619],[7.32...|\n",
      "| POSITIVE| 18|    8|2008|     34|    Mon|        0|      0|Spring|(1528,[402],[6.86...|\n",
      "| NEGATIVE| 13|    6|2011|     24|    Mon|        0|      0|Spring|        (1528,[],[])|\n",
      "| POSITIVE| 21|   10|2012|     42|    Sun|        0|      1|Spring|(1528,[0,1434],[2...|\n",
      "| POSITIVE| 12|    7|2012|     28|    Thu|        0|      0|Spring|(1528,[29,1434],[...|\n",
      "| POSITIVE| 20|    6|2012|     25|    Wed|        0|      0|Spring|(1528,[1,59,126],...|\n",
      "| POSITIVE|  3|    5|2012|     18|    Thu|        0|      0|Spring|(1528,[15,37,1434...|\n",
      "| POSITIVE| 23|   11|2011|     47|    Wed|        0|      0|Spring|        (1528,[],[])|\n",
      "| POSITIVE| 26|   10|2012|     43|    Fri|        0|      0|Spring|(1528,[10,12,21],...|\n",
      "| POSITIVE|  8|    2|2005|      6|    Tue|        0|      0|Winter|(1528,[2,44,86,45...|\n",
      "| POSITIVE| 27|    8|2010|     34|    Fri|        0|      0|Spring|(1528,[3,12,23,41...|\n",
      "| NEGATIVE| 13|    6|2012|     24|    Wed|        0|      0|Spring|(1528,[12,41,79],...|\n",
      "| POSITIVE|  5|   11|2010|     44|    Fri|        0|      0|Spring|(1528,[83],[5.507...|\n",
      "| POSITIVE| 12|    3|2010|     10|    Fri|        0|      0|Spring|(1528,[13,19,667]...|\n",
      "| POSITIVE| 29|   12|2009|     53|    Tue|        0|      0|Winter|(1528,[408],[6.88...|\n",
      "| NEGATIVE| 20|    9|2012|     38|    Thu|        0|      0|Spring|(1528,[7,178],[3....|\n",
      "| POSITIVE| 16|    8|2012|     33|    Thu|        0|      0|Spring|(1528,[3,32],[3.0...|\n",
      "| POSITIVE| 23|   12|2011|     51|    Fri|        0|      0|Winter|(1528,[0,43,927],...|\n",
      "| POSITIVE|  8|   10|2011|     40|    Sat|        0|      1|Spring|(1528,[352,1077],...|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "removedSummary = removeSummary.transform(afterIdf)\n",
    "removedSummary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an XGBoost model using H2O\n",
    "\n",
    "Up to this point, all of our data wrangling and feature engineering efforts have used Spark methods exclusively. Now we turn to H2O to train an H2O XGBoost model on the `Sentiment` column (using default settings). Note that there are many more steps involved with tuning an XGBoost model which we omit here. The full documentation for XGBoost is available at [H2O Documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysparkling.ml import ColumnPruner, H2OXGBoost\n",
    "\n",
    "xgboost = H2OXGBoost(splitRatio=0.8,\n",
    "             featuresCols=[idf.getOutputCol()],\n",
    "             labelCol=\"Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create the pipeline by defining all the stages\n",
    "\n",
    "Now we have all the pieces ready and can define the final pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[colSelect,\n",
    "                            refineTime,\n",
    "                            filterScore,\n",
    "                            regexTokenizer,\n",
    "                            stopWordsRemover,\n",
    "                            countVectorizer,\n",
    "                            idf,\n",
    "                            removeSummary,\n",
    "                            xgboost])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the pipeline model\n",
    "\n",
    "The `fit` call calls each trasformer and estimator in the pipeline and creates so called the `PipelineModel`. The model is trained from the cleaned data from previous transformers and the final model is ready to accept the raw data to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(reviews_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try predictions\n",
    "\n",
    "First, let's load the data that we can use for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "reviews_h2o_pred = h2o.upload_file(\"../../data/amazon_reviews/AmazonReviews_Predictions.csv\", \"reviews_preds.hex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then pass the data to Spark so that we can run the Spark pipeline on it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_spark_pred = hc.as_spark_frame(reviews_h2o_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|Weekday|HourOfDay|Weekend|Season|  tf_idf_frequencies|   prediction_output|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "| POSITIVE|  8|    6|2012|     23|    Fri|        0|      0|Spring|(1528,[48,647,130...|[0.21051210165023...|\n",
      "| POSITIVE| 15|   12|2011|     50|    Thu|        0|      0|Winter|        (1528,[],[])|[0.21051210165023...|\n",
      "| POSITIVE| 14|    9|2011|     37|    Wed|        0|      0|Spring|(1528,[262,306],[...|[0.21051210165023...|\n",
      "| POSITIVE| 20|   10|2011|     42|    Thu|        0|      0|Spring|(1528,[26,452],[4...|[0.21051210165023...|\n",
      "| POSITIVE|  9|    9|2012|     36|    Sun|        0|      1|Spring|(1528,[36,1417],[...|[0.21051210165023...|\n",
      "| POSITIVE|  8|    2|2012|      6|    Wed|        0|      0|Winter|        (1528,[],[])|[0.21051210165023...|\n",
      "| NEGATIVE| 18|    6|2012|     25|    Mon|        0|      0|Spring|        (1528,[],[])|[0.21051210165023...|\n",
      "| POSITIVE| 27|   11|2010|     47|    Sat|        0|      1|Spring|(1528,[2,217,484,...|[0.21051210165023...|\n",
      "| POSITIVE|  4|    4|2011|     14|    Mon|        0|      0|Spring|(1528,[217],[6.27...|[0.21051210165023...|\n",
      "| POSITIVE| 19|    2|2012|      7|    Sun|        0|      1|Winter|(1528,[0,217,621]...|[0.21051210165023...|\n",
      "| POSITIVE|  1|    2|2012|      5|    Wed|        0|      0|Winter|(1528,[23,34,1242...|[0.21051210165023...|\n",
      "| POSITIVE| 29|    3|2009|     13|    Sun|        0|      1|Spring|(1528,[155,230],[...|[0.21051210165023...|\n",
      "| POSITIVE| 28|   12|2007|     52|    Fri|        0|      0|Winter|(1528,[40],[4.812...|[0.21051210165023...|\n",
      "| POSITIVE| 13|    6|2012|     24|    Wed|        0|      0|Spring|(1528,[612],[7.33...|[0.21051210165023...|\n",
      "| NEGATIVE| 28|    4|2012|     17|    Sat|        0|      1|Spring|(1528,[6,974],[3....|[0.21051210165023...|\n",
      "| POSITIVE| 29|    1|2010|      4|    Fri|        0|      0|Winter|(1528,[213,305,13...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    1|2010|      1|    Thu|        0|      0|Winter|(1528,[36,305],[4...|[0.21051210165023...|\n",
      "| POSITIVE| 30|    3|2010|     13|    Tue|        0|      0|Spring|(1528,[61],[5.269...|[0.21051210165023...|\n",
      "| POSITIVE| 24|    6|2010|     25|    Thu|        0|      0|Spring|(1528,[1269],[8.1...|[0.21051210165023...|\n",
      "| POSITIVE| 13|    9|2009|     37|    Sun|        0|      1|Spring|(1528,[1],[2.5776...|[0.21051210165023...|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(reviews_spark_pred).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the pipeline model\n",
    "\n",
    "Later we can use the pipeline model in Scala to demonstrate the deployment of the pipeline in the JVM world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"reviews_pipeline.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick #5: Check variable importances\n",
    "\n",
    "We can inspect the model in H2O Flow and see the variable importances. However, we do not have information about the words, just the indices. We can ask the `CountVectorizer` what word is on the specific index to see what words affect our model the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'great'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages[5].vocabulary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Deploy the Application\n",
    "\n",
    "Up to this point, we have defined the PySpark pipeline. We will now demonstrate its deployment using the PySpark Streaming application in python, where the pipeline defined above will receive raw streaming data and run predictions on them in real time.\n",
    "\n",
    "The steps will be:\n",
    "\n",
    " 1. Load the schema from the schema file.\n",
    " 1. Load the pipeline from the pipeline file.\n",
    " 1. Create an input data stream and pass it the schema. The input data stream will point to a directory where a new csv files will be coming from different streaming sources.\n",
    " 1. Create and output the data stream. For the purposes of this tutorial, we store the data into memory and also into a SparkSQL table.\n",
    " 1. We can inspect the predictions in \"real time\" by regularly displaying the content of the desired table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Check again that we have spark available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a8dc4da183fe:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3d08104710>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the exported schema of our input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Id,IntegerType,false),StructField(ProductId,StringType,false),StructField(UserId,StringType,false),StructField(ProfileName,StringType,false),StructField(HelpfulnessNumerator,ShortType,false),StructField(HelpfulnessDenominator,ShortType,false),StructField(Score,ByteType,false),StructField(Time,IntegerType,false),StructField(Summary,StringType,false),StructField(Text,StringType,false)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "import json\n",
    "\n",
    "schema = StructType.fromJson(json.load(open(\"schema.json\", 'r')))\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the exported pipeline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "pipeline_model = PipelineModel.load(\"reviews_pipeline.model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Start Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.csv\t 130.csv  163.csv  196.csv  228.csv  260.csv  293.csv  40.csv  73.csv\r\n",
      "1.csv\t 131.csv  164.csv  197.csv  229.csv  261.csv  294.csv  41.csv  74.csv\r\n",
      "10.csv\t 132.csv  165.csv  198.csv  23.csv   262.csv  295.csv  42.csv  75.csv\r\n",
      "100.csv  133.csv  166.csv  199.csv  230.csv  263.csv  296.csv  43.csv  76.csv\r\n",
      "101.csv  134.csv  167.csv  2.csv    231.csv  264.csv  297.csv  44.csv  77.csv\r\n",
      "102.csv  135.csv  168.csv  20.csv   232.csv  265.csv  298.csv  45.csv  78.csv\r\n",
      "103.csv  136.csv  169.csv  200.csv  233.csv  266.csv  299.csv  46.csv  79.csv\r\n",
      "104.csv  137.csv  17.csv   201.csv  234.csv  267.csv  3.csv    47.csv  8.csv\r\n",
      "105.csv  138.csv  170.csv  202.csv  235.csv  268.csv  30.csv   48.csv  80.csv\r\n",
      "106.csv  139.csv  171.csv  203.csv  236.csv  269.csv  300.csv  49.csv  81.csv\r\n",
      "107.csv  14.csv   172.csv  204.csv  237.csv  27.csv   301.csv  5.csv   82.csv\r\n",
      "108.csv  140.csv  173.csv  205.csv  238.csv  270.csv  302.csv  50.csv  83.csv\r\n",
      "109.csv  141.csv  174.csv  206.csv  239.csv  271.csv  303.csv  51.csv  84.csv\r\n",
      "11.csv\t 142.csv  175.csv  207.csv  24.csv   272.csv  304.csv  52.csv  85.csv\r\n",
      "110.csv  143.csv  176.csv  208.csv  240.csv  273.csv  305.csv  53.csv  86.csv\r\n",
      "111.csv  144.csv  177.csv  209.csv  241.csv  274.csv  306.csv  54.csv  87.csv\r\n",
      "112.csv  145.csv  178.csv  21.csv   242.csv  275.csv  307.csv  55.csv  88.csv\r\n",
      "113.csv  146.csv  179.csv  210.csv  243.csv  276.csv  308.csv  56.csv  89.csv\r\n",
      "114.csv  147.csv  18.csv   211.csv  244.csv  277.csv  309.csv  57.csv  9.csv\r\n",
      "115.csv  148.csv  180.csv  212.csv  245.csv  278.csv  31.csv   58.csv  90.csv\r\n",
      "116.csv  149.csv  181.csv  213.csv  246.csv  279.csv  310.csv  59.csv  91.csv\r\n",
      "117.csv  15.csv   182.csv  214.csv  247.csv  28.csv   311.csv  6.csv   92.csv\r\n",
      "118.csv  150.csv  183.csv  215.csv  248.csv  280.csv  312.csv  60.csv  93.csv\r\n",
      "119.csv  151.csv  184.csv  216.csv  249.csv  281.csv  313.csv  61.csv  94.csv\r\n",
      "12.csv\t 152.csv  185.csv  217.csv  25.csv   282.csv  314.csv  62.csv  95.csv\r\n",
      "120.csv  153.csv  186.csv  218.csv  250.csv  283.csv  315.csv  63.csv  96.csv\r\n",
      "121.csv  154.csv  187.csv  219.csv  251.csv  284.csv  316.csv  64.csv  97.csv\r\n",
      "122.csv  155.csv  188.csv  22.csv   252.csv  285.csv  32.csv   65.csv  98.csv\r\n",
      "123.csv  156.csv  189.csv  220.csv  253.csv  286.csv  33.csv   66.csv  99.csv\r\n",
      "124.csv  157.csv  19.csv   221.csv  254.csv  287.csv  34.csv   67.csv\r\n",
      "125.csv  158.csv  190.csv  222.csv  255.csv  288.csv  35.csv   68.csv\r\n",
      "126.csv  159.csv  191.csv  223.csv  256.csv  289.csv  36.csv   69.csv\r\n",
      "127.csv  16.csv   192.csv  224.csv  257.csv  29.csv   37.csv   7.csv\r\n",
      "128.csv  160.csv  193.csv  225.csv  258.csv  290.csv  38.csv   70.csv\r\n",
      "129.csv  161.csv  194.csv  226.csv  259.csv  291.csv  39.csv   71.csv\r\n",
      "13.csv\t 162.csv  195.csv  227.csv  26.csv   292.csv  4.csv    72.csv\r\n"
     ]
    }
   ],
   "source": [
    "from subprocess import Popen\n",
    "Popen([\"./start_streaming.sh\"])\n",
    "\n",
    "!ls output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prepare the input data stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_stream = spark.readStream.schema(schema).csv(\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prepare the output data stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_stream = pipeline_model.transform(input_data_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Start processing the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f3ceaf10d50>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data_stream.writeStream.format(\"memory\").queryName(\"predictions\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. List the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+----+-------+-------+---------+-------+------+------------------+-----------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|Weekday|HourOfDay|Weekend|Season|tf_idf_frequencies|prediction_output|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+------------------+-----------------+\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+------------------+-----------------+\n",
      "\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|Weekday|HourOfDay|Weekend|Season|  tf_idf_frequencies|   prediction_output|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "| POSITIVE|  4|    1|2012|      1|    Wed|        0|      0|Winter|(1528,[4,13,30],[...|[0.21051210165023...|\n",
      "| POSITIVE|  8|    4|2012|     14|    Sun|        0|      1|Spring|(1528,[83,138,140...|[0.21051210165023...|\n",
      "| NEGATIVE|  1|    9|2008|     36|    Mon|        0|      0|Spring|(1528,[41,112,239...|[0.21051210165023...|\n",
      "| POSITIVE| 26|    5|2012|     21|    Sat|        0|      1|Spring|(1528,[5,51,204,2...|[0.21051210165023...|\n",
      "| NEGATIVE| 30|    4|2012|     18|    Mon|        0|      0|Spring|        (1528,[],[])|[0.21051210165023...|\n",
      "| POSITIVE|  9|    3|2012|     10|    Fri|        0|      0|Spring|(1528,[10,91,829]...|[0.21051210165023...|\n",
      "| POSITIVE| 29|   11|2010|     48|    Mon|        0|      0|Spring|(1528,[1,159],[2....|[0.21051210165023...|\n",
      "| NEGATIVE|  5|   10|2011|     40|    Wed|        0|      0|Spring|(1528,[656,1514],...|[0.21051210165023...|\n",
      "| POSITIVE| 29|    9|2012|     39|    Sat|        0|      1|Spring|(1528,[117,289],[...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    4|2011|     14|    Thu|        0|      0|Spring|(1528,[263,359],[...|[0.21051210165023...|\n",
      "| POSITIVE| 14|    8|2012|     33|    Tue|        0|      0|Spring|(1528,[8,43,393],...|[0.21051210165023...|\n",
      "| POSITIVE|  3|    9|2011|     35|    Sat|        0|      1|Spring|(1528,[0,41,301],...|[0.21051210165023...|\n",
      "| POSITIVE|  5|    3|2011|      9|    Sat|        0|      1|Spring|(1528,[23,126,908...|[0.21051210165023...|\n",
      "| POSITIVE| 13|   11|2006|     46|    Mon|        0|      0|Spring|(1528,[2,151],[2....|[0.21051210165023...|\n",
      "| POSITIVE|  5|   11|2011|     44|    Sat|        0|      1|Spring|(1528,[2,25,143],...|[0.21051210165023...|\n",
      "| POSITIVE|  4|    4|2012|     14|    Wed|        0|      0|Spring|(1528,[478,944],[...|[0.21051210165023...|\n",
      "| POSITIVE| 25|    9|2011|     38|    Sun|        0|      1|Spring|(1528,[433,606],[...|[0.21051210165023...|\n",
      "| POSITIVE| 24|    2|2012|      8|    Fri|        0|      0|Winter|(1528,[0,10,158],...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    9|2010|     36|    Tue|        0|      0|Spring|(1528,[802],[7.61...|[0.21051210165023...|\n",
      "| NEGATIVE|  9|    4|2011|     14|    Sat|        0|      1|Spring|(1528,[7,1199],[3...|[0.21051210165023...|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|Weekday|HourOfDay|Weekend|Season|  tf_idf_frequencies|   prediction_output|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "| POSITIVE|  4|    1|2012|      1|    Wed|        0|      0|Winter|(1528,[4,13,30],[...|[0.21051210165023...|\n",
      "| POSITIVE|  8|    4|2012|     14|    Sun|        0|      1|Spring|(1528,[83,138,140...|[0.21051210165023...|\n",
      "| NEGATIVE|  1|    9|2008|     36|    Mon|        0|      0|Spring|(1528,[41,112,239...|[0.21051210165023...|\n",
      "| POSITIVE| 26|    5|2012|     21|    Sat|        0|      1|Spring|(1528,[5,51,204,2...|[0.21051210165023...|\n",
      "| NEGATIVE| 30|    4|2012|     18|    Mon|        0|      0|Spring|        (1528,[],[])|[0.21051210165023...|\n",
      "| POSITIVE|  9|    3|2012|     10|    Fri|        0|      0|Spring|(1528,[10,91,829]...|[0.21051210165023...|\n",
      "| POSITIVE| 29|   11|2010|     48|    Mon|        0|      0|Spring|(1528,[1,159],[2....|[0.21051210165023...|\n",
      "| NEGATIVE|  5|   10|2011|     40|    Wed|        0|      0|Spring|(1528,[656,1514],...|[0.21051210165023...|\n",
      "| POSITIVE| 29|    9|2012|     39|    Sat|        0|      1|Spring|(1528,[117,289],[...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    4|2011|     14|    Thu|        0|      0|Spring|(1528,[263,359],[...|[0.21051210165023...|\n",
      "| POSITIVE| 14|    8|2012|     33|    Tue|        0|      0|Spring|(1528,[8,43,393],...|[0.21051210165023...|\n",
      "| POSITIVE|  3|    9|2011|     35|    Sat|        0|      1|Spring|(1528,[0,41,301],...|[0.21051210165023...|\n",
      "| POSITIVE|  5|    3|2011|      9|    Sat|        0|      1|Spring|(1528,[23,126,908...|[0.21051210165023...|\n",
      "| POSITIVE| 13|   11|2006|     46|    Mon|        0|      0|Spring|(1528,[2,151],[2....|[0.21051210165023...|\n",
      "| POSITIVE|  5|   11|2011|     44|    Sat|        0|      1|Spring|(1528,[2,25,143],...|[0.21051210165023...|\n",
      "| POSITIVE|  4|    4|2012|     14|    Wed|        0|      0|Spring|(1528,[478,944],[...|[0.21051210165023...|\n",
      "| POSITIVE| 25|    9|2011|     38|    Sun|        0|      1|Spring|(1528,[433,606],[...|[0.21051210165023...|\n",
      "| POSITIVE| 24|    2|2012|      8|    Fri|        0|      0|Winter|(1528,[0,10,158],...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    9|2010|     36|    Tue|        0|      0|Spring|(1528,[802],[7.61...|[0.21051210165023...|\n",
      "| NEGATIVE|  9|    4|2011|     14|    Sat|        0|      1|Spring|(1528,[7,1199],[3...|[0.21051210165023...|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|Weekday|HourOfDay|Weekend|Season|  tf_idf_frequencies|   prediction_output|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "| POSITIVE|  4|    1|2012|      1|    Wed|        0|      0|Winter|(1528,[4,13,30],[...|[0.21051210165023...|\n",
      "| POSITIVE|  8|    4|2012|     14|    Sun|        0|      1|Spring|(1528,[83,138,140...|[0.21051210165023...|\n",
      "| NEGATIVE|  1|    9|2008|     36|    Mon|        0|      0|Spring|(1528,[41,112,239...|[0.21051210165023...|\n",
      "| POSITIVE| 26|    5|2012|     21|    Sat|        0|      1|Spring|(1528,[5,51,204,2...|[0.21051210165023...|\n",
      "| NEGATIVE| 30|    4|2012|     18|    Mon|        0|      0|Spring|        (1528,[],[])|[0.21051210165023...|\n",
      "| POSITIVE|  9|    3|2012|     10|    Fri|        0|      0|Spring|(1528,[10,91,829]...|[0.21051210165023...|\n",
      "| POSITIVE| 29|   11|2010|     48|    Mon|        0|      0|Spring|(1528,[1,159],[2....|[0.21051210165023...|\n",
      "| NEGATIVE|  5|   10|2011|     40|    Wed|        0|      0|Spring|(1528,[656,1514],...|[0.21051210165023...|\n",
      "| POSITIVE| 29|    9|2012|     39|    Sat|        0|      1|Spring|(1528,[117,289],[...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    4|2011|     14|    Thu|        0|      0|Spring|(1528,[263,359],[...|[0.21051210165023...|\n",
      "| POSITIVE| 14|    8|2012|     33|    Tue|        0|      0|Spring|(1528,[8,43,393],...|[0.21051210165023...|\n",
      "| POSITIVE|  3|    9|2011|     35|    Sat|        0|      1|Spring|(1528,[0,41,301],...|[0.21051210165023...|\n",
      "| POSITIVE|  5|    3|2011|      9|    Sat|        0|      1|Spring|(1528,[23,126,908...|[0.21051210165023...|\n",
      "| POSITIVE| 13|   11|2006|     46|    Mon|        0|      0|Spring|(1528,[2,151],[2....|[0.21051210165023...|\n",
      "| POSITIVE|  5|   11|2011|     44|    Sat|        0|      1|Spring|(1528,[2,25,143],...|[0.21051210165023...|\n",
      "| POSITIVE|  4|    4|2012|     14|    Wed|        0|      0|Spring|(1528,[478,944],[...|[0.21051210165023...|\n",
      "| POSITIVE| 25|    9|2011|     38|    Sun|        0|      1|Spring|(1528,[433,606],[...|[0.21051210165023...|\n",
      "| POSITIVE| 24|    2|2012|      8|    Fri|        0|      0|Winter|(1528,[0,10,158],...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    9|2010|     36|    Tue|        0|      0|Spring|(1528,[802],[7.61...|[0.21051210165023...|\n",
      "| NEGATIVE|  9|    4|2011|     14|    Sat|        0|      1|Spring|(1528,[7,1199],[3...|[0.21051210165023...|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|Weekday|HourOfDay|Weekend|Season|  tf_idf_frequencies|   prediction_output|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "| POSITIVE|  4|    1|2012|      1|    Wed|        0|      0|Winter|(1528,[4,13,30],[...|[0.21051210165023...|\n",
      "| POSITIVE|  8|    4|2012|     14|    Sun|        0|      1|Spring|(1528,[83,138,140...|[0.21051210165023...|\n",
      "| NEGATIVE|  1|    9|2008|     36|    Mon|        0|      0|Spring|(1528,[41,112,239...|[0.21051210165023...|\n",
      "| POSITIVE| 26|    5|2012|     21|    Sat|        0|      1|Spring|(1528,[5,51,204,2...|[0.21051210165023...|\n",
      "| NEGATIVE| 30|    4|2012|     18|    Mon|        0|      0|Spring|        (1528,[],[])|[0.21051210165023...|\n",
      "| POSITIVE|  9|    3|2012|     10|    Fri|        0|      0|Spring|(1528,[10,91,829]...|[0.21051210165023...|\n",
      "| POSITIVE| 29|   11|2010|     48|    Mon|        0|      0|Spring|(1528,[1,159],[2....|[0.21051210165023...|\n",
      "| NEGATIVE|  5|   10|2011|     40|    Wed|        0|      0|Spring|(1528,[656,1514],...|[0.21051210165023...|\n",
      "| POSITIVE| 29|    9|2012|     39|    Sat|        0|      1|Spring|(1528,[117,289],[...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    4|2011|     14|    Thu|        0|      0|Spring|(1528,[263,359],[...|[0.21051210165023...|\n",
      "| POSITIVE| 14|    8|2012|     33|    Tue|        0|      0|Spring|(1528,[8,43,393],...|[0.21051210165023...|\n",
      "| POSITIVE|  3|    9|2011|     35|    Sat|        0|      1|Spring|(1528,[0,41,301],...|[0.21051210165023...|\n",
      "| POSITIVE|  5|    3|2011|      9|    Sat|        0|      1|Spring|(1528,[23,126,908...|[0.21051210165023...|\n",
      "| POSITIVE| 13|   11|2006|     46|    Mon|        0|      0|Spring|(1528,[2,151],[2....|[0.21051210165023...|\n",
      "| POSITIVE|  5|   11|2011|     44|    Sat|        0|      1|Spring|(1528,[2,25,143],...|[0.21051210165023...|\n",
      "| POSITIVE|  4|    4|2012|     14|    Wed|        0|      0|Spring|(1528,[478,944],[...|[0.21051210165023...|\n",
      "| POSITIVE| 25|    9|2011|     38|    Sun|        0|      1|Spring|(1528,[433,606],[...|[0.21051210165023...|\n",
      "| POSITIVE| 24|    2|2012|      8|    Fri|        0|      0|Winter|(1528,[0,10,158],...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    9|2010|     36|    Tue|        0|      0|Spring|(1528,[802],[7.61...|[0.21051210165023...|\n",
      "| NEGATIVE|  9|    4|2011|     14|    Sat|        0|      1|Spring|(1528,[7,1199],[3...|[0.21051210165023...|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|Weekday|HourOfDay|Weekend|Season|  tf_idf_frequencies|   prediction_output|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "| POSITIVE|  4|    1|2012|      1|    Wed|        0|      0|Winter|(1528,[4,13,30],[...|[0.21051210165023...|\n",
      "| POSITIVE|  8|    4|2012|     14|    Sun|        0|      1|Spring|(1528,[83,138,140...|[0.21051210165023...|\n",
      "| NEGATIVE|  1|    9|2008|     36|    Mon|        0|      0|Spring|(1528,[41,112,239...|[0.21051210165023...|\n",
      "| POSITIVE| 26|    5|2012|     21|    Sat|        0|      1|Spring|(1528,[5,51,204,2...|[0.21051210165023...|\n",
      "| NEGATIVE| 30|    4|2012|     18|    Mon|        0|      0|Spring|        (1528,[],[])|[0.21051210165023...|\n",
      "| POSITIVE|  9|    3|2012|     10|    Fri|        0|      0|Spring|(1528,[10,91,829]...|[0.21051210165023...|\n",
      "| POSITIVE| 29|   11|2010|     48|    Mon|        0|      0|Spring|(1528,[1,159],[2....|[0.21051210165023...|\n",
      "| NEGATIVE|  5|   10|2011|     40|    Wed|        0|      0|Spring|(1528,[656,1514],...|[0.21051210165023...|\n",
      "| POSITIVE| 29|    9|2012|     39|    Sat|        0|      1|Spring|(1528,[117,289],[...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    4|2011|     14|    Thu|        0|      0|Spring|(1528,[263,359],[...|[0.21051210165023...|\n",
      "| POSITIVE| 14|    8|2012|     33|    Tue|        0|      0|Spring|(1528,[8,43,393],...|[0.21051210165023...|\n",
      "| POSITIVE|  3|    9|2011|     35|    Sat|        0|      1|Spring|(1528,[0,41,301],...|[0.21051210165023...|\n",
      "| POSITIVE|  5|    3|2011|      9|    Sat|        0|      1|Spring|(1528,[23,126,908...|[0.21051210165023...|\n",
      "| POSITIVE| 13|   11|2006|     46|    Mon|        0|      0|Spring|(1528,[2,151],[2....|[0.21051210165023...|\n",
      "| POSITIVE|  5|   11|2011|     44|    Sat|        0|      1|Spring|(1528,[2,25,143],...|[0.21051210165023...|\n",
      "| POSITIVE|  4|    4|2012|     14|    Wed|        0|      0|Spring|(1528,[478,944],[...|[0.21051210165023...|\n",
      "| POSITIVE| 25|    9|2011|     38|    Sun|        0|      1|Spring|(1528,[433,606],[...|[0.21051210165023...|\n",
      "| POSITIVE| 24|    2|2012|      8|    Fri|        0|      0|Winter|(1528,[0,10,158],...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    9|2010|     36|    Tue|        0|      0|Spring|(1528,[802],[7.61...|[0.21051210165023...|\n",
      "| NEGATIVE|  9|    4|2011|     14|    Sat|        0|      1|Spring|(1528,[7,1199],[3...|[0.21051210165023...|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|Weekday|HourOfDay|Weekend|Season|  tf_idf_frequencies|   prediction_output|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "| POSITIVE|  4|    1|2012|      1|    Wed|        0|      0|Winter|(1528,[4,13,30],[...|[0.21051210165023...|\n",
      "| POSITIVE|  8|    4|2012|     14|    Sun|        0|      1|Spring|(1528,[83,138,140...|[0.21051210165023...|\n",
      "| NEGATIVE|  1|    9|2008|     36|    Mon|        0|      0|Spring|(1528,[41,112,239...|[0.21051210165023...|\n",
      "| POSITIVE| 26|    5|2012|     21|    Sat|        0|      1|Spring|(1528,[5,51,204,2...|[0.21051210165023...|\n",
      "| NEGATIVE| 30|    4|2012|     18|    Mon|        0|      0|Spring|        (1528,[],[])|[0.21051210165023...|\n",
      "| POSITIVE|  9|    3|2012|     10|    Fri|        0|      0|Spring|(1528,[10,91,829]...|[0.21051210165023...|\n",
      "| POSITIVE| 29|   11|2010|     48|    Mon|        0|      0|Spring|(1528,[1,159],[2....|[0.21051210165023...|\n",
      "| NEGATIVE|  5|   10|2011|     40|    Wed|        0|      0|Spring|(1528,[656,1514],...|[0.21051210165023...|\n",
      "| POSITIVE| 29|    9|2012|     39|    Sat|        0|      1|Spring|(1528,[117,289],[...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    4|2011|     14|    Thu|        0|      0|Spring|(1528,[263,359],[...|[0.21051210165023...|\n",
      "| POSITIVE| 14|    8|2012|     33|    Tue|        0|      0|Spring|(1528,[8,43,393],...|[0.21051210165023...|\n",
      "| POSITIVE|  3|    9|2011|     35|    Sat|        0|      1|Spring|(1528,[0,41,301],...|[0.21051210165023...|\n",
      "| POSITIVE|  5|    3|2011|      9|    Sat|        0|      1|Spring|(1528,[23,126,908...|[0.21051210165023...|\n",
      "| POSITIVE| 13|   11|2006|     46|    Mon|        0|      0|Spring|(1528,[2,151],[2....|[0.21051210165023...|\n",
      "| POSITIVE|  5|   11|2011|     44|    Sat|        0|      1|Spring|(1528,[2,25,143],...|[0.21051210165023...|\n",
      "| POSITIVE|  4|    4|2012|     14|    Wed|        0|      0|Spring|(1528,[478,944],[...|[0.21051210165023...|\n",
      "| POSITIVE| 25|    9|2011|     38|    Sun|        0|      1|Spring|(1528,[433,606],[...|[0.21051210165023...|\n",
      "| POSITIVE| 24|    2|2012|      8|    Fri|        0|      0|Winter|(1528,[0,10,158],...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    9|2010|     36|    Tue|        0|      0|Spring|(1528,[802],[7.61...|[0.21051210165023...|\n",
      "| NEGATIVE|  9|    4|2011|     14|    Sat|        0|      1|Spring|(1528,[7,1199],[3...|[0.21051210165023...|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|Weekday|HourOfDay|Weekend|Season|  tf_idf_frequencies|   prediction_output|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "| POSITIVE|  4|    1|2012|      1|    Wed|        0|      0|Winter|(1528,[4,13,30],[...|[0.21051210165023...|\n",
      "| POSITIVE|  8|    4|2012|     14|    Sun|        0|      1|Spring|(1528,[83,138,140...|[0.21051210165023...|\n",
      "| NEGATIVE|  1|    9|2008|     36|    Mon|        0|      0|Spring|(1528,[41,112,239...|[0.21051210165023...|\n",
      "| POSITIVE| 26|    5|2012|     21|    Sat|        0|      1|Spring|(1528,[5,51,204,2...|[0.21051210165023...|\n",
      "| NEGATIVE| 30|    4|2012|     18|    Mon|        0|      0|Spring|        (1528,[],[])|[0.21051210165023...|\n",
      "| POSITIVE|  9|    3|2012|     10|    Fri|        0|      0|Spring|(1528,[10,91,829]...|[0.21051210165023...|\n",
      "| POSITIVE| 29|   11|2010|     48|    Mon|        0|      0|Spring|(1528,[1,159],[2....|[0.21051210165023...|\n",
      "| NEGATIVE|  5|   10|2011|     40|    Wed|        0|      0|Spring|(1528,[656,1514],...|[0.21051210165023...|\n",
      "| POSITIVE| 29|    9|2012|     39|    Sat|        0|      1|Spring|(1528,[117,289],[...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    4|2011|     14|    Thu|        0|      0|Spring|(1528,[263,359],[...|[0.21051210165023...|\n",
      "| POSITIVE| 14|    8|2012|     33|    Tue|        0|      0|Spring|(1528,[8,43,393],...|[0.21051210165023...|\n",
      "| POSITIVE|  3|    9|2011|     35|    Sat|        0|      1|Spring|(1528,[0,41,301],...|[0.21051210165023...|\n",
      "| POSITIVE|  5|    3|2011|      9|    Sat|        0|      1|Spring|(1528,[23,126,908...|[0.21051210165023...|\n",
      "| POSITIVE| 13|   11|2006|     46|    Mon|        0|      0|Spring|(1528,[2,151],[2....|[0.21051210165023...|\n",
      "| POSITIVE|  5|   11|2011|     44|    Sat|        0|      1|Spring|(1528,[2,25,143],...|[0.21051210165023...|\n",
      "| POSITIVE|  4|    4|2012|     14|    Wed|        0|      0|Spring|(1528,[478,944],[...|[0.21051210165023...|\n",
      "| POSITIVE| 25|    9|2011|     38|    Sun|        0|      1|Spring|(1528,[433,606],[...|[0.21051210165023...|\n",
      "| POSITIVE| 24|    2|2012|      8|    Fri|        0|      0|Winter|(1528,[0,10,158],...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    9|2010|     36|    Tue|        0|      0|Spring|(1528,[802],[7.61...|[0.21051210165023...|\n",
      "| NEGATIVE|  9|    4|2011|     14|    Sat|        0|      1|Spring|(1528,[7,1199],[3...|[0.21051210165023...|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|Weekday|HourOfDay|Weekend|Season|  tf_idf_frequencies|   prediction_output|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "| POSITIVE|  4|    1|2012|      1|    Wed|        0|      0|Winter|(1528,[4,13,30],[...|[0.21051210165023...|\n",
      "| POSITIVE|  8|    4|2012|     14|    Sun|        0|      1|Spring|(1528,[83,138,140...|[0.21051210165023...|\n",
      "| NEGATIVE|  1|    9|2008|     36|    Mon|        0|      0|Spring|(1528,[41,112,239...|[0.21051210165023...|\n",
      "| POSITIVE| 26|    5|2012|     21|    Sat|        0|      1|Spring|(1528,[5,51,204,2...|[0.21051210165023...|\n",
      "| NEGATIVE| 30|    4|2012|     18|    Mon|        0|      0|Spring|        (1528,[],[])|[0.21051210165023...|\n",
      "| POSITIVE|  9|    3|2012|     10|    Fri|        0|      0|Spring|(1528,[10,91,829]...|[0.21051210165023...|\n",
      "| POSITIVE| 29|   11|2010|     48|    Mon|        0|      0|Spring|(1528,[1,159],[2....|[0.21051210165023...|\n",
      "| NEGATIVE|  5|   10|2011|     40|    Wed|        0|      0|Spring|(1528,[656,1514],...|[0.21051210165023...|\n",
      "| POSITIVE| 29|    9|2012|     39|    Sat|        0|      1|Spring|(1528,[117,289],[...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    4|2011|     14|    Thu|        0|      0|Spring|(1528,[263,359],[...|[0.21051210165023...|\n",
      "| POSITIVE| 14|    8|2012|     33|    Tue|        0|      0|Spring|(1528,[8,43,393],...|[0.21051210165023...|\n",
      "| POSITIVE|  3|    9|2011|     35|    Sat|        0|      1|Spring|(1528,[0,41,301],...|[0.21051210165023...|\n",
      "| POSITIVE|  5|    3|2011|      9|    Sat|        0|      1|Spring|(1528,[23,126,908...|[0.21051210165023...|\n",
      "| POSITIVE| 13|   11|2006|     46|    Mon|        0|      0|Spring|(1528,[2,151],[2....|[0.21051210165023...|\n",
      "| POSITIVE|  5|   11|2011|     44|    Sat|        0|      1|Spring|(1528,[2,25,143],...|[0.21051210165023...|\n",
      "| POSITIVE|  4|    4|2012|     14|    Wed|        0|      0|Spring|(1528,[478,944],[...|[0.21051210165023...|\n",
      "| POSITIVE| 25|    9|2011|     38|    Sun|        0|      1|Spring|(1528,[433,606],[...|[0.21051210165023...|\n",
      "| POSITIVE| 24|    2|2012|      8|    Fri|        0|      0|Winter|(1528,[0,10,158],...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    9|2010|     36|    Tue|        0|      0|Spring|(1528,[802],[7.61...|[0.21051210165023...|\n",
      "| NEGATIVE|  9|    4|2011|     14|    Sat|        0|      1|Spring|(1528,[7,1199],[3...|[0.21051210165023...|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "|Sentiment|Day|Month|Year|WeekNum|Weekday|HourOfDay|Weekend|Season|  tf_idf_frequencies|   prediction_output|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "| POSITIVE|  4|    1|2012|      1|    Wed|        0|      0|Winter|(1528,[4,13,30],[...|[0.21051210165023...|\n",
      "| POSITIVE|  8|    4|2012|     14|    Sun|        0|      1|Spring|(1528,[83,138,140...|[0.21051210165023...|\n",
      "| NEGATIVE|  1|    9|2008|     36|    Mon|        0|      0|Spring|(1528,[41,112,239...|[0.21051210165023...|\n",
      "| POSITIVE| 26|    5|2012|     21|    Sat|        0|      1|Spring|(1528,[5,51,204,2...|[0.21051210165023...|\n",
      "| NEGATIVE| 30|    4|2012|     18|    Mon|        0|      0|Spring|        (1528,[],[])|[0.21051210165023...|\n",
      "| POSITIVE|  9|    3|2012|     10|    Fri|        0|      0|Spring|(1528,[10,91,829]...|[0.21051210165023...|\n",
      "| POSITIVE| 29|   11|2010|     48|    Mon|        0|      0|Spring|(1528,[1,159],[2....|[0.21051210165023...|\n",
      "| NEGATIVE|  5|   10|2011|     40|    Wed|        0|      0|Spring|(1528,[656,1514],...|[0.21051210165023...|\n",
      "| POSITIVE| 29|    9|2012|     39|    Sat|        0|      1|Spring|(1528,[117,289],[...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    4|2011|     14|    Thu|        0|      0|Spring|(1528,[263,359],[...|[0.21051210165023...|\n",
      "| POSITIVE| 14|    8|2012|     33|    Tue|        0|      0|Spring|(1528,[8,43,393],...|[0.21051210165023...|\n",
      "| POSITIVE|  3|    9|2011|     35|    Sat|        0|      1|Spring|(1528,[0,41,301],...|[0.21051210165023...|\n",
      "| POSITIVE|  5|    3|2011|      9|    Sat|        0|      1|Spring|(1528,[23,126,908...|[0.21051210165023...|\n",
      "| POSITIVE| 13|   11|2006|     46|    Mon|        0|      0|Spring|(1528,[2,151],[2....|[0.21051210165023...|\n",
      "| POSITIVE|  5|   11|2011|     44|    Sat|        0|      1|Spring|(1528,[2,25,143],...|[0.21051210165023...|\n",
      "| POSITIVE|  4|    4|2012|     14|    Wed|        0|      0|Spring|(1528,[478,944],[...|[0.21051210165023...|\n",
      "| POSITIVE| 25|    9|2011|     38|    Sun|        0|      1|Spring|(1528,[433,606],[...|[0.21051210165023...|\n",
      "| POSITIVE| 24|    2|2012|      8|    Fri|        0|      0|Winter|(1528,[0,10,158],...|[0.21051210165023...|\n",
      "| POSITIVE|  7|    9|2010|     36|    Tue|        0|      0|Spring|(1528,[802],[7.61...|[0.21051210165023...|\n",
      "| NEGATIVE|  9|    4|2011|     14|    Sat|        0|      1|Spring|(1528,[7,1199],[3...|[0.21051210165023...|\n",
      "+---------+---+-----+----+-------+-------+---------+-------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# limit to 10 for hands-on\n",
    "# could replace with \n",
    "# while(True):\n",
    "\n",
    "for x in range(10):\n",
    "    spark.sql(\"select * from predictions\").show()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Shut down the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySparkling",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
