{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License \n",
    "\n",
    "Copyright 2019 Patrick Hall and the H2O.ai team\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "**DISCLAIMER:** This notebook is not legal compliance advice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increase Fairness in Your Machine Learning Project with Disparate Impact Analysis using Python and H2O\n",
    "\n",
    "#### Assess if your machine learning model is treating different groups of people similarly or differently\n",
    "\n",
    "Fairness is an incredibly important, but highly complex entity. So much so that leading scholars have yet to agree on a strict definition. However, there is a practical way to discuss and handle *observational* fairness, or how your model predictions affect different groups of people. This procedure is often known as disparate impact analysis (DIA). DIA is far from perfect, as it relies heavily on user-defined thresholds and reference levels for disparity and does not attempt to remediate disparity or provide information on sources of disparity, but it is a fairly straightforward method to quantify your model’s behavior across sensitive demographic segments or other potentially interesting groups of observations. DIA is also an accepted, regulation-compliant tool for fair-lending purposes in the U.S. financial services industry. If it’s good enough for multibillion-dollar credit portfolios, it’s probably good enough for your project. \n",
    "\n",
    "This example DIA notebook starts by training a constrained, monotonic gradient boosting machine (GBM) classifier on the UCI credit card default data using the popular open source library, h2o. A probability cutoff for making credit decisions is selected by maximizing the F1 statistic and confusion matrices are generated to summarize the GBM’s decisions across male and female customers. A basic DIA procedure is then conducted using the information stored in the confusion matrices to calculate an adverse impact ratio. Several other disparity and parity metrics are calculated, and although it's abusing the technique, those will also be discussed and analyzed in the context of DIA.  \n",
    "\n",
    "Because DIA really only considers groups of people, it's important to pair DIA with constrained models that can't treat similar people too differently. When using complex models with DIA, it's also important to look for any local, or individual, dicrimination that would not be flagged in the group quantities.\n",
    "\n",
    "For an excellent and free reference on the topics of fairness and machine learning be sure to follow the book being written by Solon Barocas, Moritz Hardt and Arvind Narayanan: https://fairmlbook.org/. \n",
    "\n",
    "For Python fairness libraries that go far beyond the scope of this notebook see: \n",
    "* aequitas: https://github.com/dssg/aequitas\n",
    "* AI Fairness 360: http://aif360.mybluemix.net/\n",
    "* Themis: https://github.com/LASER-UMASS/Themis\n",
    "\n",
    "#### Start H2O cluster\n",
    "\n",
    "\n",
    "The `os` commands below check whether this notebook is being run on the Aquarium platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "startup = '/home/h2o/bin/aquarium_startup'\n",
    "if os.path.exists(startup):\n",
    "    os.system(startup)\n",
    "    local_url = 'http://localhost:54321/h2o'\n",
    "    aquarium = True\n",
    "    !sleep 5\n",
    "else:\n",
    "    local_url = 'http://localhost:54321'\n",
    "    aquarium = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python imports\n",
    "In general, NumPy and Pandas will be used for data manipulation purposes and h2o will be used for modeling tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2o Python API with specific classes\n",
    "import h2o                                        \n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator # for GBM\n",
    "from h2o.grid.grid_search import H2OGridSearch \n",
    "\n",
    "import numpy as np   # array, vector, matrix calculations\n",
    "import pandas as pd  # DataFrame handling\n",
    "import shap          # for visualizing Shapley values\n",
    "\n",
    "import matplotlib.pyplot as plt # general plotting\n",
    "pd.options.display.max_columns = 999 # enable display of all columns in notebook\n",
    "\n",
    "# display plots in-notebook\n",
    "%matplotlib inline   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start h2o\n",
    "H2o is both a library and a server. The machine learning algorithms in the library take advantage of the multithreaded and distributed architecture provided by the server to train machine learning algorithms extremely efficiently. The API for the library was imported above in cell 2, but the server still needs to be started.\n",
    "\n",
    ">The parameters used in `h2o.init` will depend on your specific environment. Regardless of how H2O is installed, if you start a cluster, you will need to ensure that it is shut down when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321/h2o . connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>12 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.26.0.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>1 month and 9 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>h2o</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>13.98 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321/h2o</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.9 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------\n",
       "H2O cluster uptime:         12 secs\n",
       "H2O cluster timezone:       Etc/UTC\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.26.0.2\n",
       "H2O cluster version age:    1 month and 9 days\n",
       "H2O cluster name:           h2o\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    13.98 Gb\n",
       "H2O cluster total cores:    16\n",
       "H2O cluster allowed cores:  16\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://localhost:54321/h2o\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.9 final\n",
       "--------------------------  ---------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init(url=local_url, max_mem_size='2G')\n",
    "h2o.remove_all()    # remove any existing data structures from h2o memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download, explore, and prepare UCI credit card default data (REWRITE)\n",
    "\n",
    "UCI credit card default data: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
    "\n",
    "The UCI credit card default data contains demographic and payment information about credit card customers in Taiwan in the year 2005. The data set contains 23 input variables: \n",
    "\n",
    "* **`LIMIT_BAL`**: Amount of given credit (NT dollar)\n",
    "* **`SEX`**: 1 = male; 2 = female\n",
    "* **`EDUCATION`**: 1 = graduate school; 2 = university; 3 = high school; 4 = others \n",
    "* **`MARRIAGE`**: 1 = married; 2 = single; 3 = others\n",
    "* **`AGE`**: Age in years \n",
    "* **`PAY_0`, `PAY_2` - `PAY_6`**: History of past payment; `PAY_0` = the repayment status in September, 2005; `PAY_2` = the repayment status in August, 2005; ...; `PAY_6` = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; ...; 8 = payment delay for eight months; 9 = payment delay for nine months and above. \n",
    "* **`BILL_AMT1` - `BILL_AMT6`**: Amount of bill statement (NT dollar). `BILL_AMNT1` = amount of bill statement in September, 2005; `BILL_AMT2` = amount of bill statement in August, 2005; ...; `BILL_AMT6` = amount of bill statement in April, 2005. \n",
    "* **`PAY_AMT1` - `PAY_AMT6`**: Amount of previous payment (NT dollar). `PAY_AMT1` = amount paid in September, 2005; `PAY_AMT2` = amount paid in August, 2005; ...; `PAY_AMT6` = amount paid in April, 2005. \n",
    "\n",
    "These 23 input variables are used to predict the target variable, whether or not a customer defaulted on their credit card bill in late 2005.\n",
    "\n",
    "Because h2o accepts both numeric and character inputs, some variables will be recoded into more transparent character values.\n",
    "\n",
    "#### Import data and clean\n",
    "The credit card default data is available as an `.xls` file. Pandas reads `.xls` files automatically, so it's used to load the credit card default data and give the prediction target a shorter name: `DEFAULT_NEXT_MONTH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import XLS file\n",
    "path = 'default_of_credit_card_clients.xls'\n",
    "data = pd.read_excel(path, skiprows=1)\n",
    "\n",
    "# remove spaces from target column name \n",
    "data = data.rename(columns={'default payment next month': 'Default'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename data fields and censor data appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.rename(columns = {\n",
    "    'LIMIT_BAL': 'CreditLimit',\n",
    "    'SEX': 'Sex',\n",
    "    'EDUCATION': 'Education',\n",
    "    'MARRIAGE': 'Marriage',\n",
    "    'AGE': 'Age',\n",
    "    'PAY_0': 'Status1',\n",
    "    'PAY_2': 'Status2',\n",
    "    'PAY_3': 'Status3',\n",
    "    'PAY_4': 'Status4',\n",
    "    'PAY_5': 'Status5',\n",
    "    'PAY_6': 'Status6',\n",
    "    'BILL_AMT1': 'BillAmt1',\n",
    "    'BILL_AMT2': 'BillAmt2',\n",
    "    'BILL_AMT3': 'BillAmt3',\n",
    "    'BILL_AMT4': 'BillAmt4',\n",
    "    'BILL_AMT5': 'BillAmt5',\n",
    "    'BILL_AMT6': 'BillAmt6',\n",
    "    'PAY_AMT1': 'PayAmt1',\n",
    "    'PAY_AMT2': 'PayAmt2',\n",
    "    'PAY_AMT3': 'PayAmt3',\n",
    "    'PAY_AMT4': 'PayAmt4',\n",
    "    'PAY_AMT5': 'PayAmt5',\n",
    "    'PAY_AMT6': 'PayAmt6',\n",
    "})\n",
    "\n",
    "## Censor data (to ensure that we are using only what we know at the time of prediction)\n",
    "##   For Status1, replace any values >= 1 with 1\n",
    "##   ...\n",
    "##   For Status6, replace any values >= 6 with 6\n",
    "\n",
    "data['Status1'][data['Status1']>1] = 1\n",
    "data['Status2'][data['Status2']>2] = 2\n",
    "data['Status3'][data['Status3']>3] = 3\n",
    "data['Status4'][data['Status4']>4] = 4\n",
    "data['Status5'][data['Status5']>5] = 5\n",
    "data['Status6'][data['Status6']>6] = 6\n",
    "\n",
    "sex_dict = {1:'M', 2:'F'}\n",
    "data['Sex'] = data['Sex'].apply(lambda i: sex_dict[i])\n",
    "\n",
    "marriage_dict = {0:'O', 1:'M', 2:'S', 3:'D'}\n",
    "data['Marriage'] = data['Marriage'].apply(lambda i: marriage_dict[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function for recoding values in the UCI credict card default data [REWRITE]\n",
    "\n",
    "These character values can be used directly in h2o decision tree models, and the function returns the original Pandas DataFrame as an h2o object, an H2OFrame. H2o models cannot run on Pandas DataFrames. They require H2OFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "data = h2o.H2OFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign modeling roles\n",
    "The shorthand name `y` is assigned to the prediction target. `X` is assigned to all other input variables in the credit card default data except the row indentifier, `ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = Default\n",
      "X = ['CreditLimit', 'Sex', 'Education', 'Marriage', 'Age', 'Status1', 'Status2', 'Status3', 'Status4', 'Status5', 'Status6', 'BillAmt1', 'BillAmt2', 'BillAmt3', 'BillAmt4', 'BillAmt5', 'BillAmt6', 'PayAmt1', 'PayAmt2', 'PayAmt3', 'PayAmt4', 'PayAmt5', 'PayAmt6']\n"
     ]
    }
   ],
   "source": [
    "# assign target and inputs for GBM\n",
    "y = 'Default'\n",
    "X = [name for name in data.columns if name not in [y, 'ID']]\n",
    "print('y =', y)\n",
    "print('X =', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure target is handled as a categorical variable\n",
    "In H2O, a numeric variable can be treated as numeric or categorical. The target variable `Default` takes on values of `0` or `1`. To ensure this numeric variable is treated as a categorical variable, the `asfactor()` function is used to explicitly declare that it is a categorical variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[y] = data[y].asfactor() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure Education and Status1-Status6 are handled as categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_categorical = ['Education', 'Status1', 'Status2', 'Status3', 'Status4', 'Status5', 'Status6']\n",
    "for name in num_to_categorical:\n",
    "    data[name] = data[name].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display descriptive statistics\n",
    "The h2o `describe()` function displays a brief description of the credit card default data. For the categorical input variables `CreditLimit`, `Sex`, `Education`, `Marriage`, and `Status1`-`Status6`, the new character values created above in cell 5 are visible. Basic descriptive statistics are displayed for numeric inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:30000\n",
      "Cols:25\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>       </th><th>ID               </th><th>CreditLimit       </th><th>Sex  </th><th>Education  </th><th>Marriage  </th><th>Age              </th><th>Status1  </th><th>Status2  </th><th>Status3  </th><th>Status4  </th><th>Status5  </th><th>Status6  </th><th>BillAmt1         </th><th>BillAmt2         </th><th>BillAmt3          </th><th>BillAmt4          </th><th>BillAmt5         </th><th>BillAmt6         </th><th>PayAmt1          </th><th>PayAmt2          </th><th>PayAmt3          </th><th>PayAmt4           </th><th>PayAmt5           </th><th>PayAmt6          </th><th>Default  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>type   </td><td>int              </td><td>int               </td><td>enum </td><td>enum       </td><td>enum      </td><td>int              </td><td>enum     </td><td>enum     </td><td>enum     </td><td>enum     </td><td>enum     </td><td>enum     </td><td>int              </td><td>int              </td><td>int               </td><td>int               </td><td>int              </td><td>int              </td><td>int              </td><td>int              </td><td>int              </td><td>int               </td><td>int               </td><td>int              </td><td>enum     </td></tr>\n",
       "<tr><td>mins   </td><td>1.0              </td><td>10000.0           </td><td>     </td><td>           </td><td>          </td><td>21.0             </td><td>         </td><td>         </td><td>         </td><td>         </td><td>         </td><td>         </td><td>-165580.0        </td><td>-69777.0         </td><td>-157264.0         </td><td>-170000.0         </td><td>-81334.0         </td><td>-339603.0        </td><td>0.0              </td><td>0.0              </td><td>0.0              </td><td>0.0               </td><td>0.0               </td><td>0.0              </td><td>         </td></tr>\n",
       "<tr><td>mean   </td><td>15000.5          </td><td>167484.32266666673</td><td>     </td><td>           </td><td>          </td><td>35.48549999999956</td><td>         </td><td>         </td><td>         </td><td>         </td><td>         </td><td>         </td><td>51223.33090000043</td><td>49179.07516666629</td><td>47013.154799999655</td><td>43262.94896666659 </td><td>40311.40096666682</td><td>38871.76039999976</td><td>5663.580500000035</td><td>5921.163499999979</td><td>5225.681499999981</td><td>4826.076866666652 </td><td>4799.387633333336 </td><td>5215.50256666665 </td><td>         </td></tr>\n",
       "<tr><td>maxs   </td><td>30000.0          </td><td>1000000.0         </td><td>     </td><td>           </td><td>          </td><td>79.0             </td><td>         </td><td>         </td><td>         </td><td>         </td><td>         </td><td>         </td><td>964511.0         </td><td>983931.0         </td><td>1664089.0         </td><td>891586.0          </td><td>927171.0         </td><td>961664.0         </td><td>873552.0         </td><td>1684259.0        </td><td>896040.0         </td><td>621000.0          </td><td>426529.0          </td><td>528666.0         </td><td>         </td></tr>\n",
       "<tr><td>sigma  </td><td>8660.398374208891</td><td>129747.66156720246</td><td>     </td><td>           </td><td>          </td><td>9.217904068090155</td><td>         </td><td>         </td><td>         </td><td>         </td><td>         </td><td>         </td><td>73635.86057552966</td><td>71173.76878252832</td><td>69349.38742703677 </td><td>64332.856133916444</td><td>60797.15577026471</td><td>59554.1075367459 </td><td>16563.28035402577</td><td>23040.87040205719</td><td>17606.96146980311</td><td>15666.159744032062</td><td>15278.305679144742</td><td>17777.46577543531</td><td>         </td></tr>\n",
       "<tr><td>zeros  </td><td>0                </td><td>0                 </td><td>     </td><td>           </td><td>          </td><td>0                </td><td>         </td><td>         </td><td>         </td><td>         </td><td>         </td><td>         </td><td>2008             </td><td>2506             </td><td>2870              </td><td>3195              </td><td>3506             </td><td>4020             </td><td>5249             </td><td>5396             </td><td>5968             </td><td>6408              </td><td>6703              </td><td>7173             </td><td>         </td></tr>\n",
       "<tr><td>missing</td><td>0                </td><td>0                 </td><td>0    </td><td>0          </td><td>0         </td><td>0                </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0                </td><td>0                </td><td>0                 </td><td>0                 </td><td>0                </td><td>0                </td><td>0                </td><td>0                </td><td>0                </td><td>0                 </td><td>0                 </td><td>0                </td><td>0        </td></tr>\n",
       "<tr><td>0      </td><td>1.0              </td><td>20000.0           </td><td>F    </td><td>2          </td><td>M         </td><td>24.0             </td><td>1        </td><td>2        </td><td>-1       </td><td>-1       </td><td>-2       </td><td>-2       </td><td>3913.0           </td><td>3102.0           </td><td>689.0             </td><td>0.0               </td><td>0.0              </td><td>0.0              </td><td>0.0              </td><td>689.0            </td><td>0.0              </td><td>0.0               </td><td>0.0               </td><td>0.0              </td><td>1        </td></tr>\n",
       "<tr><td>1      </td><td>2.0              </td><td>120000.0          </td><td>F    </td><td>2          </td><td>S         </td><td>26.0             </td><td>-1       </td><td>2        </td><td>0        </td><td>0        </td><td>0        </td><td>2        </td><td>2682.0           </td><td>1725.0           </td><td>2682.0            </td><td>3272.0            </td><td>3455.0           </td><td>3261.0           </td><td>0.0              </td><td>1000.0           </td><td>1000.0           </td><td>1000.0            </td><td>0.0               </td><td>2000.0           </td><td>1        </td></tr>\n",
       "<tr><td>2      </td><td>3.0              </td><td>90000.0           </td><td>F    </td><td>2          </td><td>S         </td><td>34.0             </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>29239.0          </td><td>14027.0          </td><td>13559.0           </td><td>14331.0           </td><td>14948.0          </td><td>15549.0          </td><td>1518.0           </td><td>1500.0           </td><td>1000.0           </td><td>1000.0            </td><td>1000.0            </td><td>5000.0           </td><td>0        </td></tr>\n",
       "<tr><td>3      </td><td>4.0              </td><td>50000.0           </td><td>F    </td><td>2          </td><td>M         </td><td>37.0             </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>46990.0          </td><td>48233.0          </td><td>49291.0           </td><td>28314.0           </td><td>28959.0          </td><td>29547.0          </td><td>2000.0           </td><td>2019.0           </td><td>1200.0           </td><td>1100.0            </td><td>1069.0            </td><td>1000.0           </td><td>0        </td></tr>\n",
       "<tr><td>4      </td><td>5.0              </td><td>50000.0           </td><td>M    </td><td>2          </td><td>M         </td><td>57.0             </td><td>-1       </td><td>0        </td><td>-1       </td><td>0        </td><td>0        </td><td>0        </td><td>8617.0           </td><td>5670.0           </td><td>35835.0           </td><td>20940.0           </td><td>19146.0          </td><td>19131.0          </td><td>2000.0           </td><td>36681.0          </td><td>10000.0          </td><td>9000.0            </td><td>689.0             </td><td>679.0            </td><td>0        </td></tr>\n",
       "<tr><td>5      </td><td>6.0              </td><td>50000.0           </td><td>M    </td><td>1          </td><td>S         </td><td>37.0             </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>64400.0          </td><td>57069.0          </td><td>57608.0           </td><td>19394.0           </td><td>19619.0          </td><td>20024.0          </td><td>2500.0           </td><td>1815.0           </td><td>657.0            </td><td>1000.0            </td><td>1000.0            </td><td>800.0            </td><td>0        </td></tr>\n",
       "<tr><td>6      </td><td>7.0              </td><td>500000.0          </td><td>M    </td><td>1          </td><td>S         </td><td>29.0             </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>367965.0         </td><td>412023.0         </td><td>445007.0          </td><td>542653.0          </td><td>483003.0         </td><td>473944.0         </td><td>55000.0          </td><td>40000.0          </td><td>38000.0          </td><td>20239.0           </td><td>13750.0           </td><td>13770.0          </td><td>0        </td></tr>\n",
       "<tr><td>7      </td><td>8.0              </td><td>100000.0          </td><td>F    </td><td>2          </td><td>S         </td><td>23.0             </td><td>0        </td><td>-1       </td><td>-1       </td><td>0        </td><td>0        </td><td>-1       </td><td>11876.0          </td><td>380.0            </td><td>601.0             </td><td>221.0             </td><td>-159.0           </td><td>567.0            </td><td>380.0            </td><td>601.0            </td><td>0.0              </td><td>581.0             </td><td>1687.0            </td><td>1542.0           </td><td>0        </td></tr>\n",
       "<tr><td>8      </td><td>9.0              </td><td>140000.0          </td><td>F    </td><td>3          </td><td>M         </td><td>28.0             </td><td>0        </td><td>0        </td><td>2        </td><td>0        </td><td>0        </td><td>0        </td><td>11285.0          </td><td>14096.0          </td><td>12108.0           </td><td>12211.0           </td><td>11793.0          </td><td>3719.0           </td><td>3329.0           </td><td>0.0              </td><td>432.0            </td><td>1000.0            </td><td>1000.0            </td><td>1000.0           </td><td>0        </td></tr>\n",
       "<tr><td>9      </td><td>10.0             </td><td>20000.0           </td><td>M    </td><td>3          </td><td>S         </td><td>35.0             </td><td>-2       </td><td>-2       </td><td>-2       </td><td>-2       </td><td>-1       </td><td>-1       </td><td>0.0              </td><td>0.0              </td><td>0.0               </td><td>0.0               </td><td>13007.0          </td><td>13912.0          </td><td>0.0              </td><td>0.0              </td><td>0.0              </td><td>13007.0           </td><td>1122.0            </td><td>0.0              </td><td>0        </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train an H2O GBM classifier\n",
    "\n",
    "#### Split data into training and test sets for early stopping\n",
    "The credit card default data is split into training and test sets to monitor and prevent overtraining. Reproducibility is also an important factor in creating trustworthy models, and randomly splitting datasets can introduce randomness in model predictions and other results. A random seed is used here to ensure the data split is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data rows = 21060, columns = 25\n",
      "Test data rows = 8940, columns = 25\n"
     ]
    }
   ],
   "source": [
    "# split into training and validation\n",
    "train, test = data.split_frame([0.7], seed=12345)\n",
    "\n",
    "# summarize split\n",
    "print('Train data rows = %d, columns = %d' % (train.shape[0], train.shape[1]))\n",
    "print('Test data rows = %d, columns = %d' % (test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish monotonicity constraints based on Pearson correlation\n",
    "In general for high-stakes situations, it's best to use constrained machine learning models. Constrained models are easier to explain and to test for disparate impact. (Constrained models may also generalize better in low signal-to-noise scenarios.) Very complex models may be nearly impossible to test for disparate impact, as they can treat similar individuals quite differently, making it possible for local instances of discrimination to occur and not be detected by disparate impact testing. \n",
    "\n",
    "This GBM model will be constrained using the pairwise Pearson correlation between the inputs and the targets to establish the direction of the monotonicity constraints. H2o expects a dictionary with input variable names as keys and values of 1 for a positive monotonic constrain, 0 for no constraint, and -1 for a negative monotonic constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Default\n",
      "CreditLimit -0.151732\n",
      "Education    0.023140\n",
      "Age          0.003770\n",
      "Status1      0.243172\n",
      "Status2      0.247066\n",
      "Status3      0.220853\n",
      "Status4      0.201297\n",
      "Status5      0.192787\n",
      "Status6      0.183549\n",
      "BillAmt1    -0.023252\n",
      "BillAmt2    -0.019563\n",
      "BillAmt3    -0.021105\n",
      "BillAmt4    -0.018092\n",
      "BillAmt5    -0.012072\n",
      "BillAmt6    -0.011350\n",
      "PayAmt1     -0.085398\n",
      "PayAmt2     -0.062419\n",
      "PayAmt3     -0.069603\n",
      "PayAmt4     -0.055825\n",
      "PayAmt5     -0.051942\n",
      "PayAmt6     -0.047694\n",
      "\n",
      " {'CreditLimit': -1, 'Education': 1, 'Age': 1, 'Status1': 1, 'Status2': 1, 'Status3': 1, 'Status4': 1, 'Status5': 1, 'Status6': 1, 'BillAmt1': -1, 'BillAmt2': -1, 'BillAmt3': -1, 'BillAmt4': -1, 'BillAmt5': -1, 'BillAmt6': -1, 'PayAmt1': -1, 'PayAmt2': -1, 'PayAmt3': -1, 'PayAmt4': -1, 'PayAmt5': -1, 'PayAmt6': -1}\n"
     ]
    }
   ],
   "source": [
    "# display Pearson correlation between inputs and targets\n",
    "print(pd.DataFrame(test.as_data_frame()[X + [y]].corr()[y]).iloc[:-1])\n",
    "\n",
    "# positive Pearson correlation w/ P_DEFAULT_NEXT_MONTH == positive constraint == 1\n",
    "# negative Pearson correlation w/ P_DEFAULT_NEXT_MONTH == negative constraint == -1\n",
    "names = list(test.as_data_frame()[X + [y]].corr()[y].index)[:-1]\n",
    "signs = list([int(i) for i in np.sign(test.as_data_frame()[X + [y]].corr()[y].values[:-1])])\n",
    "\n",
    "mc = {}\n",
    "for i, name in enumerate(names):\n",
    "    mc[name] = signs[i]\n",
    "\n",
    "print('\\n', mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train h2o GBM classifier with monotonicity constraints\n",
    "Many tuning parameters must be specified to train a GBM using h2o. Typically a grid search would be performed to identify the best parameters for a given modeling task using the `H2OGridSearch` class. For brevity's sake, a previously-discovered set of good tuning parameters are specified here. Because gradient boosting methods typically resample training data, an additional random seed is also specified for the h2o GBM using the `seed` parameter to create reproducible predictions, error rates, and variable importance values. To avoid overfitting, the `stopping_rounds` parameter is used to stop the training process after the test error fails to decrease for 5 iterations. Monotonicity constraints are passed to the GBM using the `monotone_constraints` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "H2OResponseError",
     "evalue": "ModelBuilderErrorV3  (water.exceptions.H2OModelBuilderIllegalArgumentException):\n    timestamp = 1567706451585\n    error_url = '/3/ModelBuilders/gbm'\n    msg = \"Illegal argument(s) for GBM model: dia_gbm.  Details: ERRR on field: _monotone_constraints: Invalid constraint - column 'Education' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status1' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status2' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status3' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status4' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status5' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status6' has type Enum. Only numeric columns can have monotonic constraints.\"\n    dev_msg = \"Illegal argument(s) for GBM model: dia_gbm.  Details: ERRR on field: _monotone_constraints: Invalid constraint - column 'Education' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status1' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status2' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status3' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status4' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status5' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status6' has type Enum. Only numeric columns can have monotonic constraints.\"\n    http_status = 412\n    values = {'messages': [{'_log_level': 5, '_field_name': '_keep_cross_validation_models', '_message': 'Only for cross-validation.'}, {'_log_level': 5, '_field_name': '_keep_cross_validation_predictions', '_message': 'Only for cross-validation.'}, {'_log_level': 5, '_field_name': '_keep_cross_validation_fold_assignment', '_message': 'Only for cross-validation.'}, {'_log_level': 5, '_field_name': '_fold_assignment', '_message': 'Only for cross-validation.'}, {'_log_level': 5, '_field_name': '_tweedie_power', '_message': 'Only for Tweedie Distribution.'}, {'_log_level': 5, '_field_name': '_tweedie_power', '_message': 'Tweedie power is only used for Tweedie distribution.'}, {'_log_level': 5, '_field_name': '_quantile_alpha', '_message': 'Quantile (alpha) is only used for Quantile regression.'}, {'_log_level': 5, '_field_name': '_max_after_balance_size', '_message': 'Balance classes is false, hide max_after_balance_size'}, {'_log_level': 5, '_field_name': '_max_hit_ratio_k', '_message': 'Max K-value for hit ratio is only applicable to multi-class classification problems.'}, {'_log_level': 5, '_field_name': '_max_confusion_matrix_size', '_message': 'Only for multi-class classification problems.'}, {'_log_level': 5, '_field_name': '_max_after_balance_size', '_message': 'Only used with balanced classes'}, {'_log_level': 5, '_field_name': '_class_sampling_factors', '_message': 'Class sampling factors is only applicable if balancing classes.'}, {'_log_level': 1, '_field_name': '_monotone_constraints', '_message': \"Invalid constraint - column 'Education' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'_log_level': 1, '_field_name': '_monotone_constraints', '_message': \"Invalid constraint - column 'Status1' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'_log_level': 1, '_field_name': '_monotone_constraints', '_message': \"Invalid constraint - column 'Status2' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'_log_level': 1, '_field_name': '_monotone_constraints', '_message': \"Invalid constraint - column 'Status3' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'_log_level': 1, '_field_name': '_monotone_constraints', '_message': \"Invalid constraint - column 'Status4' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'_log_level': 1, '_field_name': '_monotone_constraints', '_message': \"Invalid constraint - column 'Status5' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'_log_level': 1, '_field_name': '_monotone_constraints', '_message': \"Invalid constraint - column 'Status6' has type Enum. Only numeric columns can have monotonic constraints.\"}], 'algo': 'GBM', 'parameters': {'_train': {'name': 'py_11_sid_b100', 'type': 'Key'}, '_valid': {'name': 'py_12_sid_b100', 'type': 'Key'}, '_nfolds': 0, '_keep_cross_validation_models': True, '_keep_cross_validation_predictions': False, '_keep_cross_validation_fold_assignment': False, '_parallelize_cross_validation': True, '_auto_rebalance': True, '_seed': 12345, '_fold_assignment': 'AUTO', '_categorical_encoding': 'AUTO', '_max_categorical_levels': 10, '_distribution': 'AUTO', '_tweedie_power': 1.5, '_quantile_alpha': 0.5, '_huber_alpha': 0.9, '_ignored_columns': ['ID'], '_ignore_const_cols': True, '_weights_column': None, '_offset_column': None, '_fold_column': None, '_check_constant_response': True, '_is_cv_model': False, '_score_each_iteration': False, '_max_runtime_secs': 0.0, '_stopping_rounds': 5, '_stopping_metric': 'AUTO', '_stopping_tolerance': 0.001, '_response_column': 'Default', '_balance_classes': False, '_max_after_balance_size': 5.0, '_class_sampling_factors': None, '_max_confusion_matrix_size': 20, '_checkpoint': None, '_pretrained_autoencoder': None, '_custom_metric_func': None, '_custom_distribution_func': None, '_export_checkpoints_dir': None, '_ntrees': 150, '_max_depth': 4, '_min_rows': 10.0, '_nbins': 20, '_nbins_cats': 1024, '_min_split_improvement': 1e-05, '_histogram_type': 'AUTO', '_r2_stopping': 1.7976931348623157e+308, '_nbins_top_level': 1024, '_build_tree_one_node': False, '_score_tree_interval': 1, '_initial_score_interval': 4000, '_score_interval': 4000, '_sample_rate': 0.9, '_sample_rate_per_class': None, '_calibrate_model': False, '_calibration_frame': None, '_col_sample_rate_change_per_level': 1.0, '_col_sample_rate_per_tree': 1.0, '_learn_rate': 0.1, '_learn_rate_annealing': 1.0, '_col_sample_rate': 0.9, '_max_abs_leafnode_pred': 1.7976931348623157e+308, '_pred_noise_bandwidth': 0.0, '_monotone_constraints': [{'_key': 'CreditLimit', '_value': -1.0}, {'_key': 'Education', '_value': 1.0}, {'_key': 'Age', '_value': 1.0}, {'_key': 'Status1', '_value': 1.0}, {'_key': 'Status2', '_value': 1.0}, {'_key': 'Status3', '_value': 1.0}, {'_key': 'Status4', '_value': 1.0}, {'_key': 'Status5', '_value': 1.0}, {'_key': 'Status6', '_value': 1.0}, {'_key': 'BillAmt1', '_value': -1.0}, {'_key': 'BillAmt2', '_value': -1.0}, {'_key': 'BillAmt3', '_value': -1.0}, {'_key': 'BillAmt4', '_value': -1.0}, {'_key': 'BillAmt5', '_value': -1.0}, {'_key': 'BillAmt6', '_value': -1.0}, {'_key': 'PayAmt1', '_value': -1.0}, {'_key': 'PayAmt2', '_value': -1.0}, {'_key': 'PayAmt3', '_value': -1.0}, {'_key': 'PayAmt4', '_value': -1.0}, {'_key': 'PayAmt5', '_value': -1.0}, {'_key': 'PayAmt6', '_value': -1.0}]}, 'error_count': 14}\n    exception_msg = \"Illegal argument(s) for GBM model: dia_gbm.  Details: ERRR on field: _monotone_constraints: Invalid constraint - column 'Education' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status1' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status2' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status3' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status4' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status5' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status6' has type Enum. Only numeric columns can have monotonic constraints.\"\n    stacktrace =\n        water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: dia_gbm.  Details: ERRR on field: _monotone_constraints: Invalid constraint - column 'Education' has type Enum. Only numeric columns can have monotonic constraints.\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status1' has type Enum. Only numeric columns can have monotonic constraints.\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status2' has type Enum. Only numeric columns can have monotonic constraints.\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status3' has type Enum. Only numeric columns can have monotonic constraints.\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status4' has type Enum. Only numeric columns can have monotonic constraints.\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status5' has type Enum. Only numeric columns can have monotonic constraints.\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status6' has type Enum. Only numeric columns can have monotonic constraints.\n        water.exceptions.H2OModelBuilderIllegalArgumentException.makeFromBuilder(H2OModelBuilderIllegalArgumentException.java:20)\n        hex.ModelBuilder.trainModelOnH2ONode(ModelBuilder.java:272)\n        water.api.ModelBuilderHandler.handle(ModelBuilderHandler.java:64)\n        water.api.ModelBuilderHandler.handle(ModelBuilderHandler.java:17)\n        water.api.RequestServer.serve(RequestServer.java:462)\n        water.api.RequestServer.doGeneric(RequestServer.java:295)\n        water.api.RequestServer.doPost(RequestServer.java:221)\n        javax.servlet.http.HttpServlet.service(HttpServlet.java:755)\n        javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n        org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n        org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)\n        org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n        org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)\n        org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n        org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n        org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)\n        org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n        water.webserver.jetty8.Jetty8ServerAdapter$LoginHandler.handle(Jetty8ServerAdapter.java:119)\n        org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)\n        org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n        org.eclipse.jetty.server.Server.handle(Server.java:370)\n        org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n        org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)\n        org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:984)\n        org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1045)\n        org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:861)\n        org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:236)\n        org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)\n        org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)\n        org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n        org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n        java.lang.Thread.run(Thread.java:748)\n    parameters = {'__meta': {'schema_version': 3, 'schema_name': 'GBMParametersV3', 'schema_type': 'GBMParameters'}, 'model_id': None, 'training_frame': {'__meta': {'schema_version': 3, 'schema_name': 'FrameKeyV3', 'schema_type': 'Key<Frame>'}, 'name': 'py_11_sid_b100', 'type': 'Key<Frame>', 'URL': '/3/Frames/py_11_sid_b100'}, 'validation_frame': {'__meta': {'schema_version': 3, 'schema_name': 'FrameKeyV3', 'schema_type': 'Key<Frame>'}, 'name': 'py_12_sid_b100', 'type': 'Key<Frame>', 'URL': '/3/Frames/py_12_sid_b100'}, 'nfolds': 0, 'keep_cross_validation_models': True, 'keep_cross_validation_predictions': False, 'keep_cross_validation_fold_assignment': False, 'parallelize_cross_validation': True, 'distribution': 'AUTO', 'tweedie_power': 1.5, 'quantile_alpha': 0.5, 'huber_alpha': 0.9, 'response_column': {'__meta': {'schema_version': 3, 'schema_name': 'ColSpecifierV3', 'schema_type': 'VecSpecifier'}, 'column_name': 'Default', 'is_member_of_frames': None}, 'weights_column': None, 'offset_column': None, 'fold_column': None, 'fold_assignment': 'AUTO', 'categorical_encoding': 'AUTO', 'max_categorical_levels': 10, 'ignored_columns': ['ID'], 'ignore_const_cols': True, 'score_each_iteration': False, 'checkpoint': None, 'stopping_rounds': 5, 'max_runtime_secs': 0.0, 'stopping_metric': 'AUTO', 'stopping_tolerance': 0.001, 'custom_metric_func': None, 'custom_distribution_func': None, 'export_checkpoints_dir': None, 'balance_classes': False, 'class_sampling_factors': None, 'max_after_balance_size': 5.0, 'max_confusion_matrix_size': 20, 'max_hit_ratio_k': 0, 'ntrees': 150, 'max_depth': 4, 'min_rows': 10.0, 'nbins': 20, 'nbins_top_level': 1024, 'nbins_cats': 1024, 'r2_stopping': 1.7976931348623157e+308, 'seed': 12345, 'build_tree_one_node': False, 'sample_rate_per_class': None, 'col_sample_rate_per_tree': 1.0, 'col_sample_rate_change_per_level': 1.0, 'score_tree_interval': 1, 'min_split_improvement': 1e-05, 'histogram_type': 'AUTO', 'calibrate_model': False, 'calibration_frame': None, 'check_constant_response': True, 'learn_rate': 0.1, 'learn_rate_annealing': 1.0, 'sample_rate': 0.9, 'col_sample_rate': 0.9, 'monotone_constraints': [{'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'CreditLimit', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Education', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Age', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Status1', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Status2', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Status3', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Status4', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Status5', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Status6', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'BillAmt1', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'BillAmt2', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'BillAmt3', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'BillAmt4', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'BillAmt5', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'BillAmt6', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'PayAmt1', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'PayAmt2', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'PayAmt3', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'PayAmt4', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'PayAmt5', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'PayAmt6', 'value': -1.0}], 'max_abs_leafnode_pred': 1.7976931348623157e+308, 'pred_noise_bandwidth': 0.0}\n    messages = [{'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'keep_cross_validation_models', 'message': 'Only for cross-validation.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'keep_cross_validation_predictions', 'message': 'Only for cross-validation.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'keep_cross_validation_fold_assignment', 'message': 'Only for cross-validation.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'fold_assignment', 'message': 'Only for cross-validation.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'tweedie_power', 'message': 'Only for Tweedie Distribution.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'tweedie_power', 'message': 'Tweedie power is only used for Tweedie distribution.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'quantile_alpha', 'message': 'Quantile (alpha) is only used for Quantile regression.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_after_balance_size', 'message': 'Balance classes is false, hide max_after_balance_size'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_hit_ratio_k', 'message': 'Max K-value for hit ratio is only applicable to multi-class classification problems.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_confusion_matrix_size', 'message': 'Only for multi-class classification problems.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_after_balance_size', 'message': 'Only used with balanced classes'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'class_sampling_factors', 'message': 'Class sampling factors is only applicable if balancing classes.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'monotone_constraints', 'message': \"Invalid constraint - column 'Education' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'monotone_constraints', 'message': \"Invalid constraint - column 'Status1' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'monotone_constraints', 'message': \"Invalid constraint - column 'Status2' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'monotone_constraints', 'message': \"Invalid constraint - column 'Status3' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'monotone_constraints', 'message': \"Invalid constraint - column 'Status4' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'monotone_constraints', 'message': \"Invalid constraint - column 'Status5' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'monotone_constraints', 'message': \"Invalid constraint - column 'Status6' has type Enum. Only numeric columns can have monotonic constraints.\"}]\n    error_count = 14\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mH2OResponseError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4fd5a5e64d55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# train a GBM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# print AUC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/miniconda3/envs/h2o/lib/python3.6/site-packages/h2o/estimators/estimator_base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, training_frame, offset_column, fold_column, weights_column, validation_frame, max_runtime_secs, ignored_columns, model_id, verbose)\u001b[0m\n\u001b[1;32m    110\u001b[0m         self._train(x=x, y=y, training_frame=training_frame, offset_column=offset_column, fold_column=fold_column,\n\u001b[1;32m    111\u001b[0m                     \u001b[0mweights_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_runtime_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_runtime_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     ignored_columns=ignored_columns, model_id=model_id, verbose=verbose)\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/miniconda3/envs/h2o/lib/python3.6/site-packages/h2o/estimators/estimator_base.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, x, y, training_frame, offset_column, fold_column, weights_column, validation_frame, max_runtime_secs, ignored_columns, model_id, verbose, extend_parms_fn)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mrest_ver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_rest_version\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"_rest_version\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparms\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mmodel_builder_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST /%d/ModelBuilders/%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrest_ver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH2OJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_builder_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Model Build\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/miniconda3/envs/h2o/lib/python3.6/site-packages/h2o/h2o.py\u001b[0m in \u001b[0;36mapi\u001b[0;34m(endpoint, data, json, filename, save_to)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# type checks are performed in H2OConnection class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0m_check_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mh2oconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/miniconda3/envs/h2o/lib/python3.6/site-packages/h2o/backend/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, endpoint, data, json, filename, save_to)\u001b[0m\n\u001b[1;32m    405\u001b[0m                                     auth=self._auth, verify=self._verify_ssl_cert, proxies=self._proxies)\n\u001b[1;32m    406\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_end_transaction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/miniconda3/envs/h2o/lib/python3.6/site-packages/h2o/backend/connection.py\u001b[0m in \u001b[0;36m_process_response\u001b[0;34m(response, save_to)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;31m# Client errors (400 = \"Bad Request\", 404 = \"Not Found\", 412 = \"Precondition Failed\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m412\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH2OErrorV3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH2OModelBuilderErrorV3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mH2OResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;31m# Server errors (notably 500 = \"Server Error\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mH2OResponseError\u001b[0m: ModelBuilderErrorV3  (water.exceptions.H2OModelBuilderIllegalArgumentException):\n    timestamp = 1567706451585\n    error_url = '/3/ModelBuilders/gbm'\n    msg = \"Illegal argument(s) for GBM model: dia_gbm.  Details: ERRR on field: _monotone_constraints: Invalid constraint - column 'Education' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status1' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status2' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status3' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status4' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status5' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status6' has type Enum. Only numeric columns can have monotonic constraints.\"\n    dev_msg = \"Illegal argument(s) for GBM model: dia_gbm.  Details: ERRR on field: _monotone_constraints: Invalid constraint - column 'Education' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status1' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status2' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status3' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status4' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status5' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status6' has type Enum. Only numeric columns can have monotonic constraints.\"\n    http_status = 412\n    values = {'messages': [{'_log_level': 5, '_field_name': '_keep_cross_validation_models', '_message': 'Only for cross-validation.'}, {'_log_level': 5, '_field_name': '_keep_cross_validation_predictions', '_message': 'Only for cross-validation.'}, {'_log_level': 5, '_field_name': '_keep_cross_validation_fold_assignment', '_message': 'Only for cross-validation.'}, {'_log_level': 5, '_field_name': '_fold_assignment', '_message': 'Only for cross-validation.'}, {'_log_level': 5, '_field_name': '_tweedie_power', '_message': 'Only for Tweedie Distribution.'}, {'_log_level': 5, '_field_name': '_tweedie_power', '_message': 'Tweedie power is only used for Tweedie distribution.'}, {'_log_level': 5, '_field_name': '_quantile_alpha', '_message': 'Quantile (alpha) is only used for Quantile regression.'}, {'_log_level': 5, '_field_name': '_max_after_balance_size', '_message': 'Balance classes is false, hide max_after_balance_size'}, {'_log_level': 5, '_field_name': '_max_hit_ratio_k', '_message': 'Max K-value for hit ratio is only applicable to multi-class classification problems.'}, {'_log_level': 5, '_field_name': '_max_confusion_matrix_size', '_message': 'Only for multi-class classification problems.'}, {'_log_level': 5, '_field_name': '_max_after_balance_size', '_message': 'Only used with balanced classes'}, {'_log_level': 5, '_field_name': '_class_sampling_factors', '_message': 'Class sampling factors is only applicable if balancing classes.'}, {'_log_level': 1, '_field_name': '_monotone_constraints', '_message': \"Invalid constraint - column 'Education' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'_log_level': 1, '_field_name': '_monotone_constraints', '_message': \"Invalid constraint - column 'Status1' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'_log_level': 1, '_field_name': '_monotone_constraints', '_message': \"Invalid constraint - column 'Status2' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'_log_level': 1, '_field_name': '_monotone_constraints', '_message': \"Invalid constraint - column 'Status3' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'_log_level': 1, '_field_name': '_monotone_constraints', '_message': \"Invalid constraint - column 'Status4' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'_log_level': 1, '_field_name': '_monotone_constraints', '_message': \"Invalid constraint - column 'Status5' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'_log_level': 1, '_field_name': '_monotone_constraints', '_message': \"Invalid constraint - column 'Status6' has type Enum. Only numeric columns can have monotonic constraints.\"}], 'algo': 'GBM', 'parameters': {'_train': {'name': 'py_11_sid_b100', 'type': 'Key'}, '_valid': {'name': 'py_12_sid_b100', 'type': 'Key'}, '_nfolds': 0, '_keep_cross_validation_models': True, '_keep_cross_validation_predictions': False, '_keep_cross_validation_fold_assignment': False, '_parallelize_cross_validation': True, '_auto_rebalance': True, '_seed': 12345, '_fold_assignment': 'AUTO', '_categorical_encoding': 'AUTO', '_max_categorical_levels': 10, '_distribution': 'AUTO', '_tweedie_power': 1.5, '_quantile_alpha': 0.5, '_huber_alpha': 0.9, '_ignored_columns': ['ID'], '_ignore_const_cols': True, '_weights_column': None, '_offset_column': None, '_fold_column': None, '_check_constant_response': True, '_is_cv_model': False, '_score_each_iteration': False, '_max_runtime_secs': 0.0, '_stopping_rounds': 5, '_stopping_metric': 'AUTO', '_stopping_tolerance': 0.001, '_response_column': 'Default', '_balance_classes': False, '_max_after_balance_size': 5.0, '_class_sampling_factors': None, '_max_confusion_matrix_size': 20, '_checkpoint': None, '_pretrained_autoencoder': None, '_custom_metric_func': None, '_custom_distribution_func': None, '_export_checkpoints_dir': None, '_ntrees': 150, '_max_depth': 4, '_min_rows': 10.0, '_nbins': 20, '_nbins_cats': 1024, '_min_split_improvement': 1e-05, '_histogram_type': 'AUTO', '_r2_stopping': 1.7976931348623157e+308, '_nbins_top_level': 1024, '_build_tree_one_node': False, '_score_tree_interval': 1, '_initial_score_interval': 4000, '_score_interval': 4000, '_sample_rate': 0.9, '_sample_rate_per_class': None, '_calibrate_model': False, '_calibration_frame': None, '_col_sample_rate_change_per_level': 1.0, '_col_sample_rate_per_tree': 1.0, '_learn_rate': 0.1, '_learn_rate_annealing': 1.0, '_col_sample_rate': 0.9, '_max_abs_leafnode_pred': 1.7976931348623157e+308, '_pred_noise_bandwidth': 0.0, '_monotone_constraints': [{'_key': 'CreditLimit', '_value': -1.0}, {'_key': 'Education', '_value': 1.0}, {'_key': 'Age', '_value': 1.0}, {'_key': 'Status1', '_value': 1.0}, {'_key': 'Status2', '_value': 1.0}, {'_key': 'Status3', '_value': 1.0}, {'_key': 'Status4', '_value': 1.0}, {'_key': 'Status5', '_value': 1.0}, {'_key': 'Status6', '_value': 1.0}, {'_key': 'BillAmt1', '_value': -1.0}, {'_key': 'BillAmt2', '_value': -1.0}, {'_key': 'BillAmt3', '_value': -1.0}, {'_key': 'BillAmt4', '_value': -1.0}, {'_key': 'BillAmt5', '_value': -1.0}, {'_key': 'BillAmt6', '_value': -1.0}, {'_key': 'PayAmt1', '_value': -1.0}, {'_key': 'PayAmt2', '_value': -1.0}, {'_key': 'PayAmt3', '_value': -1.0}, {'_key': 'PayAmt4', '_value': -1.0}, {'_key': 'PayAmt5', '_value': -1.0}, {'_key': 'PayAmt6', '_value': -1.0}]}, 'error_count': 14}\n    exception_msg = \"Illegal argument(s) for GBM model: dia_gbm.  Details: ERRR on field: _monotone_constraints: Invalid constraint - column 'Education' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status1' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status2' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status3' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status4' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status5' has type Enum. Only numeric columns can have monotonic constraints.\\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status6' has type Enum. Only numeric columns can have monotonic constraints.\"\n    stacktrace =\n        water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: dia_gbm.  Details: ERRR on field: _monotone_constraints: Invalid constraint - column 'Education' has type Enum. Only numeric columns can have monotonic constraints.\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status1' has type Enum. Only numeric columns can have monotonic constraints.\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status2' has type Enum. Only numeric columns can have monotonic constraints.\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status3' has type Enum. Only numeric columns can have monotonic constraints.\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status4' has type Enum. Only numeric columns can have monotonic constraints.\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status5' has type Enum. Only numeric columns can have monotonic constraints.\nERRR on field: _monotone_constraints: Invalid constraint - column 'Status6' has type Enum. Only numeric columns can have monotonic constraints.\n        water.exceptions.H2OModelBuilderIllegalArgumentException.makeFromBuilder(H2OModelBuilderIllegalArgumentException.java:20)\n        hex.ModelBuilder.trainModelOnH2ONode(ModelBuilder.java:272)\n        water.api.ModelBuilderHandler.handle(ModelBuilderHandler.java:64)\n        water.api.ModelBuilderHandler.handle(ModelBuilderHandler.java:17)\n        water.api.RequestServer.serve(RequestServer.java:462)\n        water.api.RequestServer.doGeneric(RequestServer.java:295)\n        water.api.RequestServer.doPost(RequestServer.java:221)\n        javax.servlet.http.HttpServlet.service(HttpServlet.java:755)\n        javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n        org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n        org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)\n        org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n        org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)\n        org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n        org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n        org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)\n        org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n        water.webserver.jetty8.Jetty8ServerAdapter$LoginHandler.handle(Jetty8ServerAdapter.java:119)\n        org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)\n        org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n        org.eclipse.jetty.server.Server.handle(Server.java:370)\n        org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n        org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)\n        org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:984)\n        org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1045)\n        org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:861)\n        org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:236)\n        org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)\n        org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)\n        org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n        org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n        java.lang.Thread.run(Thread.java:748)\n    parameters = {'__meta': {'schema_version': 3, 'schema_name': 'GBMParametersV3', 'schema_type': 'GBMParameters'}, 'model_id': None, 'training_frame': {'__meta': {'schema_version': 3, 'schema_name': 'FrameKeyV3', 'schema_type': 'Key<Frame>'}, 'name': 'py_11_sid_b100', 'type': 'Key<Frame>', 'URL': '/3/Frames/py_11_sid_b100'}, 'validation_frame': {'__meta': {'schema_version': 3, 'schema_name': 'FrameKeyV3', 'schema_type': 'Key<Frame>'}, 'name': 'py_12_sid_b100', 'type': 'Key<Frame>', 'URL': '/3/Frames/py_12_sid_b100'}, 'nfolds': 0, 'keep_cross_validation_models': True, 'keep_cross_validation_predictions': False, 'keep_cross_validation_fold_assignment': False, 'parallelize_cross_validation': True, 'distribution': 'AUTO', 'tweedie_power': 1.5, 'quantile_alpha': 0.5, 'huber_alpha': 0.9, 'response_column': {'__meta': {'schema_version': 3, 'schema_name': 'ColSpecifierV3', 'schema_type': 'VecSpecifier'}, 'column_name': 'Default', 'is_member_of_frames': None}, 'weights_column': None, 'offset_column': None, 'fold_column': None, 'fold_assignment': 'AUTO', 'categorical_encoding': 'AUTO', 'max_categorical_levels': 10, 'ignored_columns': ['ID'], 'ignore_const_cols': True, 'score_each_iteration': False, 'checkpoint': None, 'stopping_rounds': 5, 'max_runtime_secs': 0.0, 'stopping_metric': 'AUTO', 'stopping_tolerance': 0.001, 'custom_metric_func': None, 'custom_distribution_func': None, 'export_checkpoints_dir': None, 'balance_classes': False, 'class_sampling_factors': None, 'max_after_balance_size': 5.0, 'max_confusion_matrix_size': 20, 'max_hit_ratio_k': 0, 'ntrees': 150, 'max_depth': 4, 'min_rows': 10.0, 'nbins': 20, 'nbins_top_level': 1024, 'nbins_cats': 1024, 'r2_stopping': 1.7976931348623157e+308, 'seed': 12345, 'build_tree_one_node': False, 'sample_rate_per_class': None, 'col_sample_rate_per_tree': 1.0, 'col_sample_rate_change_per_level': 1.0, 'score_tree_interval': 1, 'min_split_improvement': 1e-05, 'histogram_type': 'AUTO', 'calibrate_model': False, 'calibration_frame': None, 'check_constant_response': True, 'learn_rate': 0.1, 'learn_rate_annealing': 1.0, 'sample_rate': 0.9, 'col_sample_rate': 0.9, 'monotone_constraints': [{'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'CreditLimit', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Education', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Age', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Status1', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Status2', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Status3', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Status4', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Status5', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'Status6', 'value': 1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'BillAmt1', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'BillAmt2', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'BillAmt3', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'BillAmt4', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'BillAmt5', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'BillAmt6', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'PayAmt1', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'PayAmt2', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'PayAmt3', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'PayAmt4', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'PayAmt5', 'value': -1.0}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyValueV3', 'schema_type': 'KeyValue'}, 'key': 'PayAmt6', 'value': -1.0}], 'max_abs_leafnode_pred': 1.7976931348623157e+308, 'pred_noise_bandwidth': 0.0}\n    messages = [{'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'keep_cross_validation_models', 'message': 'Only for cross-validation.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'keep_cross_validation_predictions', 'message': 'Only for cross-validation.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'keep_cross_validation_fold_assignment', 'message': 'Only for cross-validation.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'fold_assignment', 'message': 'Only for cross-validation.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'tweedie_power', 'message': 'Only for Tweedie Distribution.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'tweedie_power', 'message': 'Tweedie power is only used for Tweedie distribution.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'quantile_alpha', 'message': 'Quantile (alpha) is only used for Quantile regression.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_after_balance_size', 'message': 'Balance classes is false, hide max_after_balance_size'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_hit_ratio_k', 'message': 'Max K-value for hit ratio is only applicable to multi-class classification problems.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_confusion_matrix_size', 'message': 'Only for multi-class classification problems.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_after_balance_size', 'message': 'Only used with balanced classes'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'class_sampling_factors', 'message': 'Class sampling factors is only applicable if balancing classes.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'monotone_constraints', 'message': \"Invalid constraint - column 'Education' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'monotone_constraints', 'message': \"Invalid constraint - column 'Status1' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'monotone_constraints', 'message': \"Invalid constraint - column 'Status2' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'monotone_constraints', 'message': \"Invalid constraint - column 'Status3' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'monotone_constraints', 'message': \"Invalid constraint - column 'Status4' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'monotone_constraints', 'message': \"Invalid constraint - column 'Status5' has type Enum. Only numeric columns can have monotonic constraints.\"}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'monotone_constraints', 'message': \"Invalid constraint - column 'Status6' has type Enum. Only numeric columns can have monotonic constraints.\"}]\n    error_count = 14\n"
     ]
    }
   ],
   "source": [
    "# initialize GBM model\n",
    "model = H2OGradientBoostingEstimator(ntrees=150,              # maximum 150 trees in GBM\n",
    "                                     max_depth=4,             # trees can have maximum depth of 4\n",
    "                                     sample_rate=0.9,         # use 90% of rows in each iteration (tree)\n",
    "                                     col_sample_rate=0.9,     # use 90% of variables in each iteration (tree)\n",
    "                                     #balance_classes=True,   # sample to balance 0/1 distribution of target - can help LOCO\n",
    "                                     stopping_rounds=5,       # stop if validation error does not decrease for 5 iterations (trees)\n",
    "                                     score_tree_interval=1,   # for reproducibility, set higher for bigger data\n",
    "                                     monotone_constraints=mc, # enforces monotonicity wrt each input\n",
    "                                     model_id='dia_gbm',      # for locating the model in Flow UI \n",
    "                                     seed=12345)              # for reproducibility\n",
    "\n",
    "# train a GBM model\n",
    "model.train(y=y, x=X, training_frame=train, validation_frame=test)\n",
    "\n",
    "# print AUC\n",
    "print('GBM Test AUC = %.2f' % model.auc(valid=True))\n",
    "\n",
    "# uncomment to see model details\n",
    "# print(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Shapley variable importance\n",
    "Shapley values are a locally-accurate and globally consistent variable importance metric. Instead of relying on traditional single-value variable importance measures, local Shapley values for each input will be calculated and aggregated below to get a more holistic and consisent measurement for the global importance of each input variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributions = model.predict_contributions(test)\n",
    "contributions_matrix = contributions.as_data_frame().values\n",
    "shap_values = contributions_matrix[:,:-1]\n",
    "shap.summary_plot(shap_values, X, plot_type='bar', color='royalblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repayment `PAY_*` variables appear to be most important in this model along with a credit limit, `LIMIT_BAL`.\n",
    "\n",
    "(In a more realistic scenario, credit limit would likely not be used as a model input because it could cause target leakage. It's used here in this small data example to improve model accuracy.)\n",
    "\n",
    "# 3. Select a Probability Cutoff by Maximizing F1 Statistic\n",
    "\n",
    "#### Bind model predictions to test data for further calculations\n",
    "Model predictions will be used in numerous subsequent calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbind predictions to training frame\n",
    "# give them a nice name\n",
    "yhat = 'p_DEFAULT_NEXT_MONTH'\n",
    "preds1 = test['ID'].cbind(model.predict(test).drop(['predict', 'p0']))\n",
    "preds1.columns = ['ID', yhat]\n",
    "test_yhat = test.cbind(preds1[yhat]).as_data_frame()\n",
    "test_yhat.reset_index(drop=True, inplace=True) # necessary for later searches/joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to calculate PR-AUC Curve\n",
    "\n",
    "Predictive models often produce probabilities, not decisions. So to make a decision with a model-generated predicted probability for any one customer, you must specify a numeric cutoff above which we say a customer will default and below which we say they will not default. Cutoffs play a crucial role in DIA as they impact the underlying measurements used to calculate diparity. In fact, tuning cutoffs carefully is a potential remdiation tactic for any discovered disparity. There are many accepted ways to select a cutoff (besides simply using 0.5) and in this notebook the cutoff will be selected by striking a balance between the model's recall (true positive rate) and it's precision using the F1 statistic. Using precision and recall to select a cutoff is sometimes seen as more robust to imbalanced data than the standard ROC approach. Maximizing F1 typically results in a good balance between sensitivity and precision. \n",
    "\n",
    "Selecting a cutoff can be done intervactively with h2o Flow as well. Enter the url: http://localhost:54321/flow/index.html (or your H2O Connection URL displayed in cell 2) into your browser. Select `Models` -> `List all models` -> `dia_gbm` and you should see an interactive ROC curve where you can pick your own cutoff.\n",
    "\n",
    "To learn more about confusion matrices see: https://en.wikipedia.org/wiki/Confusion_matrix.\n",
    "\n",
    "**Note:** h2o supports `pr_auc` directly: http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/metrics.html#h2o.model.metrics_base.MetricsBase.pr_auc. This utility function is used here to give more control over the plotting of the PR curve below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prauc(frame, y, yhat, pos=1, neg=0, res=0.01):\n",
    "    \n",
    "    \"\"\" Calculates precision, recall, and f1 for a pandas dataframe of y and yhat values.\n",
    "    \n",
    "    Args:\n",
    "        frame: Pandas dataframe of actual (y) and predicted (yhat) values.\n",
    "        y: Name of actual value column.\n",
    "        yhat: Name of predicted value column.\n",
    "        pos: Primary target value, default 1.\n",
    "        neg: Secondary target value, default 0.\n",
    "        res: Resolution by which to loop through cutoffs, default 0.01.\n",
    "    \n",
    "    Returns:\n",
    "        Pandas dataframe of precision, recall, and f1 values. \n",
    "    \"\"\"\n",
    "    \n",
    "    frame_ = frame.copy(deep=True) # don't destroy original data\n",
    "    dname = 'd_' + str(y) # column for predicted decisions\n",
    "    eps = 1e-20 # for safe numerical operations\n",
    "    \n",
    "    # init p-r roc frame\n",
    "    prauc_frame = pd.DataFrame(columns=['cutoff', 'recall', 'precision', 'f1'])\n",
    "    \n",
    "    # loop through cutoffs to create p-r roc frame\n",
    "    for cutoff in np.arange(0, 1 + res, res):\n",
    "\n",
    "        # binarize decision to create confusion matrix values\n",
    "        frame_[dname] = np.where(frame_[yhat] > cutoff , 1, 0)\n",
    "        \n",
    "        # calculate confusion matrix values\n",
    "        tp = frame_[(frame_[dname] == pos) & (frame_[y] == pos)].shape[0]\n",
    "        fp = frame_[(frame_[dname] == pos) & (frame_[y] == neg)].shape[0]\n",
    "        tn = frame_[(frame_[dname] == neg) & (frame_[y] == neg)].shape[0]\n",
    "        fn = frame_[(frame_[dname] == neg) & (frame_[y] == pos)].shape[0]\n",
    "        \n",
    "        # calculate precision, recall, and f1\n",
    "        recall = (tp + eps)/((tp + fn) + eps)\n",
    "        precision = (tp + eps)/((tp + fp) + eps)\n",
    "        f1 = 2/((1/(recall + eps)) + (1/(precision + eps)))\n",
    "        \n",
    "        # add new values to frame\n",
    "        prauc_frame = prauc_frame.append({'cutoff': cutoff,\n",
    "                                          'recall': recall,\n",
    "                                          'precision': precision,\n",
    "                                          'f1': f1}, \n",
    "                                          ignore_index=True)\n",
    "    \n",
    "    # housekeeping\n",
    "    del frame_\n",
    "    \n",
    "    return prauc_frame\n",
    "        \n",
    "prauc_frame = get_prauc(test_yhat, 'DEFAULT_NEXT_MONTH', 'p_DEFAULT_NEXT_MONTH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select best cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cut = prauc_frame.loc[prauc_frame['f1'].idxmax(), 'cutoff'] # Find cutoff w/ max F1\n",
    "### !!! UNCOMMENT LINES BELOW TO REMEDIATE MINOR FAIRNESS ISSUES !!! ###\n",
    "# best_cut = 0.3  # min threshold with overall fairness\n",
    "# best_cut = 0.46 # max accuracy\n",
    "# best_cut = 0.35 # max MCC\n",
    "print('%.2f' % best_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot PR-AUC Curve\n",
    "\n",
    "An area under the curve for precision and recall (PR-AUC) plot is a typical way to visualize recall and precision for a predictive model. The F1 statistic is the harmonic mean of recall and precision, and we can visualize where F1 is maximized on with the AUC-PR curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot P-R AUC w/ best cutoff\n",
    "title_ = 'P-R Curve: Cutoff = ' + str(best_cut)\n",
    "ax = prauc_frame.plot(x='recall', y='precision', kind='scatter', title=title_, xlim=[0,1])\n",
    "_ = ax.axvline(best_cut, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In h2o Flow a traditional ROC curve is displayed, in which true positive rate is plotted against false positive rate. You can still use h2o Flow and the traditional ROC curve to verify that F1 is maximized at a probability cutoff of ~ 0.27.\n",
    "\n",
    "# 4. Report Raw Confusion Matrices\n",
    "\n",
    "The basic DIA procedure in this notebook is based on measurements found commonly in confusion matrices, so confusion matrices are calculated as a precursor to DIA and to provide a basic summary of the GBM's behavior in general and across men and women.\n",
    "\n",
    "#### Function to print confusion matrices by an input variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(frame, y, yhat, by=None, level=None, cutoff=0.5):\n",
    "\n",
    "    \"\"\" Creates confusion matrix from pandas dataframe of y and yhat values, can be sliced \n",
    "        by a variable and level.\n",
    "    \n",
    "    Args:\n",
    "        frame: Pandas dataframe of actual (y) and predicted (yhat) values.\n",
    "        y: Name of actual value column.\n",
    "        yhat: Name of predicted value column.\n",
    "        by: By variable to slice frame before creating confusion matrix, default None.\n",
    "        level: Value of by variable to slice frame before creating confusion matrix, default None.\n",
    "        cutoff: Cutoff threshold for confusion matrix, default 0.5. \n",
    "\n",
    "    Returns:\n",
    "        Confusion matrix as pandas dataframe. \n",
    "    \"\"\"\n",
    "    \n",
    "    # determine levels of target (y) variable\n",
    "    # sort for consistency\n",
    "    level_list = list(frame[y].unique())\n",
    "    level_list.sort(reverse=True)\n",
    "\n",
    "    # init confusion matrix\n",
    "    cm_frame = pd.DataFrame(columns=['actual: ' +  str(i) for i in level_list], \n",
    "                            index=['predicted: ' + str(i) for i in level_list])\n",
    "    \n",
    "    # don't destroy original data\n",
    "    frame_ = frame.copy(deep=True)\n",
    "    \n",
    "    # convert numeric predictions to binary decisions using cutoff\n",
    "    dname = 'd_' + str(y)\n",
    "    frame_[dname] = np.where(frame_[yhat] > cutoff , 1, 0)\n",
    "    \n",
    "    # slice frame\n",
    "    if (by is not None) & (level is not None):\n",
    "        frame_ = frame_[frame[by] == level]\n",
    "    \n",
    "    # calculate size of each confusion matrix value\n",
    "    for i, lev_i in enumerate(level_list):\n",
    "        for j, lev_j in enumerate(level_list):\n",
    "            cm_frame.iat[j, i] = frame_[(frame_[y] == lev_i) & (frame_[dname] == lev_j)].shape[0]\n",
    "            # i, j vs. j, i nasty little bug ... updated 8/30/19\n",
    "    \n",
    "    # output results\n",
    "    if by is None:\n",
    "        print('Confusion matrix:')\n",
    "    else:\n",
    "        print('Confusion matrix by ' + by + '=' + level)\n",
    "    \n",
    "    return cm_frame\n",
    "    \n",
    "    \n",
    "get_confusion_matrix(test_yhat, 'DEFAULT_NEXT_MONTH', 'p_DEFAULT_NEXT_MONTH', cutoff=best_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general confusion matrix shows that the GBM is more accurate than not because the true positive and true negative cells contain the largest values by far. But the GBM seems to make a larger number of type II errors or false negative predictions. False negatives can be a disparity issue, because for complex reasons, many credit scoring and other models tend to over-estimate the likelihood of non-reference groups - typically people other than white males - to default. This is both a sociological fairness problem and a financial problem if an unpriviledged group is not recieving the credit they deserve, in favor of undeserving white males. Deserving people miss out on potentially life-changing credit and lenders incur large write-off costs.\n",
    "\n",
    "#### Report confusion matrices by `SEX`\n",
    "\n",
    "The only values for `SEX` in the dataset are `female` and `male`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_levels = list(test_yhat['SEX'].unique())\n",
    "sex_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix for `SEX = male`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_cm = get_confusion_matrix(test_yhat, 'DEFAULT_NEXT_MONTH', 'p_DEFAULT_NEXT_MONTH', by='SEX', level='male', cutoff=best_cut)\n",
    "male_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix for `SEX = female`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_cm = get_confusion_matrix(test_yhat, 'DEFAULT_NEXT_MONTH', 'p_DEFAULT_NEXT_MONTH', by='SEX', level='female', cutoff=best_cut)\n",
    "female_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both confusion matrices reflect the global confusion matrix: more accurate than not with a larger number of false positive predictions (type I errors) than false negative predictions (type II errors).\n",
    "\n",
    "# 6. Disparate Impact Analysis (DIA)\n",
    "\n",
    "To perform the following basic DIA many different values from the confusion matrices reflecting different prediction behavior are calculated. These metrics essentially help us understand the GBM's overall performance and how it behaves when predicting:\n",
    "\n",
    "* Default correctly\n",
    "* Non-default correctly\n",
    "* Default incorrectly (type I errors)\n",
    "* Non-default incorrectly (type II errors)\n",
    "\n",
    "In a real-life lending scenario, type I errors essentially amount to false accusations of financial impropriety and type II errors result in awarding loans to undeserving customers. Both types of errors can be costly to the lender too. Type I errors likely result in lost interest and fees. Type II errors often result in write-offs.\n",
    "\n",
    "#### Dictionary of metrics used to assess disparity and parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent metrics as dictionary for use later\n",
    "metric_dict = {\n",
    "\n",
    "#### overall performance\n",
    "#'Prevalence': '(tp + fn) / (tp + tn +fp + fn)', # how much default actually happens for this group\n",
    "'Adverse Impact': '(tp + fp) / (tp + tn + fp + fn)', # how often the model predicted default for each group   \n",
    "'Accuracy':       '(tp + tn) / (tp + tn + fp + fn)', # how often the model predicts default and non-default correctly for this group\n",
    "\n",
    "#### predicting default will happen\n",
    "# (correctly)\n",
    "'True Positive Rate': 'tp / (tp + fn)',  # out of the people in the group *that did* default, how many the model predicted *correctly* would default              \n",
    "'Precision':          'tp / (tp + fp)',  # out of the people in the group the model *predicted* would default, how many the model predicted *correctly* would default\n",
    "\n",
    "#### predicting default won't happen\n",
    "# (correctly)\n",
    "'Specificity':              'tn / (tn + fp)', # out of the people in the group *that did not* default, how many the model predicted *correctly* would not default\n",
    "'Negative Predicted Value': 'tn / (tn + fn)', # out of the people in the group the model *predicted* would not default, how many the model predicted *correctly* would not default  \n",
    "\n",
    "#### analyzing errors - type I\n",
    "# false accusations \n",
    "'False Positive Rate':  'fp / (tn + fp)', # out of the people in the group *that did not* default, how many the model predicted *incorrectly* would default\n",
    "'False Discovery Rate': 'fp / (tp + fp)', # out of the people in the group the model *predicted* would default, how many the model predicted *incorrectly* would default\n",
    "\n",
    "#### analyzing errors - type II\n",
    "# costly ommisions\n",
    "'False Negative Rate': 'fn / (tp + fn)', # out of the people in the group *that did* default, how many the model predicted *incorrectly* would not default\n",
    "'False Omissions Rate':'fn / (tn + fn)'  # out of the people in the group the model *predicted* would not default, how many the model predicted *incorrectly* would not default\n",
    "}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility function to translate metrics into Pandas statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small utility function\n",
    "# translates abbreviated metric expressions into executable Python statements\n",
    "\n",
    "def cm_exp_parser(expression):\n",
    "    \n",
    "    # tp | fp       cm_dict[level].iat[0, 0] | cm_dict[level].iat[0, 1]\n",
    "    # -------  ==>  --------------------------------------------\n",
    "    # fn | tn       cm_dict[level].iat[1, 0] | cm_dict[level].iat[1, 1]\n",
    "\n",
    "    expression = expression.replace('tp', 'cm_dict[level].iat[0, 0]')\\\n",
    "                           .replace('fp', 'cm_dict[level].iat[0, 1]')\\\n",
    "                           .replace('fn', 'cm_dict[level].iat[1, 0]')\\\n",
    "                           .replace('tn', 'cm_dict[level].iat[1, 1]')\n",
    "\n",
    "    return expression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate and report metrics\n",
    "This nested loop calculates all the metrics defined above for men and women."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dict of confusion matrices and corresponding rows of dataframe\n",
    "cm_dict = {'male': male_cm, \n",
    "           'female': female_cm} \n",
    "\n",
    "metrics_frame = pd.DataFrame(index=sex_levels) # frame for metrics\n",
    "\n",
    "# nested loop through:\n",
    "# - sex levels\n",
    "# - metrics \n",
    "for level in sex_levels:\n",
    "    for metric in metric_dict.keys():\n",
    "              \n",
    "        # parse metric expressions into executable pandas statements\n",
    "        expression = cm_exp_parser(metric_dict[metric])\n",
    "\n",
    "        # dynamically evaluate metrics to avoid code duplication\n",
    "        metrics_frame.loc[level, metric] = eval(expression)  \n",
    "\n",
    "# display results                \n",
    "metrics_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From eyeballing the raw metrics it appears that the model is treating men and women roughly similarly as groups. (These metrics do not indicate very much about specific, individual cases of bias, but only give information for the groups as a whole.)\n",
    "\n",
    "#### Plot false positive rate by `SEX`\n",
    "\n",
    "Because the confusion matrices indicated there might be a problem with false positives, that metric will be examined more closely than others. (Of course other metrics are important from a fairness and business perspective. For an excellent in-depth discussion and example regarding numerous disparity metrics, see: https://github.com/dssg/aequitas/blob/master/docs/source/examples/compas_demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = metrics_frame['False Positive Rate'].plot(kind='bar', color='b', title='False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate and report disparity\n",
    "To calculate disparity we compare the confusion matrix for each sex to the metrics for a user-defined reference level and to user-defined thresholds. In this case, we take the class of people who seem most priviledged as the reference level, i.e. `SEX = male`. (Usually the reference level would be `race = white` or `sex = male`.) According to the four-fifths rule (https://en.wikipedia.org/wiki/Disparate_impact#The_80%_rule) thresholds are set such that metrics 20% lower or higher than the reference level metric will be flagged as disparate. Technically, the four-fifths rule only applies to the adverse impact ratio, or \"adverse impact disparity\" below, but it will be applied to all other displayed metric as a rule of thumb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_level = 'male' # user-defined reference level\n",
    "\n",
    "parity_threshold_low = 0.8    # user-defined low threshold value\n",
    "parity_threshold_hi = 1.25    # user-defined high threshold value\n",
    "\n",
    "# init frame to store disparity measures\n",
    "disp_frame = pd.DataFrame(index=sex_levels)\n",
    "\n",
    "# compare all metrics to reference level\n",
    "disp_frame = metrics_frame/metrics_frame.loc[ref_level, :]\n",
    "\n",
    "# change column names\n",
    "disp_frame.columns=[col + ' Disparity' for col in metrics_frame.columns]\n",
    "\n",
    "# small utility function to format pandas table output\n",
    "def disparate_red(val):\n",
    "    \n",
    "    color = 'blue' if (parity_threshold_low < val < parity_threshold_hi) else 'red'\n",
    "    return 'color: %s' % color \n",
    "\n",
    "# display results\n",
    "disp_frame.style.applymap(disparate_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the selected thresholds, the GBM appears to have only one value for metrics that is low out-of-range, false positive rate. According to the more common and accepted test of `0.8 < Adverse Impact Disparity < 1.25` this model could be considered to be free from disparate impact w.r.t to `SEX`. \n",
    "\n",
    "#### Plot false positive rate disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = disp_frame['False Positive Rate Disparity'].plot(kind='bar', color='b', title='False Positive Rate Disparity')\n",
    "_ = ax.axhline(parity_threshold_low, color='r', linestyle='--')\n",
    "_ = ax.axhline(parity_threshold_hi, color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess and report parity\n",
    "A binary indication of parity for metrics is reported by simply checking whether disparity values are within the user-defined thresholds. Further parity indicators are defined as combinations of other disparity values:\n",
    "\n",
    "* Type I Parity: Fairness in both FDR Parity and FPR Parity\n",
    "* Type II Parity: Fairness in both FOR Parity and FNR Parity\n",
    "* Equalized Odds: Fairness in both FPR Parity and TPR Parity\n",
    "* Supervised Fairness: Fairness in both Type I and Type II Parity\n",
    "* Overall Fairness: Fairness across all parities for all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parity checks\n",
    "# low_threshold (0.8) < *_metric/white_metric < (high_threshold) 1.25 => parity, else disparity \n",
    "\n",
    "# init frame for parity\n",
    "par_frame = pd.DataFrame(index=sex_levels, \n",
    "                         columns=[col + ' Parity' for col in metrics_frame.columns])\n",
    "# nested loop through: \n",
    "# - races\n",
    "# - disparity metrics\n",
    "for i, _ in enumerate(sex_levels):\n",
    "    for j, _ in enumerate(par_frame.columns):\n",
    "        par_frame.iat[i, j] = (parity_threshold_low < disp_frame.iat[i, j] < parity_threshold_hi)\n",
    "\n",
    "# add overall parity checks\n",
    "# Type I Parity: Fairness in both FDR Parity and FPR Parity\n",
    "# Type II Parity: Fairness in both FOR Parity and FNR Parity\n",
    "# Equalized Odds: Fairness in both FPR Parity and TPR Parity\n",
    "# Supervised Fairness: Fairness in both Type I and Type II Parity\n",
    "# Overall Fairness: Fairness across all parities for all metrics\n",
    "par_frame['Type I Parity'] = (par_frame['False Discovery Rate Parity']) & (par_frame['False Positive Rate Parity'])\n",
    "par_frame['Type II Parity'] = (par_frame['False Omissions Rate Parity']) & (par_frame['False Negative Rate Parity'])\n",
    "par_frame['Equalized Odds'] = (par_frame['False Positive Rate Parity']) & (par_frame['True Positive Rate Parity'])\n",
    "par_frame['Supervised Fairness'] = (par_frame['Type I Parity']) & (par_frame['Type II Parity'])\n",
    "par_frame['Overall Fairness'] = par_frame.all(axis='columns')\n",
    "par_frame.loc['all', :] = par_frame.all(axis='index')\n",
    "    \n",
    "# small utility function to format pandas table output    \n",
    "def color_false_red(val):\n",
    "\n",
    "    color = 'red' if not val else 'blue'\n",
    "    return 'color: %s' % color \n",
    "    \n",
    "par_frame.style.applymap(color_false_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, the GBM appears to have adverse impact parity, and many other types of positive parity characteristics. However, the model is still suffering from a minor disparity problem due to it's propensity to make false positive predictions. To address such disparate impact users could tune the GBM cutoff or regularization, could try new methods for reweighing data prior to model training, try new modeling methods specifically designed for fairness, or post-process the decisions. Before attempting remediation, local or individual fairness will also be investigated.\n",
    "\n",
    "## 7. Investigate Individual Disparity \n",
    "\n",
    "In nonlinear models, similar people can be treated differenly by the model, so even if the model is mostly fair for most kinds of people, there could still be people the model treated unfairly. This could occur for multiple reasons, including the functional form of the learned model or because different variables are combined by the model to represent strong signals. If a variable is important in a dataset, model, or problem domain it's likely that a nonlinear model will find combinations of other variables to act as proxies for the problematic variable -- potentially even different combinations for different rows of data! So by simply testing for group fairness, you may miss instances of individual discrimination.\n",
    "\n",
    "#### Augment predictions with decisions and logloss residuals for women with false positive predictions\n",
    "In this notebook, residuals for false positive predictions for women will be examined in an attempt to locate any individual instances of model discrimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yhat_female = test_yhat[test_yhat['SEX'] == 'female'].copy(deep=True)\n",
    "\n",
    "\n",
    "test_yhat_female['d_DEFAULT_NEXT_MONTH'] = 0\n",
    "test_yhat_female.loc[test_yhat_female[yhat] > best_cut, 'd_DEFAULT_NEXT_MONTH'] = 1\n",
    "\n",
    "test_yhat_female['r_DEFAULT_NEXT_MONTH'] = -test_yhat_female[y]*np.log(test_yhat_female[yhat]) -\\\n",
    "                       (1 - test_yhat_female[y])*np.log(1 - test_yhat_female[yhat]) \n",
    "    \n",
    "test_yhat_female_fp = test_yhat_female[(test_yhat_female[y] == 0) &\\\n",
    "                                       (test_yhat_female['d_DEFAULT_NEXT_MONTH'] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot logloss residuals\n",
    "There are clear high-magnitude outliers in the logloss residuals for women who were issued false positive predictions. Given that those people were the people the model was most wrong about, those predictions will be checked first, just in case something very strange happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "fig, ax_ = plt.subplots(figsize=(8, 8))         \n",
    "\n",
    "ax_.plot(test_yhat_female_fp[yhat],\n",
    "         test_yhat_female_fp['r_DEFAULT_NEXT_MONTH'],\n",
    "         marker='o', linestyle='', alpha=0.3)\n",
    "\n",
    "# annotate plot\n",
    "_ = plt.xlabel(yhat)\n",
    "_ = plt.ylabel('r_DEFAULT_NEXT_MONTH')\n",
    "_ = ax_.legend(loc=4)\n",
    "_ = plt.title('Logloss Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine top 3 outlying individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yhat_female_fp.sort_values(by='r_DEFAULT_NEXT_MONTH', ascending=False).head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the top 3 highest residual predictions, it's clear these were customers with a horrible payment track record that somehow were able to make a last-minute payment. Although it's not possible to say they were not discrimated against by the model, it seems very clear that the model was justified to issue default predictions for these women. \n",
    "\n",
    "#### Adverse Action Notices \n",
    "Anyone that is denied further credit due to this model in the U.S. must be given reasons why. Shapley values provide a mechanism to rank the contributions of input variables to any given model decision and may be suitable for adverse action notice generation. Because the highest residual false positive female seems like a probable candidate to be rejected for further credit at some point, Shapley values can be used to say why according to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select highest residual female false positive\n",
    "row = test_yhat_female_fp.sort_values(by='r_DEFAULT_NEXT_MONTH', ascending=False).head(n=1)\n",
    "\n",
    "# search for her index in shap_values array\n",
    "# create Pandas DataFrame and plot\n",
    "s_df = pd.DataFrame(shap_values[row.index[0], :].reshape(len(X), 1), columns=['Reason Codes'], index=X)\n",
    "_ = s_df.sort_values(by='Reason Codes', ascending=False).plot(kind='bar', legend=False, \n",
    "                                                              title='Shapley Contributions to Predictions')\n",
    "\n",
    "# check that Shapley is locally accurate\n",
    "print('Sum of Shapley contributions and bias:', s_df.sum()[0] + contributions_matrix[0, -1])\n",
    "print('Model prediction in margin space:', np.log(row[yhat].values/(1 - row[yhat].values))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the Shapley values with the individual's data values allows ranked reasons to be given for denial of credit:\n",
    "\n",
    "1. Two months late on most recent payment\n",
    "2. Two months late on second most recent payment\n",
    "3. Seven months late on sixth most recent payment\n",
    "\n",
    "Because monotonicity contraints were used during model training, these reasons should be consistent across all denial decisions meaning that no one who was denied credit would have a better value for an input variable than someone who was granted credit.\n",
    "\n",
    "#### Examine low logloss residual individuals\n",
    "\n",
    "Given that high-residual individuals were not clearly discriminated against, their low-residual counter parts could be interesting to investigate next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yhat_female_fp.sort_values(by='r_DEFAULT_NEXT_MONTH', ascending=True).head(n=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examing the low-residual false positives, it can be seen that perhaps the F1-selected cutoff is a bit too conservative. Many women just above the cutoff have missed 0-2 payments, and only been late 1-2 months on the fw payments they missed, if any. This potential bias problem can be remediated by increasing the cutoff in cell 14.\n",
    "\n",
    "#### Shutdown H2O\n",
    "After using h2o, it's typically best to shut it down. However, before doing so, users should ensure that they have saved any h2o data structures, such as models and H2OFrames, or scoring artifacts, such as POJOs and MOJOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O session _sid_9421 closed.\n"
     ]
    }
   ],
   "source": [
    "# be careful, this can erase your work!\n",
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "This notebook outlines a basic approach to DIA (and over-generalizes that phrase). In a complex, real-world machine learning project the hard-to-define phenomenas of sociological bias and unfairness can materialize in many ways and from many different sources. Although far from a flawless technique, the beauty of DIA is it is straightforward to implement, functions in a model-agnostic fashion on known labels and model predictions, and is applied in complex real-world fair lending situations, so it can probably be applied to your model too! \n",
    "\n",
    "Why risk being called out in the media for training an unfair model? Or why not investigate the monetary opportunity costs of type I errors and potential losses from type II errors? Why not do the right thing and investigate how your model treats people?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
