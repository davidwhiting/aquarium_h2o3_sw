{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection with Isolation Forests using H2O\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Anomaly detection is a common data science problem where the goal is to identify odd or suspicious observations, events, or items in our data that might be indicative of some issues in our data collection process (such as broken sensors, typos in collected forms, etc.) or unexpected events like security breaches, server failures, and so on. \n",
    "\n",
    "Anomaly detection can be performed in a supervised, semi-supervised, and unsupervised manner.\n",
    "\n",
    "For a supervised approach, we need to know whether each observation, event or item is anomalous or genuine, and we use this information during training. Obtaining labels for each observation might often be unrealistic.\n",
    "\n",
    "A semi-supervised approach uses the assumption that we only know which observations are genuine, non-anomalous, and we do not have any information on the anomalous observations. Therefore we only use the genuine data for training. During prediction, the model evaluates how similar the new observation is to the training data and how well it fits the model. \n",
    "\n",
    "An unsupervised approach assumes that the training set contains both genuine and anomalous observations. As labeling the data or having just clean data is often hard and time consuming, I would like to focus more on one of the unsupervised approaches to anomaly detection using isolation forests.\n",
    "\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "Before we dive into the anomaly detection, let's initialize the h2o cluster and load our data in. We will be using the [credit card data set](https://www.kaggle.com/mlg-ulb/creditcardfraud), which contains information on various properties of credit card transactions. There are 492 fraudulent and 284,807 genuine transactions, which makes the target class highly imbalanced. We will not use the label during the anomaly detection modeling, but we will use it during the evaluation of our anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:47:14.445025Z",
     "start_time": "2018-11-02T13:47:02.960415Z"
    }
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "h2o.init()\n",
    "\n",
    "df = h2o.import_file(\"../../data/creditcardfraud/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forests\n",
    "\n",
    "There are multiple approaches to an unsupervised anomaly detection problem that try to exploit the differences between the properties of common and unique observations. The idea behind the [Isolation Forest](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf) is as follows.\n",
    "\n",
    "* We start by building multiple decision trees such that the trees isolate the observations in their leaves. Ideally, each leaf of the tree isolates exactly one observation from your data set. The trees are being split randomly. We assume that if one observation is similar to others in our data set, it will take more random splits to perfectly isolate this observation, as opposed to isolating an outlier.\n",
    "\n",
    "* For an outlier that has some feature values significantly different from the other observations, randomly finding the split isolating it should not be too hard. As we build multiple isolation trees, hence the isolation forest, for each observation we can calculate the average number of splits across all the trees that isolate the observation. The average number of splits is then used as a score, where the less splits the observation needs, the more likely it is to be anomalous. \n",
    "\n",
    "Let's train our isolation forest and see how the predictions look. The last column (index 30) of the data contains the class label, so we exclude it from the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:47:26.036250Z",
     "start_time": "2018-11-02T13:47:14.448072Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "ntrees = 100\n",
    "isoforest = h2o.estimators.H2OIsolationForestEstimator(\n",
    "    ntrees=ntrees, seed=seed)\n",
    "isoforest.train(x=df.col_names[0:31], training_frame=df)\n",
    "predictions = isoforest.predict(df)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Inspecting Predictions\n",
    "We can see that the prediction h2o frame contains two columns: **predict** showing a normalized anomaly score, and **mean_length** showing the average number of splits across all trees to isolate the observation.\n",
    "\n",
    "These two columns should have the property of inverse proportion by their definition, as the less random splits you need to isolate the observation, the more anomalous it is. We can easily check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:47:26.136731Z",
     "start_time": "2018-11-02T13:47:26.039581Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions.cor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Anomalies using Quantile\n",
    "\n",
    "As we formulated this problem in an unsupervised fashion, how do we go from the average number of splits / anomaly score to the actual predictions? Using a threshold! If we have an idea about the relative number of outliers in our dataset, we can find the corresponding quantile value of the score and use it as a threshold for our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:47:26.217136Z",
     "start_time": "2018-11-02T13:47:26.139670Z"
    }
   },
   "outputs": [],
   "source": [
    "quantile = 0.95\n",
    "quantile_frame = predictions.quantile([quantile])\n",
    "quantile_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the threshold to predict the anomalous class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:47:26.351261Z",
     "start_time": "2018-11-02T13:47:26.219561Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = quantile_frame[0, \"predictQuantiles\"]\n",
    "predictions[\"predicted_class\"] = predictions[\"predict\"] > threshold\n",
    "predictions[\"class\"] = df[\"Class\"]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Because the isolation forest is an unsupervised method, it makes sense to have a look at the classification metrics that are not dependent on the prediction threshold and give an estimate of the quality of scoring. Two such metrics are Area Under the Receiver Operating Characteristic Curve (AUC) and Area under the Precision-Recall Curve (AUCPR).\n",
    "\n",
    "AUC is a metric evaluating how well a binary classification model distinguishes true positives from false positives. The perfect AUC score is 1; the baseline score of a random guessing is 0.5.\n",
    "\n",
    "AUCPR is a metric evaluating the precision recall trade-off of a binary classification using different thresholds of the continuous prediction score. The perfect AUCPR score is 1; the baseline score is the relative count of the positive class.  \n",
    "\n",
    "For highly imbalanced data, AUCPR is recommended over AUC as the AUCPR is more sensitive to True positives, False positives and False negatives, while not caring about True negatives, which in large quantity usually overshadow the effect of other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:47:28.281672Z",
     "start_time": "2018-11-02T13:47:26.353468Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def get_auc(labels, scores):\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    return fpr, tpr, auc_score\n",
    "\n",
    "\n",
    "def get_aucpr(labels, scores):\n",
    "    precision, recall, th = precision_recall_curve(labels, scores)\n",
    "    aucpr_score = np.trapz(recall, precision)\n",
    "    return precision, recall, aucpr_score\n",
    "\n",
    "\n",
    "def plot_metric(ax, x, y, x_label, y_label, plot_label, style=\"-\"):\n",
    "    ax.plot(x, y, style, label=plot_label)\n",
    "    ax.legend()\n",
    "    \n",
    "    ax.set_ylabel(x_label)\n",
    "    ax.set_xlabel(y_label)\n",
    "\n",
    "\n",
    "def prediction_summary(labels, predicted_score, info, plot_baseline=True, axes=None):\n",
    "    if axes is None:\n",
    "        axes = [plt.subplot(1, 2, 1), plt.subplot(1, 2, 2)]\n",
    "\n",
    "    fpr, tpr, auc_score = get_auc(labels, predicted_score)\n",
    "    plot_metric(axes[0], fpr, tpr, \"False positive rate\",\n",
    "                \"True positive rate\", \"{} AUC = {:.4f}\".format(info, auc_score))\n",
    "    if plot_baseline:\n",
    "        plot_metric(axes[0], [0, 1], [0, 1], \"False positive rate\",\n",
    "                \"True positive rate\", \"baseline AUC = 0.5\", \"r--\")\n",
    "\n",
    "    precision, recall, aucpr_score = get_aucpr(labels, predicted_score)\n",
    "    plot_metric(axes[1], recall, precision, \"Recall\",\n",
    "                \"Precision\", \"{} AUCPR = {:.4f}\".format(info, aucpr_score))\n",
    "    if plot_baseline:\n",
    "        thr = sum(labels)/len(labels)\n",
    "        plot_metric(axes[1], [0, 1], [thr, thr], \"Recall\",\n",
    "                \"Precision\", \"baseline AUCPR = {:.4f}\".format(thr), \"r--\")\n",
    "\n",
    "    plt.show()\n",
    "    return axes\n",
    "\n",
    "\n",
    "def figure():\n",
    "    fig_size = 4.5\n",
    "    f = plt.figure()\n",
    "    f.set_figheight(fig_size)\n",
    "    f.set_figwidth(fig_size*2)\n",
    "\n",
    "\n",
    "h2o_predictions = predictions.as_data_frame()\n",
    "\n",
    "figure()\n",
    "axes = prediction_summary(\n",
    "    h2o_predictions[\"class\"], h2o_predictions[\"predict\"], \"h2o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forests in scikit-learn\n",
    "\n",
    "We can perform the same anomaly detection using scikit-learn. The version of the scikit-learn used in this example is 0.20. Some of the behavior can differ in other versions. The *score_samples* method returns the opposite of the anomaly score; therefore it is inverted. Scikit-learn also takes in a *contamination* parameter, which is the proportion of outliers in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:48:14.172040Z",
     "start_time": "2018-11-02T13:47:28.284096Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "\n",
    "df_pandas = df.as_data_frame()\n",
    "df_train_pandas = df_pandas.iloc[:, :30]\n",
    "\n",
    "x = IsolationForest(random_state=seed, contamination=(1-quantile),\n",
    "                    n_estimators=ntrees, behaviour=\"new\").fit(df_train_pandas)\n",
    "\n",
    "iso_predictions = x.predict(df_train_pandas)\n",
    "iso_score = x.score_samples(df_train_pandas)\n",
    "\n",
    "sk_predictions = pd.DataFrame({\n",
    "    \"predicted_class\": list(map(lambda x: 1*(x == -1), iso_predictions)),\n",
    "    \"class\": h2o_predictions[\"class\"],\n",
    "    \"predict\": -iso_score\n",
    "})\n",
    "\n",
    "sk_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:48:14.358429Z",
     "start_time": "2018-11-02T13:48:14.174668Z"
    }
   },
   "outputs": [],
   "source": [
    "figure()\n",
    "axes = prediction_summary(\n",
    "    sk_predictions[\"class\"], sk_predictions[\"predict\"], \"sklearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T11:06:16.216866Z",
     "start_time": "2018-11-02T11:06:16.213785Z"
    }
   },
   "source": [
    "## Compare Isolation Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:48:14.593533Z",
     "start_time": "2018-11-02T13:48:14.360215Z"
    }
   },
   "outputs": [],
   "source": [
    "figure()\n",
    "axes = prediction_summary(h2o_predictions[\"class\"], h2o_predictions[\"predict\"],\n",
    "                        \"h2o\", plot_baseline=False)\n",
    "axes = prediction_summary(sk_predictions[\"class\"], sk_predictions[\"predict\"],\n",
    "                        \"sklearn\", axes=axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Robust Comparison\n",
    "\n",
    "Because there is a lot of randomness in the isolation forests training, we will train the isolation forest 20 times for each library using different seeds, and then we will compare the statistics. While 20 times might not be enough, it could give us some insight into how the isolation forests perform on our anomaly detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T14:02:34.743235Z",
     "start_time": "2018-11-02T13:51:38.911233Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "def stability_check(train_predict_fn, x, y, ntimes=20):\n",
    "    scores = [\"AUC\", \"AUCPR\"]\n",
    "    scores = {key: [] for key in scores}\n",
    "    seeds = np.linspace(1, (2**32) - 1, ntimes).astype(int)\n",
    "    for seed in tqdm_notebook(seeds):\n",
    "        predictions = train_predict_fn(x, int(seed))\n",
    "        _, _, auc_score = get_auc(y, predictions)\n",
    "        _, _, aucpr_score = get_aucpr(y, predictions)\n",
    "\n",
    "        scores[\"AUC\"].append(auc_score)\n",
    "        scores[\"AUCPR\"].append(aucpr_score)\n",
    "\n",
    "    return pd.DataFrame(scores)\n",
    "\n",
    "\n",
    "def iso_forests_h2o(data, seed):\n",
    "    isoforest = h2o.estimators.H2OIsolationForestEstimator(\n",
    "        ntrees=ntrees, seed=seed)\n",
    "    isoforest.train(x=data.col_names, training_frame=data)\n",
    "    preds = isoforest.predict(data)\n",
    "    return preds.as_data_frame()[\"predict\"]\n",
    "\n",
    "\n",
    "def iso_forests_sklearn(data, seed):\n",
    "    iso = IsolationForest(random_state=seed, n_estimators=ntrees,\n",
    "                          behaviour=\"new\", contamination=(1-quantile))\n",
    "    iso.fit(data)\n",
    "    iso_score = iso.score_samples(data)\n",
    "    return -iso_score\n",
    "\n",
    "\n",
    "h2o.no_progress()\n",
    "\n",
    "h2o_check = stability_check(iso_forests_h2o, df[:30], h2o_predictions[\"class\"])\n",
    "sklearn_check = stability_check(\n",
    "    iso_forests_sklearn, df_train_pandas, sk_predictions[\"class\"])\n",
    "\n",
    "sklearn_check.join(h2o_check, rsuffix=\"_h2o\", lsuffix=\"_sklearn\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Results\n",
    "\n",
    "Looking at the results of the 20 runs, we can see that the h2o isolation forest implementation on average scores similarly to the scikit-learn implementation in both AUC and AUCPR. The big advantage of h2o is the ability to easily scale up to hundreds of nodes and work seamlessly with Apache Spark using Sparkling Water. This allows you to process extremely large datasets, which might be crucial in the transactional data setting.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We have learned about the isolation forests, their underlying principle, how to apply them for unsupervised anomaly detection, and how to evaluate the quality of anomaly detection once we have corresponding labels. Even though we did not go deep into internal parameters of the isolation forest algorithm, you should now be able to apply it to any anomaly detection task you might face. I hope you have enjoyed this journey as much as I did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
