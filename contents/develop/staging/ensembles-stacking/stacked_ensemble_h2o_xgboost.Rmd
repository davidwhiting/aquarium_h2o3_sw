---
title: "Stacked Ensembles of H2O and XGBoost Models"
author: "Erin LeDell"
date: "4/25/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This tutorial will demonstrate how to use the **h2o** R package to combine H2O models with XGBoost models into a Stacked Ensemble.  This is a preview of upcoming H2O functionality.  Please provide any feedback to erin_at_ h2o.ai.

## Install XGBoost-enabled H2O

Currently, XGBoost is available in a special development edition of H2O.  The Mac OS X version (with XGBoost compiled for Mac) is available (temporarily) [here](https://slack-files.com/T0329MHH6-F53HJUP5E-11e85e527b).  Download the file, unzip it, and install the R package: `R CMD install ./R/h2o_3.11.0.99999.tar.gz`  Linux binary available upon request.

H2O ships with everything except the system library for multithreading (openMP).  On a Mac, you will need to install OpenMP, which you get easily via Homebrew.

```bash
# Install OpenMP (required of xgboost-enabled h2o)
brew install gcc --without-multilib
```
Once the special edition **h2o** R package is installed, you are all set to start training H2O and XGBoost models from H2O.  This distribution process will be modified when it's officially released (and these instructions will be updated accordingly).


## Train Base Learners

Let's train and cross-validate a set of H2O and XGBoost models and then create a [Stacked Ensemble](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html) using the **h2o** R package.

### Start H2O Cluster & Load Data

```{r h2o_load_data}
library(h2o)
h2o.init(nthreads = -1)
h2o.no_progress() # Don't show progress bars in RMarkdown output

# Import a sample binary outcome train/test set into H2O
train <- h2o.importFile("https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv")
test <- h2o.importFile("https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv")

# Identify predictors and response
y <- "response"
x <- setdiff(names(train), y)

# For binary classification, response should be a factor
train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])

# Number of CV folds (to generate level-one data for stacking)
nfolds <- 5
```

### H2O base models

```{r h2o_train_cv}
# Train & Cross-validate a GBM
my_gbm <- h2o.gbm(x = x,
                  y = y,
                  training_frame = train,
                  distribution = "bernoulli",
                  max_depth = 3,
                  min_rows = 2,
                  learn_rate = 0.2,
                  nfolds = nfolds,
                  fold_assignment = "Modulo",
                  keep_cross_validation_predictions = TRUE,
                  seed = 1)

# Train & Cross-validate a RF
my_rf <- h2o.randomForest(x = x,
                          y = y,
                          training_frame = train,
                          nfolds = nfolds,
                          fold_assignment = "Modulo",
                          keep_cross_validation_predictions = TRUE,
                          seed = 1)

# Train & Cross-validate a DNN
my_dl <- h2o.deeplearning(x = x,
                          y = y,
                          training_frame = train,
                          l1 = 0.001,
                          l2 = 0.001,
                          hidden = c(200, 200, 200),
                          nfolds = nfolds,
                          fold_assignment = "Modulo",
                          keep_cross_validation_predictions = TRUE,
                          seed = 1)
```



### XGBoost base models

```{r xgb_train_cv}
# Train & Cross-validate a (shallow) XGB-GBM
my_xgb1 <- h2o.xgboost(x = x,
                       y = y,
                       training_frame = train,
                       distribution = "bernoulli",
                       ntrees = 50,
                       max_depth = 3,
                       min_rows = 2,
                       learn_rate = 0.2,
                       nfolds = nfolds,
                       fold_assignment = "Modulo",
                       keep_cross_validation_predictions = TRUE,
                       seed = 1)


# Train & Cross-validate another (deeper) XGB-GBM
my_xgb2 <- h2o.xgboost(x = x,
                       y = y,
                       training_frame = train,
                       distribution = "bernoulli",
                       ntrees = 50,
                       max_depth = 8,
                       min_rows = 1,
                       learn_rate = 0.1,
                       sample_rate = 0.7,
                       col_sample_rate = 0.9,
                       nfolds = nfolds,
                       fold_assignment = "Modulo",
                       keep_cross_validation_predictions = TRUE,
                       seed = 1)
```


## Create a Stacked Ensemble

To maximize predictive power, will create an [H2O Stacked Ensemble](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html) from the models we created above and print the performance gain the ensemble has over the best base model.

```{r create_ensemble}
# Train a stacked ensemble using the H2O and XGBoost models from above
base_models <- list(my_gbm@model_id, my_rf@model_id, my_dl@model_id,  
                    my_xgb1@model_id, my_xgb2@model_id)

ensemble <- h2o.stackedEnsemble(x = x,
                                y = y,
                                training_frame = train,
                                base_models = base_models)

# Eval ensemble performance on a test set
perf <- h2o.performance(ensemble, newdata = test)


# Compare to base learner performance on the test set
get_auc <- function(mm) h2o.auc(h2o.performance(h2o.getModel(mm), newdata = test))
baselearner_aucs <- sapply(base_models, get_auc)
baselearner_best_auc_test <- max(baselearner_aucs)
ensemble_auc_test <- h2o.auc(perf)
```

Compare the test set performance of the best base model to the ensemble.

```{r print_perf}
print(sprintf("Best Base-learner Test AUC:  %s", baselearner_best_auc_test))
print(sprintf("Ensemble Test AUC:  %s", ensemble_auc_test))
```
