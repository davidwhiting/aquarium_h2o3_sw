{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License \n",
    "\n",
    "<span style=\"color:gray\"> Copyright 2019 David Whiting and the H2O.ai team\n",
    "\n",
    "<span style=\"color:gray\"> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "<span style=\"color:gray\">     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "<span style=\"color:gray\"> Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "<span style=\"color:gray\"> **DISCLAIMER:** This notebook is not legal compliance advice. </span>\n",
    "\n",
    "<hr style=\"background-color: gray;height: 2.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to H2O-3: Lesson 3\n",
    "\n",
    "This is the fourth in a series of instructional Jupyter notebooks on H2O-3. These notebooks are built to be run on the H2O.ai Aquarium training platform [http://aquarium.h2o.ai](http://aquarium.h2o.ai) under the `Coursework` lab. There is an accompanying instructional video with additional commentary found **_here_** _(link to be added)_.\n",
    "\n",
    "\n",
    "<div style=\"margin-left: 3em;\">\n",
    "\n",
    "\n",
    "### Intended Audience\n",
    "\n",
    "The target audience for this training notebook is data scientists, machine learning engineers, and other experienced modelers. (Technically advanced analysts might also find this training understandable -- wordsmith)\n",
    "\n",
    "A working knowledge of python is assumed.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This assumes that students have completed Lessons 1-3\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "By the end of this notebook, you will be able to ...\n",
    "\n",
    "- [ ] Feature engineering: target encoding\n",
    "- [ ] Load data directly into the H2O-3 cluster\n",
    "- [ ] Use H2O-3 commands to perform basic data munging tasks \n",
    "- [ ] Engineer new data features\n",
    "- [ ] Train and evaluate an XGBoost ML model\n",
    "- [ ] Create and save a MOJO for model production\n",
    "- [ ] Use H2O Flow for monitoring H2O-cluster activity and health\n",
    "- [ ] Use H2O Flow for inspecting data and models\n",
    "\n",
    "#### \"Machine Learning\" Outcomes\n",
    "\n",
    "- [ ] Advanced XGBoost\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr style=\"background-color: rgb(170,0,0);height: 2.0px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM and XGBoost Gridsearch Demo\n",
    "\n",
    "In this tutorial, we will go through a step-by-step workflow to demonstrate how easy it is to use H2OXGBoost with Gridsearch.\n",
    "\n",
    "\n",
    "## REDO WITH LENDING CLUB DATA SET\n",
    "## Start the H2O-3 cluster\n",
    "\n",
    "The `os` commands below check whether this notebook is being run on the Aquarium platform. We use the `h2o.init` command to connect to the H2O-3 cluster, starting it if it is not already up. The parameters used in `h2o.init` will depend on your specific environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h2o\n",
    "\n",
    "startup = '/home/h2o/bin/aquarium_startup'\n",
    "if os.path.exists(startup):\n",
    "    os.system(startup)\n",
    "    local_url = 'http://localhost:54321/h2o'\n",
    "    aquarium = True\n",
    "else:\n",
    "    local_url = 'http://localhost:54321'\n",
    "    aquarium = False\n",
    "\n",
    "h2o.init(url=local_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The method you use for starting and stopping an H2O-3 cluster will depend on how H2O is installed and configured on your system. Regardless of how H2O is installed, if you start a cluster, you will need to ensure that it is shut down when you are done.\n",
    "\n",
    "## Titanic Data Set\n",
    "\n",
    "We will look at the famous Titanic passenger data set and try to predict who lived and who died...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aquarium:\n",
    "    input_csv = \"/home/h2o/data/titanic/titanic.csv\"\n",
    "else:\n",
    "    input_csv = \"http://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv\"\n",
    "\n",
    "titanic = h2o.import_file(path = input_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The method you use for starting and stopping an H2O-3 cluster will depend on how H2O is installed and configured on your system. Regardless of how H2O is installed, if you start a cluster, you will need to ensure that it is shut down when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `survived` as a factor so that H2O can build a classification model. Also cast `ticket` as a factor rather than numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"survived\"] = titanic[\"survived\"].asfactor()\n",
    "titanic[\"ticket\"] = titanic[\"ticket\"].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the predictors and response variables. Note that we exclude `name` because it is a text variable. We also exclude `boat` and `body`, because those variables would not have been known at the time of setting sail. Including those is a classic example of *data leakage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set predictors and response variable\n",
    "response = \"survived\"\n",
    "exclude = [\"name\", \"survived\", \"boat\", \"body\"]\n",
    "# not including boat or body due to data leakage\n",
    "\n",
    "predictors = list(set(titanic.col_names) - set(exclude))\n",
    "predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create training and test data sets. Rather than creating a validation data set, we will use k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = titanic.split_frame(seed = 1234, \n",
    "                                  ratios = [0.75], \n",
    "                                  destination_frames = [\"train.hex\", \"test.hex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default GBM Model\n",
    "\n",
    "Build a GBM model with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "\n",
    "#We only provide the required parameters, everything else is default\n",
    "gbm_model = H2OGradientBoostingEstimator(seed = 1234, nfolds = 5)\n",
    "gbm_model.train(x = predictors\n",
    "                , y = response\n",
    "                , training_frame = train\n",
    "                , validation_frame = test\n",
    "                , model_id = \"gbm_default.hex\"\n",
    "               )\n",
    "\n",
    "## Show a detailed model summary\n",
    "print(gbm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "gbm_model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data\")\n",
    "gbm_model.model_performance(train = True).plot()\n",
    "print(\"Cross-Validation\")\n",
    "gbm_model.model_performance(xval = True).plot()\n",
    "print(\"Testing Data\")\n",
    "gbm_model.model_performance(valid = True).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default GBM model overtrained pretty severely.\n",
    "\n",
    "## Default XGBoost Models\n",
    "\n",
    "Build an XGBoost default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OXGBoostEstimator\n",
    "\n",
    "param = {\"seed\": 1234,\n",
    "         \"nfolds\": 5\n",
    "        }\n",
    "\n",
    "xgboost_model = H2OXGBoostEstimator(**param)\n",
    "xgboost_model.train(x = predictors\n",
    "                    , y = response\n",
    "                    , training_frame = train\n",
    "                    , validation_frame = test\n",
    "                    , model_id = \"xgb_default.hex\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgboost_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data\")\n",
    "xgboost_model.model_performance(train = True).plot()\n",
    "print(\"Cross-Validation\")\n",
    "xgboost_model.model_performance(xval = True).plot()\n",
    "print(\"Testing Data\")\n",
    "xgboost_model.model_performance(valid = True).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default XGBoost model gives us a better result. Let's use gridsearch with early stopping on both models to see if we can improve their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM Gridsearch \n",
    "\n",
    "### Notes on parameter values\n",
    "\n",
    "Our strategy is to start with a large number of trees and a small learning rate in combination with early stopping.\n",
    "\n",
    "- Early stopping kicks in if the AUC doesn't improve by 0.001 for 5 consecutive scoring intervals. \n",
    "- We begin with a not-so-small 0.05 learning rate, but use `learn_rate_annealing` to decrease the learning rate by 1% after each tree. (Alternately, we could set annealing to 1 and make the learning rate smaller.)\n",
    "- We sample 80% of rows per tree (`sample_rate`)\n",
    "- We sample 80% of columns per split (`col_sample_rate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "gbm_params = {'max_depth': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25]\n",
    "              , 'ntrees': [5000]\n",
    "              , 'learn_rate': [0.05]\n",
    "              , 'learn_rate_annealing': [0.99]\n",
    "              , 'sample_rate': [0.8]\n",
    "              , 'col_sample_rate': [0.8]\n",
    "              , 'stopping_metric': 'AUC'\n",
    "              , 'stopping_rounds': [5]\n",
    "              , 'stopping_tolerance': [0.001]\n",
    "             }\n",
    "\n",
    "gbm_grid = H2OGridSearch(model = H2OGradientBoostingEstimator,\n",
    "                         hyper_params = gbm_params\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping is only reproducible if we use `score_tree_interval`; here we set it to score every 10 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_grid.train(x = predictors, y = response\n",
    "               , training_frame = train\n",
    "               , validation_frame = test\n",
    "               , score_tree_interval = 10\n",
    "               , seed = 1234\n",
    "               , grid_id = \"gbm_grid\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Gridsearch\n",
    "\n",
    "Let's do the same with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_params = {'max_depth': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25]\n",
    "                  , 'ntrees': [5000]\n",
    "                  , 'learn_rate': [1, 0.1, 0.01, 0.001]\n",
    "                  , 'sample_rate': [0.8]\n",
    "                  , 'col_sample_rate': [0.8]\n",
    "                  , 'stopping_metric': 'AUC'\n",
    "                  , 'stopping_rounds': [5]\n",
    "                  , 'stopping_tolerance': [0.001]\n",
    "                 }\n",
    "\n",
    "xgboost_grid = H2OGridSearch(model = H2OXGBoostEstimator\n",
    "                             , hyper_params = xgboost_params\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_grid.train(x = predictors, y = response\n",
    "                   , training_frame = train               \n",
    "                   , validation_frame = test               \n",
    "                   , score_tree_interval = 10              \n",
    "                   , seed = 1234\n",
    "                   , grid_id = \"xgboost_grid\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid summary\n",
    "\n",
    "### GBM Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort the grid models by decreasing AUC\n",
    "sorted_gbm_grid = gbm_grid.get_grid(sort_by=\"auc\", decreasing = True)\n",
    "sorted_gbm_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gbm = sorted_gbm_grid.models[0]\n",
    "best_gbm_perf = best_gbm.model_performance(test)\n",
    "best_gbm_perf.auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort the grid models by decreasing AUC\n",
    "sorted_xgboost_grid = xgboost_grid.get_grid(sort_by=\"auc\", decreasing = True)\n",
    "sorted_xgboost_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgboost = sorted_xgboost_grid.models[0]\n",
    "best_xgboost_perf = best_xgboost.model_performance(test)\n",
    "best_xgboost_perf.auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with gridsearch, XGBoost does a better job than GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop H2O Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
