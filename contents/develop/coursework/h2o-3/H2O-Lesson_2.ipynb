{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License \n",
    "\n",
    "<span style=\"color:gray\"> Copyright 2019 David Whiting and the H2O.ai team\n",
    "\n",
    "<span style=\"color:gray\"> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "<span style=\"color:gray\">     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "<span style=\"color:gray\"> Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "<span style=\"color:gray\"> **DISCLAIMER:** This notebook is not legal compliance advice. </span>\n",
    "\n",
    "<hr style=\"background-color: gray;height: 2.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to H2O-3: Lesson 2\n",
    "\n",
    "This is the second in a series of instructional Jupyter notebooks on H2O-3. These notebooks are built to be run on the H2O.ai Aquarium training platform [http://aquarium.h2o.ai](http://aquarium.h2o.ai) under the `Coursework` lab. There is an accompanying instructional video with additional commentary found <span style=\"color:red\"> **_here_** _(link to be added)_.</span>\n",
    "\n",
    "<div style=\"margin-left: 3em;\">\n",
    "\n",
    "### Intended Audience\n",
    "\n",
    "The target audience for this training notebook is data scientists, machine learning engineers, and other experienced modelers. (Technically advanced analysts might also find this training understandable -- wordsmith)\n",
    "\n",
    "A working knowledge of python is assumed.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This assumes that students have completed Lessons 1-3\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "By the end of this notebook, you will be able to ...\n",
    "\n",
    "- [ ] Feature engineering: target encoding\n",
    "- [ ] Load data directly into the H2O-3 cluster\n",
    "- [ ] Use H2O-3 commands to perform basic data munging tasks \n",
    "- [ ] Engineer new data features\n",
    "- [ ] Train and evaluate an XGBoost ML model\n",
    "- [ ] Create and save a MOJO for model production\n",
    "- [ ] Use H2O Flow for monitoring H2O-cluster activity and health\n",
    "- [ ] Use H2O Flow for inspecting data and models\n",
    "\n",
    "#### \"Machine Learning\" Outcomes\n",
    "\n",
    "- [ ] Advanced XGBoost\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr style=\"background-color: rgb(170,0,0);height: 2.0px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending Club Loan Default\n",
    "\n",
    "In this tutorial, we will go through a step-by-step workflow to determine loan deliquency. We will make predictions based only on the information available at the time the loan was issued.  The data for this exercise come from the public Lending Club data set, a description can be found [here](https://www.kaggle.com/pragyanbo/a-hitchhiker-s-guide-to-lending-club-loan-data/notebook).\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Start the H2O-3 cluster\n",
    "2. Import data\n",
    "3. Clean data\n",
    "4. Feature engineering\n",
    "5. Model training\n",
    "6. Examine model accuracy\n",
    "7. Interpret model\n",
    "8. Save and reuse model\n",
    "9. AutoML (optional)\n",
    "10. Stop H2O-3 cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 (of 10). Start the H2O-3 cluster\n",
    "\n",
    "The `os` commands below check whether this notebook is being run on the Aquarium platform. We use `h2o.init` command to connect to the H2O-3 cluster, starting it if it is not already up. The parameters used in `h2o.init` will depend on your specific environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h2o\n",
    "\n",
    "startup = '/home/h2o/bin/aquarium_startup'\n",
    "if os.path.exists(startup):\n",
    "    os.system(startup)\n",
    "    local_url = 'http://localhost:54321/h2o'\n",
    "    aquarium = True\n",
    "else:\n",
    "    local_url = 'http://localhost:54321'\n",
    "    aquarium = False\n",
    "\n",
    "h2o.init(url=local_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The method you use for starting and stopping an H2O-3 cluster will depend on how H2O is installed and configured on your system. Regardless of how H2O is installed, if you start a cluster, you will need to ensure that it is shut down when you are done.\n",
    "\n",
    "# Step 2 (of 10). Import data\n",
    "\n",
    "The data set we use below is a local copy of https://s3-us-west-2.amazonaws.com/h2o-tutorials/data/topics/lending/lending_club/LoanStats3a.csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aquarium:\n",
    "    input_csv = \"/home/h2o/data/lending_club/LoanStats3a.csv\"\n",
    "else:\n",
    "    input_csv = \"https://s3-us-west-2.amazonaws.com/h2o-tutorials/data/topics/lending/lending_club/LoanStats3a.csv\"\n",
    "\n",
    "loans = h2o.import_file(input_csv,\n",
    "                        col_types = {\"int_rate\":\"string\", \n",
    "                                     \"revol_util\":\"string\", \n",
    "                                     \"emp_length\":\"string\", \n",
    "                                     \"verification_status\":\"string\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unfiltered_loans = loans.dim[0]\n",
    "\n",
    "ongoing_status = [\"Current\",\n",
    "                  \"In Grace Period\",\n",
    "                  \"Late (16-30 days)\",\n",
    "                  \"Late (31-120 days)\",\n",
    "                  \"Does not meet the credit policy.  Status:Current\",\n",
    "                  \"Does not meet the credit policy.  Status:In Grace Period\"\n",
    "                 ]\n",
    "loans = loans[~loans[\"loan_status\"].isin(ongoing_status)]\n",
    "\n",
    "num_filtered_loans = loans.dim[0]\n",
    "num_loans_filtered_out = num_unfiltered_loans - num_filtered_loans\n",
    "\n",
    "fully_paid = [\"Fully Paid\",\n",
    "              \"Does not meet the credit policy.  Status:Fully Paid\"]\n",
    "loans[\"bad_loan\"] = ~(loans[\"loan_status\"].isin(fully_paid))\n",
    "\n",
    "loans[\"bad_loan\"] = loans[\"bad_loan\"].asfactor()\n",
    "\n",
    "bad_loan_dist = loans[\"bad_loan\"].table()\n",
    "bad_loan_dist[\"Percentage\"] = (100 * bad_loan_dist[\"Count\"] / loans.nrow).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"int_rate\"] = loans[\"int_rate\"].gsub(pattern = \"%\", replacement = \"\") \n",
    "loans[\"int_rate\"] = loans[\"int_rate\"].trim()\n",
    "loans[\"int_rate\"] = loans[\"int_rate\"].asnumeric()\n",
    "\n",
    "loans[\"revol_util\"] = loans[\"revol_util\"].gsub(pattern=\"%\", replacement=\"\")\n",
    "loans[\"revol_util\"] = loans[\"revol_util\"].trim()\n",
    "loans[\"revol_util\"] = loans[\"revol_util\"].asnumeric()\n",
    "\n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].gsub(pattern=\"([ ]*+[a-zA-Z].*)|(n/a)\", replacement=\"\") \n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].trim()\n",
    "\n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].gsub(pattern=\"< 1\", replacement=\"0\")\n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].gsub(pattern=\"10\\\\+\", replacement=\"10\")\n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].asnumeric()\n",
    "\n",
    "loans[\"verification_status\"] = loans[\"verification_status\"].sub(pattern=\"VERIFIED - income source\", \n",
    "                                                                replacement=\"verified\")\n",
    "loans[\"verification_status\"] = loans[\"verification_status\"].sub(pattern=\"VERIFIED - income\", \n",
    "                                                                replacement=\"verified\")\n",
    "loans[\"verification_status\"] = loans[\"verification_status\"].asfactor()\n",
    "\n",
    "loans[\"credit_length\"] = loans[\"issue_d\"].year() - loans[\"earliest_cr_line\"].year()\n",
    "loans[\"credit_length\"].head()\n",
    "\n",
    "loans[\"issue_d_year\"] = loans[\"issue_d\"].year()\n",
    "loans[\"issue_d_month\"] = loans[\"issue_d\"].month().asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 (of 10).  NLP Feature engineering\n",
    "\n",
    "Feature engineering can be considered the \"secret sauce\" in building a superior predictive model: it is often (although not always) more important than the choice of machine learning algorithm. A very good summary of feature engineering recipes can be found in the online [Driverless AI Documentation](http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/transformations.html). \n",
    "\n",
    "We will use NLP (natural language processing) to create word embedding features from the loan description text field in our data.\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "One of the columns in our dataset is a user-provided description of why the loan was requested. The first few descriptions in the dataset are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"desc\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptions may contain information that would assist in predicting default, but supervised learning algorithms in general have a hard time understanding text. We need to convert these strings into a numeric representation of the text in order for our algorithms to operate on it. There are multiple choices for doing so, in this example we will use the Word2Vec algorithm.\n",
    "\n",
    "We start by defining stop words (terms that are considered too frequent to carry much information) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = [\"ax\",\"i\",\"you\",\"edu\",\"s\",\"t\",\"m\",\"subject\",\"can\",\"lines\",\"re\",\"what\",\n",
    "              \"there\",\"all\",\"we\",\"one\",\"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\n",
    "              \"but\",\"is\",\"in\",\"a\",\"not\",\"with\",\"as\",\"was\",\"if\",\"they\",\"are\",\"this\",\"and\",\"it\",\"have\",\n",
    "              \"from\",\"at\",\"my\",\"be\",\"by\",\"not\",\"that\",\"to\",\"from\",\"com\",\"org\",\"like\",\"likes\",\"so\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next _tokenize_ the descriptions by breaking the text into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences, stop_word = STOP_WORDS):\n",
    "    tokenized = sentences.tokenize(\"\\\\W+\")\n",
    "    tokenized_lower = tokenized.tolower()\n",
    "    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n",
    "    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n",
    "    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n",
    "    return tokenized_words\n",
    "\n",
    "words = tokenize(loans[\"desc\"].ascharacter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train our Word2Vec model on the words extracted from our descriptions. We choose an output vector size of 100.\n",
    "\n",
    ">What does Word2Vec do? At a high level, it is a dimensionality reduction method for numerical representations of text. But it reduces dimensionality while preserving relationships between words in the text.\n",
    ">\n",
    ">Suppose we were to create a dictionary of all the words in our descriptions, and further suppose that dictionary contained 2500 unique words. At one extreme, we could create an indicator variable for each word (i.e., one-hot encoding). This would yield 2500 new features that would certainly lead to massive overfitting of models.\n",
    ">\n",
    ">At the other extreme, suppose we had someone classify those words into different groups and create indicator variables for each group: e.g., `risky_words` (\"bankruptcy\", \"default\", \"forfeit\", \"lien\", etc.), `angry_words` (profanity, \"complaint\", etc.), and so on. This reduces dimensionality by manually grouping words, but it is extremely labor intensive.\n",
    ">\n",
    ">Word2Vec starts with the entire dictionary size $K$ as inputs and the selected vector size $k$ as the target number of outputs. Passing through the intermediate layer(s) of the Word2Vec neural net, a $k$-dimensional numeric representation of each word is derived. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "\n",
    "w2v_model = H2OWord2vecEstimator(vec_size=100, model_id=\"w2v\")\n",
    "w2v_model.train(training_frame=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we quickly sanity check the Word2Vec model is by finding synonyms for specified words, e.g., \"car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.find_synonyms(\"car\", count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we are satisfied with our Word2Vec model results, we next calculate a vector for each description by averaging over all of the words in that description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "desc_vecs = w2v_model.transform(words, aggregate_method=\"AVERAGE\")\n",
    "desc_vecs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add the aggregated word embeddings from the Word2Vec model to the loans data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = loans.cbind(desc_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 (of 10). Model training\n",
    "\n",
    "Now that we have cleaned our data and added new columns, we train a model to predict bad loans. First split our loans data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = loans.split_frame(seed=25, ratios=[0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create a list of predictors as a subset of the columns of the `loans` H2O Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = [\"initial_list_status\",\n",
    "                  \"out_prncp\",\n",
    "                  \"out_prncp_inv\",\n",
    "                  \"total_pymnt\",\n",
    "                  \"total_pymnt_inv\",\n",
    "                  \"total_rec_prncp\", \n",
    "                  \"total_rec_int\",\n",
    "                  \"total_rec_late_fee\",\n",
    "                  \"recoveries\",\n",
    "                  \"collection_recovery_fee\",\n",
    "                  \"last_pymnt_d\", \n",
    "                  \"last_pymnt_amnt\",\n",
    "                  \"next_pymnt_d\",\n",
    "                  \"last_credit_pull_d\",\n",
    "                  \"collections_12_mths_ex_med\" , \n",
    "                  \"mths_since_last_major_derog\",\n",
    "                  \"policy_code\",\n",
    "                  \"loan_status\",\n",
    "                  \"funded_amnt\",\n",
    "                  \"funded_amnt_inv\",\n",
    "                  \"mths_since_last_delinq\",\n",
    "                  \"mths_since_last_record\",\n",
    "                  \"id\",\n",
    "                  \"member_id\",\n",
    "                  \"desc\",\n",
    "                  \"zip_code\"]\n",
    "\n",
    "predictors = list(set(loans.col_names) - set(cols_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an XGBoost model for predicting loan default. This model is being run with almost all of the model-tuning values at their defaults. Later we may want to optimize the hyperparameters using a grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OXGBoostEstimator\n",
    "\n",
    "param = {\n",
    "      \"ntrees\" : 20\n",
    "    , \"nfolds\" : 5\n",
    "    , \"seed\": 25\n",
    "}\n",
    "xgboost_model = H2OXGBoostEstimator(**param)\n",
    "xgboost_model.train(x = predictors,\n",
    "                    y = \"bad_loan\",\n",
    "                    training_frame=train,\n",
    "                    validation_frame=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 (of 10).  Examine model accuracy\n",
    "\n",
    "The plot below shows the performance of the model as more trees are built.  This graph can help us see at what point our model begins overfitting.  Our test data error rate stops improving at around 8-10 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "xgboost_model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve of the training and testing data are shown below.  The area under the ROC curve is much higher for the training data than the test data, indicating that the model is beginning to memorize the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data\")\n",
    "xgboost_model.model_performance(train = True).plot()\n",
    "print(\"Testing Data\")\n",
    "xgboost_model.model_performance(valid = True).plot()\n",
    "print(\"X-Val\")\n",
    "xgboost_model.model_performance(xval=True).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 (of 10). Interpret model\n",
    "\n",
    "The variable importance plot shows us which variables are most important to predicting `bad_loan`.  We can use partial dependency plots to learn more about how these variables affect the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model.varimp_plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, interest rate appears to be the most important feature in predicting loan default. The partial dependency plot of the `int_rate` predictor shows us that as the interest rate increases, the likelihood of the loan defaulting also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pdp = xgboost_model.partial_plot(cols=[\"int_rate\"], data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 (of 10). Save and reuse model\n",
    "\n",
    "The model can either be embedded into a self-contained Java MOJO package\n",
    "or it can be saved and later loaded directly into an H2O-3 cluster. For production\n",
    "use, we recommend using MOJO as it is optimized for speed. See the [guide](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html) for further information.\n",
    "\n",
    "### Downloading MOJO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model.download_mojo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and reuse the model \n",
    "\n",
    "We can save the model to disk for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = h2o.save_model(model=xgboost_model, force=True)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the H2O cluster shuts down, all unsaved data and models are lost. At some future date, we can load the model for batch scoring in the H2O cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = h2o.load_model(path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using that model, we can also score new data with the predict function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bad_loan_hat = loaded_model.predict(test)\n",
    "bad_loan_hat.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9 (of 10). AutoML (optional)\n",
    "\n",
    "AutoML can be used for automating the machine learning workflow, which includes automatic training and tuning of many models within a user-specified time-limit or user specified model build limit. \n",
    "\n",
    "Stacked Ensembles will be automatically trained on collections of individual models to produce highly predictive ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "aml = H2OAutoML(max_models=5, \n",
    "                max_runtime_secs_per_model=60, \n",
    "                include_algos = [\"GLM\", \"DRF\", \"XGBoost\", \"StackedEnsemble\"],\n",
    "                seed=25)\n",
    "aml.train(x=predictors, y='bad_loan', training_frame=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_While the AutoML job is running, you can open H2O Flow and monitor the model building process._\n",
    "\n",
    "Once complete, the leaderboard contains the performance metrics of the models generated by AutoML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aml.leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we provided only the training H2O Frame during training, the models are sorted by their cross-validated performance metrics (AUC by default for classification). We can evaluate the best model (`leader`) on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml.leader.model_performance(test_data=test).plot()\n",
    "aml.leader.model_performance(test_data=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another convenient use of H2O Flow is to explore the various models built by AutoML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10 (of 10). Stop H2O-3 cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your work is completed, shutting down the H2O cluster frees up the resources reserved by H2O.\n",
    "\n",
    "# Bonus: H2O-3 documentation\n",
    "\n",
    "* http://docs.h2o.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
