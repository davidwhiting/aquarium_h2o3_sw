{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Washington State Cannabis Sale Forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In November 2012, Washington State voters approved to legalize marijuana through [Washington Initiative 502 (I-502)](https://sos.wa.gov/_assets/elections/initiatives/i502.pdf) by a margin of approximately [56 to 44](https://results.vote.wa.gov/results/20121106/Initiative-Measure-No-502-Concerns-marijuana_ByCounty.html). As part of this initiative, Washington State created a new agency, the [Washington State Liquor and Cannabis Board (WSLCB)](https://lcb.wa.gov), for licensing and regulating liquor and marijuana. As part of its mandate the WSLCB provides public access to [data](https://data.lcb.wa.gov) on how Washington Statesâ€™s marijuana market is performing.\n",
    "\n",
    "In this exercise, we will analyze daily sales data from 2015-11-01 to 2017-03-04 in an attempt to answer the following questions:\n",
    "\n",
    "1. How many previous weeks influence cannabis sales?\n",
    "1. What days exhibit unusual sales?\n",
    "1. Do stores in counties that voted to legalize marijuana behave differently that those that didn't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This analysis will use H2O's Sparkling Water to analyze cannabis sales data, where Spark will be used for data management and manipulation and H2O will be use for data analysis and python will be used as the client. This Jupyter notebook assumes that it was launched with a PySparkling context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create H2O Context inside Spark Cluster\n",
    "\n",
    "When using PySparkling, the first step is to create a Sparkling Water context within the Spark context, `spark`, so data can be passed back and forth between Spark and H2O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysparkling import *\n",
    "hc = H2OContext.getOrCreate(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation using Spark\n",
    "\n",
    "The next step is to use Spark to prepare the data for analysis by H2O. The particular data manipulations are:\n",
    "\n",
    "1. Read daily sales transactions\n",
    "1. Create daily sales aggregates by store\n",
    "1. Find unusual days\n",
    "1. Join county voting results for I-502 with aggregated data\n",
    "1. Collapse infrequent counties and cities for analysis, i.e. manage high cardinality categorical columsn\n",
    "1. Create lagged predictors\n",
    "1. Create train / test splits for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Daily Sales Transactions for Different Stores\n",
    "\n",
    "Reading the data involve a straight-forward use of the `spark.read.csv` function with the schema metadata. The data file contains six columns:\n",
    "\n",
    "|     | Column Name | Description |\n",
    "| --- | ----------- | ----------- |\n",
    "|  1  | SalesDate | Date of sale|\n",
    "|  2  | Organization | Organization that owns the store |\n",
    "|  3  | County | County of store location|\n",
    "|  4  | City | City of store location |\n",
    "|  5  | Sales Price | Price of line item |\n",
    "|  6  | Freq | Number of occurrences |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('SalesDate', DateType(), metadata = {'desc': 'Date of sale'}),\n",
    "                     StructField('Organization', StringType(), metadata = {'desc': 'Organization that owns the store'}),\n",
    "                     StructField('County', StringType(), metadata = {'desc': 'County of store location'}),\n",
    "                     StructField('City', StringType(), metadata = {'desc': 'City of store location'}),\n",
    "                     StructField('SalesPrice', DoubleType(), metadata = {'desc': 'Price of line item'}),\n",
    "                     StructField('Freq', IntegerType(), metadata = {'desc': 'Number of occurrences'})\n",
    "                    ])\n",
    "\n",
    "# https://s3-us-west-2.amazonaws.com/h2o-tutorials/data/topics/time_series/wa_cannabis/WA_Cannabis_Sales_Daily.csv\n",
    "raw_sales = spark.read.csv('../../data/time_series/wa_cannabis/WA_Cannabis_Sales_Daily.csv',\n",
    "                           header = True, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe numeric and string columns\n",
    "raw_sales.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional summaries\n",
    "raw_sales.select([count('*').alias('nrows'), min('SalesDate'), max('SalesDate'), countDistinct('Organization')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Daily Sales Aggregates by Store\n",
    "\n",
    "In order to analyze aggreate sales demand, the transactional sales data are aggregated three ways, each of which uses the `log(x + 1)` function to manage their inherent skewness:\n",
    "\n",
    "1. `Log1pDemandInThou = log1p(sum(store sales)/1000)`\n",
    "1. `Log1pOtherDemandInThou = log1p(sum(citywise sales)/1000) - log1p(sum(store sales)/1000)`, demand from the rest of the organizations\n",
    "1. `Log1pNumSales = log1p(sum(I(store sales > 0)))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand = raw_sales.groupBy('SalesDate', 'Organization', 'County', 'City') \\\n",
    "    .agg(log1p(sum(col('Freq') * col('SalesPrice')) / 1000).alias('Log1pDemandInThou'),\n",
    "         log1p(sum(when(col('SalesPrice') > 0, col('Freq')).otherwise(0))).alias('Log1pNumSales')) \\\n",
    "    .alias('demand')\n",
    "print(\"Number of Organization-Days: \", demand.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_demand = raw_sales.groupBy('SalesDate') \\\n",
    "    .agg(log1p(sum(col('Freq') * col('SalesPrice')) / 1000).alias('Log1pCitywideDemandInThou')) \\\n",
    "    .alias('daily_demand')\n",
    "print(\"Number of Days: \", daily_demand.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand = demand.join(daily_demand, demand.SalesDate == daily_demand.SalesDate, how = \"left_outer\") \\\n",
    "    .select('demand.*', 'daily_demand.Log1pCitywideDemandInThou')\n",
    "print(\"Number of Organization-Days: \", demand.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand = demand.select('SalesDate', 'Organization', 'County', 'City', 'Log1pDemandInThou',\n",
    "                       (col('Log1pCitywideDemandInThou') - col('Log1pDemandInThou')).alias('Log1pOtherDemandInThou'),\n",
    "                       'Log1pNumSales') \\\n",
    "         .alias('demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand.describe(['Log1pDemandInThou', 'Log1pOtherDemandInThou', 'Log1pNumSales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Unusual Days\n",
    "\n",
    "Fifteen unusual days are discovered by examining the week-over-week ratios in `Log1pCitywideDemandInThou`. Not surprisingly, these days are at or around [420](https://en.wikipedia.org/wiki/420_%28cannabis_culture%29) and the holidays Fourth of July, Thanksgiving, Christmas, and New Year's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window().orderBy(col('SalesDate'))\n",
    "plot_data = \\\n",
    "  daily_demand.select('SalesDate', 'Log1pCitywideDemandInThou',\n",
    "                      round(col('Log1pCitywideDemandInThou') / lag('Log1pCitywideDemandInThou', count = 7).over(w), 4).alias('WoW'),\n",
    "                      round(abs(col('Log1pCitywideDemandInThou') / lag('Log1pCitywideDemandInThou', count = 7).over(w) - 1), 4).alias('AbsWoWDiff')) \\\n",
    "                      .orderBy('AbsWoWDiff', ascending = False).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_data['WoW'].plot.hist(bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (20, 10))\n",
    "for dt in ['2015-11-25', '2015-12-02', '2015-12-23', '2015-12-24', '2015-12-25', '2015-12-26', '2016-01-01',\n",
    "           '2016-01-07', '2016-04-20', '2016-04-27', '2016-07-01',\n",
    "           '2016-11-23', '2016-11-30',                             '2016-12-25',               '2017-01-01']:\n",
    "    plt.axvline(x = datetime.datetime.strptime(dt, '%Y-%m-%d'), color = 'orange', linestyle='--')\n",
    "plot_data.plot(x = 'SalesDate', y = 'Log1pCitywideDemandInThou', ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand = demand.select('*',\n",
    "                       when(col('SalesDate') == '2016-04-20', 'FourTwenty')\n",
    "            .otherwise(when(col('SalesDate') == '2016-04-27', 'FourTwentySeven')\n",
    "            .otherwise(when(col('SalesDate') == '2016-07-01', 'PreJuly4th')\n",
    "            .otherwise(when(col('SalesDate') == '2015-11-25', 'ThanksgivingMinusOne')\n",
    "            .otherwise(when(col('SalesDate') == '2016-11-23', 'ThanksgivingMinusOne')\n",
    "            .otherwise(when(col('SalesDate') == '2015-12-02', 'ThanksgivingPlusSix')\n",
    "            .otherwise(when(col('SalesDate') == '2016-11-30', 'ThanksgivingPlusSix')\n",
    "            .otherwise(when(col('SalesDate') == '2015-12-23', 'ChristmasMinusTwo')\n",
    "            .otherwise(when(col('SalesDate') == '2015-12-24', 'ChristmasMinusOne')\n",
    "            .otherwise(when(col('SalesDate') == '2015-12-25', 'Christmas')\n",
    "            .otherwise(when(col('SalesDate') == '2016-12-25', 'Christmas')\n",
    "            .otherwise(when(col('SalesDate') == '2015-12-26', 'ChristmasPlusOne')\n",
    "            .otherwise(when(col('SalesDate') == '2016-01-01', 'NewYearsDay')\n",
    "            .otherwise(when(col('SalesDate') == '2017-01-01', 'NewYearsDay')\n",
    "            .otherwise(when(col('SalesDate') == '2016-01-07', 'NewYearsDayPlusSix')\n",
    "            .otherwise('N/A'))))))))))))))).alias('DayOfInterest')).alias('demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand.groupBy('DayOfInterest').count().sort('count', ascending = False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Sales Data with Washington Initiative 502 Vote to Legalize Cannabis\n",
    "\n",
    "Generally speaking counties in Western Washington voted to legalize cannabis sales, while those in Eastern Washington did not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('County', StringType(), metadata = {'desc': 'County'}),\n",
    "                     StructField('LegalizationVote', DoubleType(), metadata = {'desc': 'Fraction voting to legalize'})\n",
    "                    ])\n",
    "\n",
    "# https://s3-us-west-2.amazonaws.com/h2o-tutorials/data/topics/time_series/wa_cannabis/Initiative-Measure-No-502-Concerns-marijuana_ByCounty.csv\n",
    "legalization = spark.read.csv('../../data/topics/time_series/wa_cannabis/Initiative-Measure-No-502-Concerns-marijuana_ByCounty.csv',\n",
    "                              header = True, schema = schema).alias('legalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legalization.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legalization.sort('LegalizationVote', ascending = False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legalization.sort('LegalizationVote').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand = demand.join(legalization, demand.County == legalization.County, how = \"left_outer\") \\\n",
    "    .select('demand.*', 'legalization.LegalizationVote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapse Infrequent Counties and Cities\n",
    "\n",
    "In order to correct for high cardinality county and city features, the infrequent locations are collapsed into an `OTHER` category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand.select('Organization', 'County').distinct().groupBy('County') \\\n",
    "    .agg(count('*').alias('Freq')).orderBy('Freq', ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand = demand.withColumn('County',\n",
    "                           when(col('County') == 'KING', 'KING')\n",
    "                .otherwise(when(col('County') == 'SPOKANE', 'SPOKANE')\n",
    "                .otherwise(when(col('County') == 'SNOHOMISH', 'SNOHOMISH')\n",
    "                .otherwise(when(col('County') == 'PIERCE', 'PIERCE')\n",
    "                .otherwise(when(col('County') == 'KITSAP', 'KITSAP')\n",
    "                .otherwise(when(col('County') == 'THURSTON', 'THURSTON')\n",
    "                .otherwise(when(col('County') == 'WHATCOM', 'WHATCOM')\n",
    "                .otherwise(when(col('County') == 'CLARK', 'CLARK')\n",
    "                .otherwise('OTHER')))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand.select('Organization', 'County').distinct().groupBy('County') \\\n",
    "    .agg(count('*').alias('Freq')).orderBy('Freq', ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand.select('Organization', 'City').distinct().groupBy('City') \\\n",
    "    .agg(count('*').alias('Freq')).orderBy('Freq', ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand = demand.withColumn('City',\n",
    "                           when(col('City') == 'SEATTLE', 'SEATTLE')\n",
    "                .otherwise(when(col('City') == 'SPOKANE', 'SPOKANE')\n",
    "                .otherwise(when(col('City') == 'TACOMA', 'TACOMA')\n",
    "                .otherwise('OTHER'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand.select('Organization', 'City').distinct().groupBy('City') \\\n",
    "    .agg(count('*').alias('Freq')).orderBy('Freq', ascending = False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lagged Predictors\n",
    "\n",
    "Up to seven weeks of sales will be considered as features in the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window().partitionBy([col(x) for x in ['Organization']]).orderBy(col('SalesDate'))\n",
    "demand = demand.select('SalesDate', 'DayOfInterest', 'Organization', 'County', 'City', 'LegalizationVote',\n",
    "                       'Log1pDemandInThou',\n",
    "                       lag('Log1pDemandInThou', count = 7).over(w).alias('Log1pDemandInThou_L7'),\n",
    "                       lag('Log1pDemandInThou', count = 14).over(w).alias('Log1pDemandInThou_L14'),\n",
    "                       lag('Log1pDemandInThou', count = 21).over(w).alias('Log1pDemandInThou_L21'),\n",
    "                       lag('Log1pDemandInThou', count = 28).over(w).alias('Log1pDemandInThou_L28'),\n",
    "                       lag('Log1pDemandInThou', count = 35).over(w).alias('Log1pDemandInThou_L35'),\n",
    "                       lag('Log1pOtherDemandInThou', count = 7).over(w).alias('Log1pOtherDemandInThou_L7'),\n",
    "                       lag('Log1pNumSales', count = 7).over(w).alias('Log1pNumSales_L7'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train / Test Splits for Modeling\n",
    "\n",
    "Given the time series nature of this exercise, the train and test splits are based on time, where everything up to 2017-02-25 is in the training set and everything from 2017-02-26 onwards in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = demand.filter(demand.SalesDate <= '2017-02-25')\n",
    "test = demand.filter(demand.SalesDate >= '2017-02-26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select(countDistinct('SalesDate').alias('Number of Dates in Training Data Set')).show()\n",
    "test.select(countDistinct('SalesDate').alias('Number of Dates in Testing Data Set')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Data in H2O\n",
    "\n",
    "The steps for analyzing the data in H2O are as follows:\n",
    "1. Copy data from Spark to H2O\n",
    "1. Segment organizations into folds for cross-validation\n",
    "1. Run automatic machine learning to experiment with generalized linear models, random forests, extreme random trees, and gradient boosting machines.\n",
    "1. Answer questions using leading model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy Data from Spark to H2O\n",
    "\n",
    "First copy the training and test data inside the H2O context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "train_hf = hc.as_h2o_frame(train, \"train\")\n",
    "test_hf = hc.as_h2o_frame(train, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in ['Organization', 'County', 'City', 'DayOfInterest']:\n",
    "    train_hf[j] = train_hf[j].asfactor()\n",
    "    test_hf[j] = test_hf[j].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Organizations into Folds for Cross-Validation\n",
    "\n",
    "Then segment the training data into folds for cross-validation using the organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations = train_hf['Organization'].unique().sort(0).as_data_frame()\n",
    "organizations = organizations.rename(columns = {'C1': 'Organization'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(2307)\n",
    "organizations = organizations.assign(Fold = np.random.randint(1,6, size = organizations.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(organizations.groupby(['Fold']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations_hf = h2o.H2OFrame(organizations, 'Organization')\n",
    "organizations_hf['Organization'] = organizations_hf['Organization'].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hf = train_hf.merge(organizations_hf, all_x = True, all_y = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Automatic Machine Learning\n",
    "\n",
    "With the training and test data, we use H2O's Automatic Machine Learning to explore models of daily sales data based on generalized linear models, random forest, extreme random trees, and gradient boosting machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Predictors\n",
    "predictors = ['DayOfInterest', 'County', 'City', 'LegalizationVote',\n",
    "              'Log1pDemandInThou_L7', 'Log1pDemandInThou_L14', 'Log1pDemandInThou_L21',\n",
    "              'Log1pDemandInThou_L28', 'Log1pDemandInThou_L35',\n",
    "              'Log1pOtherDemandInThou_L7', 'Log1pNumSales_L7']\n",
    "response = 'Log1pDemandInThou'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.automl import H2OAutoML\n",
    "aml = H2OAutoML(max_models = 6, exclude_algos = ['DeepLearning'])\n",
    "aml.train(x = predictors, y = response,\n",
    "          training_frame = train_hf,\n",
    "          leaderboard_frame = test_hf,\n",
    "          fold_column = 'Fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aml.leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Questions using Leading Model\n",
    "\n",
    "We can now return to the questions that motivated this analysis:\n",
    "\n",
    "1. How many previous weeks influence cannabis sales?\n",
    "1. What days exhibit unusual sales?\n",
    "1. Do stores in counties that voted to legalize marijuana behave differently that those that didn't?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = h2o.get_model(aml.leaderboard[0,'model_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R^2: train = {:.4f}, valid = {:.4f}, xval = {:.4f}\" \\\n",
    "      .format(best_model.r2(train = True), best_model.r2(valid = True), best_model.r2(xval = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.varimp_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining demand lags\n",
    "\n",
    "The variable importance plot of the leading model as well as the partial dependency plots show that five week's worth of sales should be sufficient for forecasting cannabis sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_demand = best_model.partial_plot(data = train_hf,\n",
    "                                     cols = ['Log1pDemandInThou_L7', 'Log1pDemandInThou_L14',\n",
    "                                             'Log1pDemandInThou_L21', 'Log1pDemandInThou_L28',\n",
    "                                             'Log1pDemandInThou_L35'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_other_lagged = best_model.partial_plot(data = train_hf,\n",
    "                                           cols = ['Log1pOtherDemandInThou_L7', 'Log1pNumSales_L7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm = h2o.get_model(aml.leaderboard[7, 'model_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R^2: train = {:.4f}, valid = {:.4f}, xval = {:.4f}\" \\\n",
    "      .format(glm.r2(train = True), glm.r2(valid = True), glm.r2(xval = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in ['Log1pDemandInThou_L7', 'Log1pDemandInThou_L14', 'Log1pDemandInThou_L21', 'Log1pDemandInThou_L28',\n",
    "          'Log1pDemandInThou_L35']:\n",
    "    print(j + \": {:.4f}\".format(glm.coef()[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effects of unusual days\n",
    "\n",
    "As the time series plot suggested, the effects of unusual days, such as 420 and Christmas, are factor into the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_doi = best_model.partial_plot(data = train_hf, cols = ['DayOfInterest'], plot = False)[0].as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_doi.sort_values('mean_response', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of legalization vote\n",
    "\n",
    "Not too surprisingly, stores in different counties tend to behave similarly despite the differences in their voters' desire to legalize cannabis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_legalization = best_model.partial_plot(data = train_hf, cols = ['LegalizationVote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_cats = best_model.partial_plot(data = train_hf, cols = ['County', 'City'], plot = False)\n",
    "pdp_county = pdp_cats[0].as_data_frame()\n",
    "pdp_city = pdp_cats[1].as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_county.sort_values('mean_response', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_city.sort_values('mean_response', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown Sparkling Water Services\n",
    "\n",
    "The last step in this script is to be a good cloud citizen and shut down the H2O cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySparkling",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
