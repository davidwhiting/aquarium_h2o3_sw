{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "***\n",
    "\n",
    "Copyright 2018-2019 Lingyao Meng (danielle@h2o.ai), J. Patrick Hall (phall@h2o.ai), and the H2O.ai team. \n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "**DISCLAIMER:** This notebook is not legal compliance advice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Dependence, Individual Conditional Expectation, and Surrogate Models [FIX THIS]\n",
    "\n",
    "#### Start H2O cluster\n",
    "\n",
    "\n",
    "The `os` commands below check whether this notebook is being run on the Aquarium platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "startup = '/home/h2o/bin/aquarium_startup'\n",
    "if os.path.exists(startup):\n",
    "    os.system(startup)\n",
    "    local_url = 'http://localhost:54321/h2o'\n",
    "    aquarium = True\n",
    "    !sleep 5\n",
    "else:\n",
    "    local_url = 'http://localhost:54321'\n",
    "    aquarium = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python imports\n",
    "In general, NumPy and Pandas will be used for data manipulation purposes and H2o and XGBoost will be used for modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for handling external processes to generate PNG file of decision tree\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "# in-notebook display of decision tree\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# to generate synthetic data w/ known interactions\n",
    "from data_maker_and_getter import DataMakerAndGetter\n",
    "\n",
    "import h2o                                                        # Python API for h2o library and server \n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator # h2o, for single tree\n",
    "from h2o.backend import H2OLocalServer                            # h2o, for plotting local tree in-notebook\n",
    "\n",
    "import matplotlib.pyplot as plt                                   # basic plotting\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np                                                # basic math and array and matrix handling \n",
    "import pandas as pd                                               # basic Dataframe handling\n",
    "import xgboost as xgb                                             # for training gradient boosting machines \n",
    "\n",
    "# ignore irrelevant warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start h2o\n",
    "H2o is both a library and a server. The machine learning algorithms in the library take advantage of the multithreaded and distributed architecture provided by the server to train machine learning algorithms extremely efficiently. The API for the library was imported above in cell 2, but the server still needs to be started.\n",
    "\n",
    ">The parameters used in `h2o.init` will depend on your specific environment. Regardless of how H2O is installed, if you start a cluster, you will need to ensure that it is shut down when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321/h2o ."
     ]
    },
    {
     "ename": "H2OResponseError",
     "evalue": "Server error water.exceptions.H2ONotFoundArgumentException:\n  Error: Resource /h2o/3/Cloud not found\n  Request: GET /3/Cloud\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mH2OResponseError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6a3de92bd851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_mem_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2G'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# remove any existing data structures from h2o memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/miniconda3/envs/h2o/lib/python3.6/site-packages/h2o/h2o.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(url, ip, port, name, https, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                                      \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                                      _msgs=(\"Checking whether there is an H2O instance running at {url} \",\n\u001b[0;32m--> 265\u001b[0;31m                                             \"connected.\", \"not found.\"))\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mH2OConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# Backward compatibility: in init() port parameter really meant \"baseport\" when starting a local server...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/miniconda3/envs/h2o/lib/python3.6/site-packages/h2o/backend/connection.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(server, url, ip, port, name, https, auth, verify_ssl_certificates, proxy, cookies, verbose, _msgs)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m             \u001b[0;31m# If a server is unable to respond within 1s, it should be considered a bug. However we disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;31m# setting for now, for no good reason other than to ignore all those bugs :(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/miniconda3/envs/h2o/lib/python3.6/site-packages/h2o/backend/connection.py\u001b[0m in \u001b[0;36m_test_connection\u001b[0;34m(self, max_retries, messages)\u001b[0m\n\u001b[1;32m    568\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mH2OServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Local server was unable to start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m                 \u001b[0mcld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET /3/Cloud\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/miniconda3/envs/h2o/lib/python3.6/site-packages/h2o/backend/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, endpoint, data, json, filename, save_to)\u001b[0m\n\u001b[1;32m    405\u001b[0m                                     auth=self._auth, verify=self._verify_ssl_cert, proxies=self._proxies)\n\u001b[1;32m    406\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_end_transaction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/miniconda3/envs/h2o/lib/python3.6/site-packages/h2o/backend/connection.py\u001b[0m in \u001b[0;36m_process_response\u001b[0;34m(response, save_to)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;31m# Client errors (400 = \"Bad Request\", 404 = \"Not Found\", 412 = \"Precondition Failed\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m412\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH2OErrorV3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH2OModelBuilderErrorV3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mH2OResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;31m# Server errors (notably 500 = \"Server Error\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mH2OResponseError\u001b[0m: Server error water.exceptions.H2ONotFoundArgumentException:\n  Error: Resource /h2o/3/Cloud not found\n  Request: GET /3/Cloud\n"
     ]
    }
   ],
   "source": [
    "h2o.init(url=local_url, max_mem_size='2G')\n",
    "h2o.remove_all()    # remove any existing data structures from h2o memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Dataset with Known Signal Generating Functions\n",
    "\n",
    "Create dataset with a known signal generating function with noise: \n",
    "\n",
    "$$ y = x_1 * x_4 + |x_8| * (x_9)^2 + e $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation and XGBoost Training\n",
    "\n",
    "Generate synthetic data with:\n",
    "* 200,000 rows\n",
    "* Binary target\n",
    "* A single, known signal-generating function (see above)\n",
    "* With noise via label switching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ = DataMakerAndGetter(nrows=200000, target='binary', one_function=True, noise=True)\n",
    "rson = ds_.make_random_with_signal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram of important variable in generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rson_pd = rson.as_data_frame()\n",
    "_ = rson_pd['num9'].plot(kind='hist', bins=30, title='Histogram: num9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign modeling roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 'target'\n",
    "X = [name for name in rson.columns if name not in [y,'row_id','function','cat1','cat2','cat3']]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rson[y] = rson[y].asfactor()\n",
    "train, valid, _ = rson.split_frame([0.4, 0.3], seed = 12345)\n",
    "print(train.shape)\n",
    "print(valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert h2o frame into Pandas Dataframe for various activites below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsontrain_pd = train.as_data_frame()\n",
    "rsonvalid_pd = valid.as_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Pandas Dataframes into XGBoost DMatrices (LightSVM format), required for training XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsontrain_dm = xgb.DMatrix(rsontrain_pd[X],\n",
    "                           rsontrain_pd[y])\n",
    "rsonvalid_dm = xgb.DMatrix(rsonvalid_pd[X],\n",
    "                           rsonvalid_pd[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use average of y as XGBoost null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ave_y = rsontrain_pd['target'].mean()\n",
    "print(ave_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train XGBoost GBM\n",
    "Hyperparameters selected by Cartesian grid search: https://gist.github.com/jphall663/705595e3bc72e8fdfee8fa56220503a5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "     'base_score': ave_y,\n",
    "     'booster': 'gbtree',\n",
    "     'colsample_bytree': 0.9,\n",
    "     'eta': 0.01,\n",
    "     'eval_metric': 'auc',\n",
    "     'max_depth': 12,\n",
    "     'nthread': 4,\n",
    "     'objective': 'binary:logistic',\n",
    "     'reg_alpha': 0.001,\n",
    "     'reg_lambda': 0.01,\n",
    "     'seed': 12345,\n",
    "     'silent': 0,\n",
    "     'subsample': 0.1}\n",
    "\n",
    "watchlist = [(rsontrain_dm, 'train'), (rsonvalid_dm, 'eval')]\n",
    "\n",
    "rson_model = xgb.train(params, \n",
    "                       rsontrain_dm, \n",
    "                       400,\n",
    "                       early_stopping_rounds=50,\n",
    "                       evals=watchlist, \n",
    "                       verbose_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and Display Partial Dependence and ICE Curves\n",
    "\n",
    "#### Function for calculating partial dependence and ICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def par_dep(xs, frame, model, resolution=20, bins=None):\n",
    "    \n",
    "    \"\"\" Creates Pandas DataFrame containing partial dependence for a \n",
    "        single variable.\n",
    "    \n",
    "    Args:\n",
    "        xs: Variable for which to calculate partial dependence.\n",
    "        frame: Pandas DataFrame for which to calculate partial dependence.\n",
    "        model: XGBoost model for which to calculate partial dependence.\n",
    "        resolution: The number of points across the domain of xs for which \n",
    "                    to calculate partial dependence, default 20.\n",
    "        bins: List of values at which to set xs, default 20 equally-spaced \n",
    "              points between column minimum and maximum.\n",
    "    \n",
    "    Returns:\n",
    "        Pandas DataFrame containing partial dependence values.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # turn off pesky Pandas copy warning\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    # initialize empty Pandas DataFrame with correct column names\n",
    "    par_dep_frame = pd.DataFrame(columns=[xs, 'partial_dependence'])\n",
    "    \n",
    "    # cache original column values \n",
    "    col_cache = frame.loc[:, xs].copy(deep=True)\n",
    "  \n",
    "    # determine values at which to calculate partial dependence\n",
    "    if bins == None:\n",
    "        min_ = frame[xs].min()\n",
    "        max_ = frame[xs].max()\n",
    "        by = (max_ - min_)/resolution\n",
    "        bins = np.arange(min_, max_, by)\n",
    "        \n",
    "    # calculate partial dependence  \n",
    "    # by setting column of interest to constant \n",
    "    # and scoring the altered data and taking the mean of the predictions\n",
    "    for j in bins:\n",
    "        frame.loc[:, xs] = j\n",
    "        dframe = xgb.DMatrix(frame)\n",
    "        par_dep_i = pd.DataFrame(model.predict(dframe))\n",
    "        par_dep_j = par_dep_i.mean()[0]\n",
    "        par_dep_frame = par_dep_frame.append({xs:j,\n",
    "                                              'partial_dependence': par_dep_j}, \n",
    "                                              ignore_index=True)\n",
    "        \n",
    "    # return input frame to original cached state    \n",
    "    frame.loc[:, xs] = col_cache\n",
    "\n",
    "    return par_dep_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate partial dependence for the most important input variables in the GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_dep_num9 = par_dep('num9', rsonvalid_pd[X], rson_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display partial dependence for important variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_dep_num9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bind XGBoost predictions to training data and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rson_preds = pd.DataFrame(rson_model.predict(rsonvalid_dm))\n",
    "rson_decile_frame = pd.concat([rsonvalid_pd, rson_preds], axis=1)\n",
    "rson_decile_frame = rson_decile_frame.rename(columns={0: 'predict'})\n",
    "rson_decile_hframe = h2o.H2OFrame(rson_decile_frame)\n",
    "rson_decile_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find percentiles of XGBoost predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rson_percentile_dict = ds_.get_percentile_dict('predict', 'row_id', rson_decile_hframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display percentiles with row identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rson_percentile_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate ICE curve values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreive bins from original partial dependence calculation\n",
    "bins_num9 = list(par_dep_num9['num9'])\n",
    "\n",
    "# for each percentile in percentile_dict\n",
    "# create a new column in the par_dep frame \n",
    "# representing the ICE curve for that percentile\n",
    "# and the variables of interest\n",
    "for i in sorted(rson_percentile_dict.keys()):\n",
    "    \n",
    "    col_name = 'Percentile_' + str(i)\n",
    "    \n",
    "    # ICE curves for num11 across percentiles at bins_num11 intervals\n",
    "    par_dep_num9[col_name] = par_dep('num9', \n",
    "                                     rsonvalid_pd[rsonvalid_pd['row_id'] == int(rson_percentile_dict[i])][X], \n",
    "                                     rson_model, \n",
    "                                     bins=bins_num9)['partial_dependence']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display partial dependence and ICE for num9 -- all calculated DIRECTLY from the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_dep_num9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot partial dependence and ICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_par_dep_ICE(xs, par_dep_frame):\n",
    "\n",
    "    \n",
    "    \"\"\" Plots ICE overlayed onto partial dependence for a single variable.\n",
    "    \n",
    "    Args: \n",
    "        xs: Name of variable for which to plot ICE and partial dependence.\n",
    "        par_dep_frame: Name of Pandas DataFrame containing ICE and partial\n",
    "                       dependence values.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # plot ICE curves\n",
    "    par_dep_frame.drop('partial_dependence', axis=1).plot(x=xs, \n",
    "                                                          colormap='gnuplot',\n",
    "                                                          ax=ax)\n",
    "\n",
    "    # overlay partial dependence, annotate plot\n",
    "    par_dep_frame.plot(title='Partial Dependence and ICE for ' + str(xs),\n",
    "                       x=xs, \n",
    "                       y='partial_dependence',\n",
    "                       style='r-', \n",
    "                       linewidth=3, \n",
    "                       ax=ax)\n",
    "\n",
    "    # add legend\n",
    "    _ = plt.legend(bbox_to_anchor=(1.05, 0),\n",
    "                   loc=3, \n",
    "                   borderaxespad=0.)\n",
    "\n",
    "plot_par_dep_ICE('num9', par_dep_num9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the divergence of the ICE curves from the red partial dependence curve for `~-1 < num9 < ~1`. This divergence can be indicative of an interaction between input variables. The surrogate tree at the bottom of the notebook can provide further insight into which input variables are driving the detected interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Display Surrogate Decision Tree\n",
    "\n",
    "#### Train single h2o decision tree as surrogate between the XGBoost predictions and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'dt_surrogate_mojo' # gives MOJO artifact a recognizable name\n",
    "\n",
    "# initialize single tree surrogate model\n",
    "surrogate = H2ORandomForestEstimator(ntrees=1,          # use only one tree\n",
    "                                     sample_rate=1,     # use all rows in that tree\n",
    "                                     mtries=-2,         # use all columns in that tree\n",
    "                                     max_depth=5,       # shallow trees are easier to understand\n",
    "                                     seed=12345,        # random seed for reproducibility\n",
    "                                     model_id=model_id) # gives MOJO artifact a recognizable name\n",
    "\n",
    "# train single tree surrogate model\n",
    "surrogate.train(x=X, y='predict', training_frame=rson_decile_hframe)\n",
    "\n",
    "# persist MOJO (compiled, representation of trained model)\n",
    "# from which to generate plot of surrogate\n",
    "mojo_path = surrogate.download_mojo(path='.')\n",
    "print('Generated MOJO path:\\n', mojo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create GraphViz dot file of tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title for plot\n",
    "title = 'Known Signal Data (with Validation) Decision Tree Surrogate'  \n",
    "\n",
    "# locate h2o jar\n",
    "hs = H2OLocalServer()\n",
    "h2o_jar_path = hs._find_jar()\n",
    "print('Discovered H2O jar path:\\n', h2o_jar_path)\n",
    "\n",
    "# construct command line call to generate graphviz version of \n",
    "# surrogate tree see for more information: \n",
    "# http://docs.h2o.ai/h2o/latest-stable/h2o-genmodel/javadoc/index.html\n",
    "gv_file_name = model_id + '.gv'\n",
    "gv_args = str('-cp ' + h2o_jar_path +\n",
    "              ' hex.genmodel.tools.PrintMojo --tree 0 -i '\n",
    "              + mojo_path + ' -o').split()\n",
    "gv_args.insert(0, 'java')\n",
    "gv_args.append(gv_file_name)\n",
    "if title is not None:\n",
    "    gv_args = gv_args + ['--title', title]\n",
    "    \n",
    "# call \n",
    "print()\n",
    "print('Calling external process ...')\n",
    "print(' '.join(gv_args))\n",
    "_ = subprocess.call(gv_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create PNG from GraphViz dot file and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct call to generate PNG from \n",
    "# graphviz representation of the tree\n",
    "png_file_name = model_id + '.png'\n",
    "png_args = str('dot -Tpng ' + gv_file_name + ' -o ' + png_file_name)\n",
    "png_args = png_args.split()\n",
    "\n",
    "# call\n",
    "print('Calling external process ...')\n",
    "print(' '.join(png_args))\n",
    "_ = subprocess.call(png_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display surrogate tree in-notebook\n",
    "Double click to zoom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image((png_file_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the decision tree above, there are parent-child node relationships between num9 and several other input variables for the range `~-1 < num9 < ~1`. These parent-child relationships can be indicative of the variables driving the interactions detected in the partial dependence and ICE plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shutdown H2O\n",
    "After using h2o, it's typically best to shut it down. However, before doing so, users should ensure that they have saved any h2o data structures, such as models and H2OFrames, or scoring artifacts, such as POJOs and MOJOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be careful, this can erase your work!\n",
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: In this notebook, direct and indirect model explanation techniques were combined to gain insights into the behavior of a complex ML model. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
