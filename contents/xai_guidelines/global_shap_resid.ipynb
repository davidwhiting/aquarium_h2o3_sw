{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License \n",
    "\n",
    "Copyright 2017 - 2019 Patrick Hall, Navdeep Gill, and the H2O.ai team\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "**DISCLAIMER:** This notebook is not legal compliance advice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Shapley Values and Residual Analysis\n",
    "\n",
    "#### Start H2O cluster\n",
    "\n",
    "\n",
    "The `os` commands below check whether this notebook is being run on the Aquarium platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "startup = '/home/h2o/bin/aquarium_startup'\n",
    "if os.path.exists(startup):\n",
    "    os.system(startup)\n",
    "    local_url = 'http://localhost:54321/h2o'\n",
    "    aquarium = True\n",
    "    !sleep 5\n",
    "else:\n",
    "    local_url = 'http://localhost:54321'\n",
    "    aquarium = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python imports\n",
    "In general, NumPy and Pandas will be used for data manipulation purposes and h2o will be used for modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2o Python API with specific classes\n",
    "import h2o \n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "\n",
    "import numpy as np   # array, vector, matrix calculations\n",
    "import pandas as pd  # DataFrame handling\n",
    "\n",
    "pd.options.display.max_columns = 999 # enable display of all columns in notebook\n",
    "\n",
    "# plotting functionality\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import shap # Python Shapley value package\n",
    "\n",
    "# display plots in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start h2o\n",
    "H2o is both a library and a server. The machine learning algorithms in the library take advantage of the multithreaded and distributed architecture provided by the server to train machine learning algorithms extremely efficiently. The API for the library was imported above in cell 2, but the server still needs to be started.\n",
    "\n",
    ">The parameters used in `h2o.init` will depend on your specific environment. Regardless of how H2O is installed, if you start a cluster, you will need to ensure that it is shut down when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init(url=local_url, max_mem_size='2G')\n",
    "h2o.remove_all()    # remove any existing data structures from h2o memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, explore, and prepare UCI credit card default data\n",
    "\n",
    "UCI credit card default data: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
    "\n",
    "The UCI credit card default data contains demographic and payment information about credit card customers in Taiwan in the year 2005. The data set contains 23 input variables: \n",
    "\n",
    "* **`LIMIT_BAL`**: Amount of given credit (NT dollar)\n",
    "* **`SEX`**: 1 = male; 2 = female\n",
    "* **`EDUCATION`**: 1 = graduate school; 2 = university; 3 = high school; 4 = others \n",
    "* **`MARRIAGE`**: 1 = married; 2 = single; 3 = others\n",
    "* **`AGE`**: Age in years \n",
    "* **`PAY_0`, `PAY_2` - `PAY_6`**: History of past payment; `PAY_0` = the repayment status in September, 2005; `PAY_2` = the repayment status in August, 2005; ...; `PAY_6` = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; ...; 8 = payment delay for eight months; 9 = payment delay for nine months and above. \n",
    "* **`BILL_AMT1` - `BILL_AMT6`**: Amount of bill statement (NT dollar). `BILL_AMNT1` = amount of bill statement in September, 2005; `BILL_AMT2` = amount of bill statement in August, 2005; ...; `BILL_AMT6` = amount of bill statement in April, 2005. \n",
    "* **`PAY_AMT1` - `PAY_AMT6`**: Amount of previous payment (NT dollar). `PAY_AMT1` = amount paid in September, 2005; `PAY_AMT2` = amount paid in August, 2005; ...; `PAY_AMT6` = amount paid in April, 2005. \n",
    "\n",
    "The shorthand name y is assigned to the prediction target. X is assigned to all other input variables in the credit card default data except the row indentifier, ID, and the demographic variables are dropped from the analysis.\n",
    "\n",
    "Because h2o accepts both numeric and character inputs, some variables will be recoded into more transparent character values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import XLS file\n",
    "path = 'default_of_credit_card_clients.xls'\n",
    "data = pd.read_excel(path,\n",
    "                     skiprows=1)\n",
    "\n",
    "# remove spaces from target column name \n",
    "data = data.rename(columns={'default payment next month': 'DEFAULT_NEXT_MONTH'}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign modeling roles\n",
    "The shorthand name y is assigned to the prediction target. X is assigned to all other input variables in the credit card default data except the row indentifier, ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign target and inputs for GBM\n",
    "y = 'DEFAULT_NEXT_MONTH'\n",
    "X = [name for name in data.columns if name not in [y, 'ID', 'AGE', 'EDUCATION', 'SEX', 'MARRIAGE', 'LIMIT_BAL']]\n",
    "print('y =', y)\n",
    "print('X =', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function for recoding values in the UCI credict card default data\n",
    "This simple function maps longer, more understandable character string values from the UCI credit card default data dictionary to the original integer values of the input variables found in the dataset. These character values can be used directly in h2o decision tree models, and the function returns the original Pandas DataFrame as an h2o object, an H2OFrame. H2o models cannot run on Pandas DataFrames. They require H2OFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_cc_data(frame):\n",
    "    \n",
    "    \"\"\" Recodes numeric categorical variables into categorical character variables\n",
    "    with more transparent values. \n",
    "    \n",
    "    Args:\n",
    "        frame: Pandas DataFrame version of UCI credit card default data.\n",
    "        \n",
    "    Returns: \n",
    "        H2OFrame with recoded values.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # define recoded values\n",
    "    sex_dict = {1:'male', 2:'female'}\n",
    "    education_dict = {0:'other', 1:'graduate school', 2:'university', 3:'high school', \n",
    "                      4:'other', 5:'other', 6:'other'}\n",
    "    marriage_dict = {0:'other', 1:'married', 2:'single', 3:'divorced'}\n",
    "    pay_dict = {-2:'no consumption (-2)', -1:'pay duly (-1)', 0:'use of revolving credit (0)', 1:'1 month delay', \n",
    "                2:'2 month delay', 3:'3 month delay', 4:'4 month delay', 5:'5 month delay', 6:'6 month delay', \n",
    "                7:'7 month delay', 8:'8 month delay', 9:'9+ month delay'}\n",
    "    \n",
    "    # recode values using Pandas apply() and anonymous function\n",
    "    frame['SEX'] = frame['SEX'].apply(lambda i: sex_dict[i])\n",
    "    frame['EDUCATION'] = frame['EDUCATION'].apply(lambda i: education_dict[i])    \n",
    "    frame['MARRIAGE'] = frame['MARRIAGE'].apply(lambda i: marriage_dict[i]) \n",
    "    for name in frame.columns:\n",
    "        if name in ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']:\n",
    "            frame[name] = frame[name].apply(lambda i: pay_dict[i])            \n",
    "                \n",
    "    return h2o.H2OFrame(frame)\n",
    "\n",
    "data = recode_cc_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure target is handled as a categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[y] = data[y].asfactor() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an H2O GBM classifier\n",
    "\n",
    "#### Split data into training and test sets for early stopping\n",
    "The credit card default data is split into training and test sets to monitor and prevent overtraining. Reproducibility is also important factor in creating trustworthy models, and randomly splitting datasets can introduce randomness in model predictions and other results. A random seed is used here to ensure the data split is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and validation\n",
    "train, test = data.split_frame([0.7], seed=12345)\n",
    "\n",
    "# summarize split\n",
    "print('Train data rows = %d, columns = %d' % (train.shape[0], train.shape[1]))\n",
    "print('Test data rows = %d, columns = %d' % (test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train h2o an unconstrained GBM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize GBM model\n",
    "model = H2OGradientBoostingEstimator(ntrees=150,            # maximum 150 trees in GBM\n",
    "                                     max_depth=4,           # trees can have maximum depth of 4\n",
    "                                     sample_rate=0.9,       # use 90% of rows in each iteration (tree)\n",
    "                                     col_sample_rate=0.9,   # use 90% of variables in each iteration (tree)\n",
    "                                     stopping_rounds=5,     # stop if validation error does not decrease for 5 iterations (trees)\n",
    "                                     seed=12345)            # for reproducibility\n",
    "\n",
    "# train a GBM model\n",
    "model.train(y=y, x=X, training_frame=train, validation_frame=test)\n",
    "\n",
    "# print AUC\n",
    "print('GBM Test AUC = %.4f' % model.auc(valid=True))\n",
    "\n",
    "# uncomment to see model details\n",
    "# print(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Shapley variable importance\n",
    "Shapley values are a locally-accurate and globally consistent variable importance metric. Instead of relying on traditional single-value variable importance measures, local Shapley values for each input will be calculated and aggregated below to get a more holistic and consisent measurement for the global importance of each input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributions = model.predict_contributions(test)\n",
    "contributions_matrix = contributions.as_data_frame().as_matrix()\n",
    "shap_values = contributions_matrix[:,:-1]\n",
    "shap.summary_plot(shap_values, X, plot_type='bar', color='royalblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct residual analysis to debug model\n",
    "Residuals refer to the difference between the recorded value of a dependent variable and the predicted value of a dependent variable for every row in a data set. Plotting the residual values against the predicted values is a time-honored model assessment technique and a great way to see all your modeling results in two dimensions.\n",
    "\n",
    "#### Bind model predictions onto test data \n",
    "To calculate the residuals for our GBM model, first the model predictions are merged onto the test set. The test data is used here to see how the model behaves on holdout data, which should be closer to its behavior on new data than analyzing residuals for the training inputs and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = 'p_DEFAULT_NEXT_MONTH'\n",
    "preds1 = model.predict(test).drop(['predict', 'p0'])\n",
    "preds1.columns = [yhat]\n",
    "test_yhat = test.cbind(preds1[yhat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate deviance residuals for binomial classification\n",
    "For binomial classification, deviance residuals are related to the logloss cost function. Like analyzing $y - \\hat{y}$ for linear regression, these residuals are related to the quantities that the GBM sought to minimize. Deviance residual values are calculated by applying the formula in the cell directly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Pandas for adding columns and plotting\n",
    "test_yhat = test_yhat.as_data_frame()\n",
    "test_yhat['s'] = 1\n",
    "test_yhat.loc[test_yhat['DEFAULT_NEXT_MONTH'] == 0, 's'] = -1\n",
    "test_yhat['r_DEFAULT_NEXT_MONTH'] = test_yhat['s'] * np.sqrt(-2*(test_yhat[y]*np.log(test_yhat[yhat]) +\n",
    "                                                                 ((1 - test_yhat[y])*np.log(1 - test_yhat[yhat]))))\n",
    "test_yhat = test_yhat.drop('s', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort data by residuals and display data and residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yhat = test_yhat.sort_values(by='r_DEFAULT_NEXT_MONTH', ascending=False).reset_index(drop=True)\n",
    "test_yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot residuals by most important input variable \n",
    "Plotting residuals is a model debugging and diagnostic tool that enables users to see modeling results, and any anomolies, in a single two-dimensional plot. Here the green points represent customers who defaulted, and the blue points represent customers who did not. A few potential outliers are visible. There appear to be several cases in the test data with relatively large negative residuals. Understanding and addressing the factors that cause these outliers could lead to a more acccurate model.\n",
    "\n",
    "Residuals can also be plotted for important input variables to understand how the values of a single input variable affect prediction errors. When plotted by PAY_0, the residuals confirm that the GBM is struggling to accurately predict cases where default status is not correlated with recent payment behavior in an obvious way. The residual plots for values of PAY_0 indicating timely payment behavior (e.g., use of revolving credit, pay duly, and no consumption) generally display the highest positive residuals and relatively small negative residuals. Residuals for the other values of PAY_0, those that represent late recent payments, tend to show large negative residuals and relatively small positive residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort for better organization in plot\n",
    "sorted_ = test_yhat.sort_values(by='PAY_0')\n",
    "sorted_.rename(columns={yhat: 'Predicted', \n",
    "                        'r_DEFAULT_NEXT_MONTH': 'Residual',\n",
    "                        y: 'Default'}, \n",
    "               inplace=True)\n",
    "\n",
    "# use Seaborn FacetGrid for convenience\n",
    "sns.set(font_scale=1.2) \n",
    "sns.set_style('whitegrid', {'axes.grid': False})\n",
    "sns.set_palette((sns.diverging_palette(255, 133, l=60, n=2, center='dark')))\n",
    "g = sns.FacetGrid(sorted_, col='PAY_0', hue='Default', col_wrap=4)\n",
    "_ = g.map(plt.scatter, 'Predicted', 'Residual', alpha=0.4)\n",
    "_ = g.add_legend() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shutdown H2O\n",
    "After using h2o, it's typically best to shut it down. However, before doing so, users should ensure that they have saved any h2o data structures, such as models and H2OFrames, or scoring artifacts, such as POJOs and MOJOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be careful, this can erase your work!\n",
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: The unconstrained GBM in this notebook makes predictable mistakes, that are plainly obvious after applying straightforward explanation and model debugging techniques. This model is explainable, but not trustworthy. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
