{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License \n",
    "\n",
    "Copyright 2017 - 2019 Patrick Hall and the H2O.ai team\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "**DISCLAIMER:** This notebook is not legal compliance advice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increase Transparency and Accountability in Your Machine Learning Project with Python and H2O\n",
    "#### Explain your complex models with decision tree surrogates, GBM feature importance, and reason codes\n",
    "\n",
    "Decision trees and decision tree ensembles are some of the most popular machine learning models used in commercial practice. They can train and make predictions on data containing character values and missing values - both common in large commercial data stores. Single decision trees are easily represented as directed graphs, which can drastically increase their interpretability and transparency. Decision tree ensembles (i.e., random forests and gradient boosting machines (GBMs)), can be used to increase the accuracy and stability of single decision tree models, but are far less intepretable than single trees. These characteristics of decision trees will be leveraged here to increase transparency and accountability in complex, nonlinear, machine learning models.\n",
    "\n",
    "This notebook starts by training a GBM on the UCI credit card default data using the popular open source library, h2o. A single decision tree *surrogate* model will then be trained on the original UCI credit card default data and the predictions from the h2o GBM, to create an approximate flow chart for the GBM's global decision-making processes. A technique known as leave-one-covariate/column-out (LOCO) will then be used to generate local explanations for any row-wise prediction made by the GBM model. Finally, local explanations are ensembled together from multiple similar models to increase explanation stability. \n",
    "\n",
    "**Note**: As of the h2o 3.24 \"Yates\" release, Shapley values are supported in h2o and LOCO is no longer recommended. To see Shapley values for an h2o GBM in action please see: https://github.com/jphall663/interpretable_machine_learning_with_python/blob/master/dia.ipynb.\n",
    "\n",
    "#### Start H2O cluster\n",
    "\n",
    ">_**Note**: The `os.system` command below is used solely for the H2O Aquarium training platform._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('/home/h2o/bin/startup')\n",
    "!sleep 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python imports\n",
    "In general, NumPy and Pandas will be used for data manipulation purposes and h2o will be used for modeling tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# h2o Python API with specific classes\n",
    "import h2o                                        \n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator       # for GBM\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator # for single tree\n",
    "from h2o.backend import H2OLocalServer                            # for plotting local tree in-notebook\n",
    "\n",
    "import numpy as np   # array, vector, matrix calculations\n",
    "import pandas as pd  # DataFrame handling\n",
    "\n",
    "# system packages for calling external graphviz processes\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "# in-notebook display\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start h2o\n",
    "H2o is both a library and a server. The machine learning algorithms in the library take advantage of the multithreaded and distributed architecture provided by the server to train machine learning algorithms extremely efficiently. The API for the library was imported above in cell 2, but the server still needs to be started.\n",
    "\n",
    ">The parameters used in `h2o.init` will depend on your specific environment. In particular, the\n",
    ">```\n",
    ">url='http://localhost:54321/h2o'\n",
    ">```\n",
    ">command below is required only in aquarium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init(url='http://localhost:54321/h2o', max_mem_size='2G')\n",
    "h2o.remove_all()    # remove any existing data structures from h2o memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download, explore, and prepare UCI credit card default data\n",
    "\n",
    "UCI credit card default data: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
    "\n",
    "The UCI credit card default data contains demographic and payment information about credit card customers in Taiwan in the year 2005. The data set contains 23 input variables: \n",
    "\n",
    "* **`LIMIT_BAL`**: Amount of given credit (NT dollar)\n",
    "* **`SEX`**: 1 = male; 2 = female\n",
    "* **`EDUCATION`**: 1 = graduate school; 2 = university; 3 = high school; 4 = others \n",
    "* **`MARRIAGE`**: 1 = married; 2 = single; 3 = others\n",
    "* **`AGE`**: Age in years \n",
    "* **`PAY_0`, `PAY_2` - `PAY_6`**: History of past payment; `PAY_0` = the repayment status in September, 2005; `PAY_2` = the repayment status in August, 2005; ...; `PAY_6` = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; ...; 8 = payment delay for eight months; 9 = payment delay for nine months and above. \n",
    "* **`BILL_AMT1` - `BILL_AMT6`**: Amount of bill statement (NT dollar). `BILL_AMNT1` = amount of bill statement in September, 2005; `BILL_AMT2` = amount of bill statement in August, 2005; ...; `BILL_AMT6` = amount of bill statement in April, 2005. \n",
    "* **`PAY_AMT1` - `PAY_AMT6`**: Amount of previous payment (NT dollar). `PAY_AMT1` = amount paid in September, 2005; `PAY_AMT2` = amount paid in August, 2005; ...; `PAY_AMT6` = amount paid in April, 2005. \n",
    "\n",
    "These 23 input variables are used to predict the target variable, whether or not a customer defaulted on their credit card bill in late 2005.\n",
    "\n",
    "Because h2o accepts both numeric and character inputs, some variables will be recoded into more transparent character values.\n",
    "\n",
    "#### Import data and clean\n",
    "The credit card default data is available as an `.xls` file. Pandas reads `.xls` files automatically, so it's used to load the credit card default data and give the prediction target a shorter name: `DEFAULT_NEXT_MONTH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import XLS file\n",
    "path = 'default_of_credit_card_clients.xls'\n",
    "data = pd.read_excel(path,\n",
    "                     skiprows=1)\n",
    "\n",
    "# remove spaces from target column name \n",
    "data = data.rename(columns={'default payment next month': 'DEFAULT_NEXT_MONTH'}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign modeling roles\n",
    "The shorthand name `y` is assigned to the prediction target. `X` is assigned to all other input variables in the credit card default data except the row indentifier, `ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign target and inputs for GBM\n",
    "y = 'DEFAULT_NEXT_MONTH'\n",
    "X = [name for name in data.columns if name not in [y, 'ID']]\n",
    "print('y =', y)\n",
    "print('X =', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function for recoding values in the UCI credict card default data\n",
    "This simple function maps longer, more understandable character string values from the UCI credit card default data dictionary to the original integer values of the input variables found in the dataset. These character values can be used directly in h2o decision tree models, and the function returns the original Pandas DataFrame as an h2o object, an H2OFrame. H2o models cannot run on Pandas DataFrames. They require H2OFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_cc_data(frame):\n",
    "    \n",
    "    \"\"\" Recodes numeric categorical variables into categorical character variables\n",
    "    with more transparent values. \n",
    "    \n",
    "    Args:\n",
    "        frame: Pandas DataFrame version of UCI credit card default data.\n",
    "        \n",
    "    Returns: \n",
    "        H2OFrame with recoded values.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # define recoded values\n",
    "    sex_dict = {1:'male', 2:'female'}\n",
    "    education_dict = {0:'other', 1:'graduate school', 2:'university', 3:'high school', \n",
    "                      4:'other', 5:'other', 6:'other'}\n",
    "    marriage_dict = {0:'other', 1:'married', 2:'single', 3:'divorced'}\n",
    "    pay_dict = {-2:'no consumption', -1:'pay duly', 0:'use of revolving credit', 1:'1 month delay', \n",
    "                2:'2 month delay', 3:'3 month delay', 4:'4 month delay', 5:'5 month delay', 6:'6 month delay', \n",
    "                7:'7 month delay', 8:'8 month delay', 9:'9+ month delay'}\n",
    "    \n",
    "    # recode values using Pandas apply() and anonymous function\n",
    "    frame['SEX'] = frame['SEX'].apply(lambda i: sex_dict[i])\n",
    "    frame['EDUCATION'] = frame['EDUCATION'].apply(lambda i: education_dict[i])    \n",
    "    frame['MARRIAGE'] = frame['MARRIAGE'].apply(lambda i: marriage_dict[i]) \n",
    "    for name in frame.columns:\n",
    "        if name in ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']:\n",
    "            frame[name] = frame[name].apply(lambda i: pay_dict[i])            \n",
    "                \n",
    "    return h2o.H2OFrame(frame)\n",
    "\n",
    "data = recode_cc_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure target is handled as a categorical variable\n",
    "\n",
    "In h2o, a numeric variable can be treated as numeric or categorical. The target variable `DEFAULT_NEXT_MONTH` takes on values of `0` or `1`. To ensure this numeric variable is treated as a categorical variable, the `asfactor()` function is used to explicitly declare that it is a categorical variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[y] = data[y].asfactor() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display descriptive statistics\n",
    "The h2o `describe()` function displays a brief description of the credit card default data. For the categorical input variables `LIMIT_BAL`, `SEX`, `EDUCATION`, `MARRIAGE`, and `PAY_0`-`PAY_6`, the new character values created above in cell 5 are visible. Basic descriptive statistics are displayed for numeric inputs. Also, it's easy to see there are no missing values in this dataset, which will be an important consideration for calculating LOCO values in section 5 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[X + [y]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train an H2O GBM classifier\n",
    "\n",
    "#### Split data into training and test sets for early stopping\n",
    "The credit card default data is split into training and test sets to monitor and prevent overtraining. Reproducibility is also an important factor in creating trustworthy models, and randomly splitting datasets can introduce randomness in model predictions and other results. A random seed is used here to ensure the data split is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and validation\n",
    "train, test = data.split_frame([0.7], seed=12345)\n",
    "\n",
    "# summarize split\n",
    "print('Train data rows = %d, columns = %d' % (train.shape[0], train.shape[1]))\n",
    "print('Test data rows = %d, columns = %d' % (test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train h2o GBM classifier\n",
    "Many tuning parameters must be specified to train a GBM using h2o. Typically a grid search would be performed to identify the best parameters for a given modeling task using the `H2OGridSearch` class. For brevity's sake, a previously-discovered set of good tuning parameters are specified here. Because gradient boosting methods typically resample training data, an additional random seed is also specified for the h2o GBM using the `seed` parameter to create reproducible predictions, error rates, and variable importance values. To avoid overfitting, the `stopping_rounds` parameter is used to stop the training process after the test error fails to decrease for 5 iterations.\n",
    "\n",
    "The `balance_classes` parameter ensures the positive and negative classes of the target variable are seen by the model in equal proportions during training. This can be very important for the LOCO calculations in section 5 and 6 for unbalanced data. From experiments across several data sets, explanations for rows with a majority class label for the target variable (e.g., 0) generated by LOCO are more likely to match those generated by another popular explanatory technique, LIME, when the target class is rebalanced during training. `balance_classes` is commented below because the row explained in this notebook has a minority class label (e.g., 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize GBM model\n",
    "model = H2OGradientBoostingEstimator(ntrees=150,            # maximum 150 trees in GBM\n",
    "                                     max_depth=4,           # trees can have maximum depth of 4\n",
    "                                     sample_rate=0.9,       # use 90% of rows in each iteration (tree)\n",
    "                                     col_sample_rate=0.9,   # use 90% of variables in each iteration (tree)\n",
    "                                     #balance_classes=True, # sample to balance 0/1 distribution of target - can help LOCO\n",
    "                                     stopping_rounds=5,     # stop if validation error does not decrease for 5 iterations (trees)\n",
    "                                     score_tree_interval=1, # for reproducibility, set higher for bigger data\n",
    "                                     seed=12345)            # for reproducibility\n",
    "\n",
    "# train a GBM model\n",
    "model.train(y=y, x=X, training_frame=train, validation_frame=test)\n",
    "\n",
    "# print AUC\n",
    "print('GBM Test AUC = %.2f' % model.auc(valid=True))\n",
    "\n",
    "# uncomment to see model details\n",
    "# print(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display variable importance\n",
    "During training, the h2o GBM aggregates the improvement in error caused by each split in each decision tree across all the decision trees in the ensemble classifier. These values are attributed to the input variable used in each split and give an indication of the contribution each input variable makes toward the model's predictions. The variable importance ranking should be parsimonious with human domain knowledge and reasonable expectations. In this case, a customer's most recent payment behavior, `PAY_0`, is by far the most important variable followed by their second most recent payment, `PAY_2`, and third most recent payment, `PAY_3`, behavior. This result is well-aligned with business practices in credit lending: people who miss their most recent payments are likely to default soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.varimp_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a decision tree surrogate model to describe GBM\n",
    "\n",
    "A surrogate model is a simple model that is used to explain a complex model. One of the original references for surrogate models is available here: https://papers.nips.cc/paper/1152-extracting-tree-structured-representations-of-trained-networks.pdf. In this example, a single decision tree will be trained on the original inputs and predictions of the h2o GBM model and the tree will be visualized using special functionality in h2o and GraphViz. The variable importance, interactions, and decision paths displayed in the directed graph of the trained decision tree surrogate model are then assumed to be indicative of the internal mechanisms of the more complex GBM model, creating an approximate, overall flowchart for the GBM. There are few mathematical guarantees that the simple surrogate model is highly representative of the more complex GBM, but a recent preprint article has put forward ideas on strenghthening the theoretical relationship between surrogate models and more complex models: https://arxiv.org/pdf/1705.08504.pdf. Since surrogate models alone do not gaurantee accurate transparency, they will be used along with GBM variable importance and LOCO to build a cohesive narrative about the mechansims within the GBM. **Because many currently-available explanatory techniques are approximate, it is recommended that users employ several different explanatory techniques and trust only consisent results across techniques. Also, as of h2o 3.24, Shapley values are supported for h2o GBM. Use them instead of LOCO for any high-stakes application.**\n",
    "\n",
    "#### Create dataset for surrogate model\n",
    "\n",
    "To train a surrogate model, the predictions and original inputs of the complex model to be explained need to be in the same dataset. The test data is used here to see how the model behaves on holdout data, which should be closer to its behavior on new data than analyzing the surrogate model for the training inputs and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbind predictions to training frame\n",
    "# give them a nice name\n",
    "yhat = 'p_DEFAULT_NEXT_MONTH'\n",
    "preds1 = test['ID'].cbind(model.predict(test).drop(['predict', 'p0']))\n",
    "preds1.columns = ['ID', yhat]\n",
    "test_yhat = test.cbind(preds1[yhat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train single h2o decision tree\n",
    "A single decision tree is trained on the test  inputs and predictions. To simulate a single decision tree in h2o, the `H2ORandomForestEstimator` class is used, but only one tree is trained instead of a forest of decision trees. Setting the `mtry` parameter to `-2` tells the `H2ORandomForestEstimator` to consider all variables in all splits of a tree, instead of considering a random subset of columns. It is also recommended to set a random seed for reproducibility and to set `max_depth` to a lower number, say less than 6, so that the surrogate model will not become overly complex and hard to explain and understand. Once the tree is trained, a model optimized java object (MOJO) representation of the tree is saved. H2o provides a way to visualize the trained tree in detail using the MOJO and Graphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'dt_surrogate_mojo' # gives MOJO artifact a recognizable name\n",
    "\n",
    "# initialize single tree surrogate model\n",
    "surrogate = H2ORandomForestEstimator(ntrees=1,          # use only one tree\n",
    "                                     sample_rate=1,     # use all rows in that tree\n",
    "                                     mtries=-2,         # use all columns in that tree\n",
    "                                     max_depth=3,       # shallow trees are easier to understand\n",
    "                                     seed=12345,        # random seed for reproducibility\n",
    "                                     model_id=model_id) # gives MOJO artifact a recognizable name\n",
    "\n",
    "# train single tree surrogate model\n",
    "surrogate.train(x=X, y=yhat, training_frame=test_yhat)\n",
    "\n",
    "# persist MOJO (compiled, representation of trained model)\n",
    "# from which to generate plot of surrogate\n",
    "mojo_path = surrogate.download_mojo(path='.')\n",
    "print('Generated MOJO path:\\n', mojo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create GraphViz dot file\n",
    "GraphViz is an open source graph visualization tool. It is freely available from this url: http://www.graphviz.org/. To plot the trained decision tree surrogate model, a special h2o class, `PrintMojo`, is executed against the MOJO to create a GraphViz dot file representation of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title for plot\n",
    "title = 'Credit Card Default Decision Tree Surrogate'  \n",
    "\n",
    "# locate h2o jar\n",
    "hs = H2OLocalServer()\n",
    "h2o_jar_path = hs._find_jar()\n",
    "print('Discovered H2O jar path:\\n', h2o_jar_path)\n",
    "\n",
    "# construct command line call to generate graphviz version of \n",
    "# surrogate tree see for more information: \n",
    "# http://docs.h2o.ai/h2o/latest-stable/h2o-genmodel/javadoc/index.html\n",
    "gv_file_name = model_id + '.gv'\n",
    "gv_args = str('-cp ' + h2o_jar_path +\n",
    "              ' hex.genmodel.tools.PrintMojo --tree 0 -i '\n",
    "              + mojo_path + ' -o').split()\n",
    "gv_args.insert(0, 'java')\n",
    "gv_args.append(gv_file_name)\n",
    "if title is not None:\n",
    "    gv_args = gv_args + ['--title', title]\n",
    "    \n",
    "# call \n",
    "print()\n",
    "print('Calling external process ...')\n",
    "print(' '.join(gv_args))\n",
    "_ = subprocess.call(gv_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create PNG from GraphViz dot file and display\n",
    "Then a GraphViz command line tool is used to create a static PNG image from the dot file ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct call to generate PNG from \n",
    "# graphviz representation of the tree\n",
    "png_file_name = model_id + '.png'\n",
    "png_args = str('dot -Tpng ' + gv_file_name + ' -o ' + png_file_name)\n",
    "png_args = png_args.split()\n",
    "\n",
    "# call\n",
    "print('Calling external process ...')\n",
    "print(' '.join(png_args))\n",
    "_ = subprocess.call(png_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display surrogate decision tree in notebook\n",
    "... and the image is displayed in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display in-notebook\n",
    "display(Image((png_file_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze surrogate model and compare to global GBM variable importance\n",
    "\n",
    "The displayed tree is comparable with the global GBM variable importance. A simple heuristic rule for variable importance in a decision tree relates to the depth and frequency at which a variable is split on in a tree: variables used higher in the tree and more frequently in the tree are more important. Most of the variables pictured in this tree also appear as highly important in the GBM variable importance plot. In both cases, `PAY_0` is appearing as crucially important, with other payment behavior variables following close behind. The surrogate decision tree enables users to understand and confirm not only what input variables are important, but also how their values contribute to model decisions. For instance, to fall into the lowest probability of default leaf node in the surrogate decision tree a customer must make their first and second payments in a timely fashion and then pay more than 1515.5 New Tiawanese Dollars for their fifth payment. Conversely, customers who miss their first, fifth, and third payments fall into the highest probability of default leaf node of the surrogate decision tree. It is also imperative to compare these results to domain knowledge and reasonable expectations. In this case, the global explanatory methods applied thus far tell a consisent and reasonable story about the GBM's behavior. If this was not so, steps should be taken to either reconcile or remove inconsistencies and unreasonable prediction behavior.\n",
    "\n",
    "## 5. Generate reason codes using the LOCO method \n",
    "\n",
    "Now that a solid understanding of global model behavior has been attained, local behavior for any given row of data and prediction can be analyzed and validated using LOCO. The LOCO method presented here is adapted from *Distribution-Free Predictive Inference for Regression* by Jing Lei et al., http://www.stat.cmu.edu/~ryantibs/papers/conformal.pdf. Here the local contribution of an input variable to a prediction for a single row of data is estimated by rescoring the GBM on that row one time for each input variable, each time leaving out one input variable (e.g., \"covariate\") by setting it to missing, and then subtracting the new score from the original score. By default, h2o scores missing data in decision trees by running them through the majority decision path. This means LOCO will be a numeric measure of how different the local contribution of an input variable is from the most common local contribution of that variable in the model. This variant of LOCO differs from the original method, in which one input variable is dropped from the model and the model is retrained without that variable. For nonlinear models, nonlinear dependencies can allow variables to nearly completely replace one another when a variable is dropped and the model is retrained. Hence, the approach of injecting missing values is used to estimate local contributions of input variables for nonlinear models here, as opposed to dropping a variable and retraining the model.\n",
    "\n",
    "#### Calculate LOCO reason values for each row of the test set\n",
    "To implement LOCO, GBM model predicitions are calculated once for the test data and then again for each input variable, setting the entire input variable column to missing. Once the prediction without the variable is found for every row of data in the test set, that column vector of predictions on corrupted data can be subtracted from the column vector of predictions on the original, non-corrupted data to estimate the local contribution of that variable for each prediction in the test data. For better local accuracy and explainability, LOCO contributions are scaled such that contributions for each prediction plus the overall average of `DEFAULT_NEXT_MONTH` always sum to the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.no_progress() # turn off h2o gratuitous progress bars\n",
    "\n",
    "# create set of original predictions and row ID\n",
    "preds2 = test['ID'].cbind(model.predict(test).drop(['predict', 'p0']))\n",
    "preds2.columns = ['ID', yhat]\n",
    "\n",
    "# calculate LOCO for each variable\n",
    "print('Calculating LOCO contributions ...')\n",
    "for k, i in enumerate(X):\n",
    "\n",
    "    # train and predict with x_i set to missing\n",
    "    test_loco = h2o.deep_copy(test, 'test_loco')\n",
    "    test_loco[i] = np.nan\n",
    "    preds_loco = model.predict(test_loco).drop(['predict','p0'])\n",
    "    \n",
    "    # create a new, named column for the LOCO prediction\n",
    "    preds_loco.columns = [i]\n",
    "    preds2 = preds2.cbind(preds_loco)\n",
    "    \n",
    "    # subtract the LOCO prediction from the original prediction\n",
    "    preds2[i] = preds2[yhat] - preds2[i]\n",
    "    \n",
    "    # update progress\n",
    "    print('LOCO Progress: ' + i + ' (' + str(k+1) + '/' + str(len(X)) + ') ...')\n",
    "    \n",
    "# scale contributions to sum to yhat - y_0\n",
    "print('\\nScaling contributions ...')\n",
    "\n",
    "y_0 = test[y].mean()[0]\n",
    "preds2_pd = preds2.as_data_frame()\n",
    "pred_ = preds2_pd[yhat]\n",
    "scaler = (pred_ - y_0) / preds2_pd[X].sum(axis=1)\n",
    "preds2_pd[X] = preds2_pd[X].multiply(scaler, axis=0)   \n",
    "\n",
    "print('Done.')  \n",
    "\n",
    "preds2_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numeric LOCO values in each column are an estimate of how much each variable contributed to each prediction. LOCO can indicate how a variable and its values were weighted in any given decision by the model. These values are crucially important for machine learning interpretability and are related to \"local feature importance\", \"reason codes\", or \"turn-down codes.\" The latter phrases are borrowed from credit scoring. Credit lenders in the U.S. must provide reasons for automatically rejecting a credit application. Reason codes can be easily extracted from LOCO local variable contribution values by simply ranking the variables that played the largest role in any given decision.\n",
    "\n",
    "#### Helper function for finding percentile indices\n",
    "The function below finds and returns the row indices for the minimum, the maximum, and the deciles of one column in terms of another, in this case the model predictions (`p_DEFAULT_NEXT_MONTH`) and the row identifier (`ID`), respectively. These indices are used as a starting point for finding potentially interesting predictions. Outlying predictions found through residual analysis is another group of potentially interesting local predictions to analyze with LOCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentile_dict(yhat, id_, frame):\n",
    "\n",
    "    \"\"\" Returns the minimum, maximum, and percentiles of a column, yhat, \n",
    "        as the indices based on another column id_.\n",
    "    \n",
    "    Args:\n",
    "        yhat: Column in which to find percentiles.\n",
    "        id_: Id column that stores indices for percentiles of yhat.\n",
    "        frame: H2OFrame containing yhat and id_. \n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of percentile values and index column values.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # convert to Pandas and sort \n",
    "    sort_df = preds2_pd.copy(deep=True)\n",
    "    sort_df.sort_values(yhat, inplace=True)\n",
    "    sort_df.reset_index(inplace=True)\n",
    "\n",
    "    # find top and bottom percentiles\n",
    "    percentiles_dict = {}\n",
    "    percentiles_dict[0] = sort_df.loc[0, id_]\n",
    "    percentiles_dict[99] = sort_df.loc[sort_df.shape[0]-1, id_]\n",
    "    inc = sort_df.shape[0]//10\n",
    "    \n",
    "    # find 10th-90th percentiles    \n",
    "    for i in range(1, 10):\n",
    "        percentiles_dict[i * 10] = sort_df.loc[i * inc,  id_]\n",
    "\n",
    "    return percentiles_dict\n",
    "\n",
    "# display percentiles dictionary\n",
    "# ID values for rows\n",
    "# from lowest prediction \n",
    "# to highest prediction\n",
    "percentile_dict = get_percentile_dict(yhat, 'ID', preds2_pd)\n",
    "percentile_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some reason codes for a risky customer\n",
    "Investigating customers with very high or low predicted probabilities to determine if their local explanations justify their extreme predictions is typically a productive exercise in boundary testing, model debugging, and validation. Reason codes are generated for the customer with the highest probability of default in the test data set below in cell 18, but LOCO can create local explanations for any or all rows in the training or test datasets, and on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select single customer\n",
    "# convert to Pandas\n",
    "# drop prediction and row ID\n",
    "\n",
    "risky_loco = preds2_pd[preds2_pd['ID'] == int(percentile_dict[99])].drop(['ID', yhat], axis=1)   \n",
    "\n",
    "# transpose into column vector and sort        \n",
    "risky_loco = risky_loco.T.sort_values(by=8674, ascending=False)[:5]\n",
    "\n",
    "# plot\n",
    "_ = risky_loco.plot(kind='bar', \n",
    "                    title='Top Five Reason Codes for a Risky Customer\\n', \n",
    "                    legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the customer in the test dataset that the GBM predicts as most likely to default, the most important input variables in the prediction are, in descending order, `PAY_0`, `PAY_6`, `PAY_3`, `PAY_5`, and `AGE`.\n",
    "\n",
    "#### Display customer in question \n",
    "\n",
    "The local contributions for this customer appear reasonable, especially when considering her payment information. Her most recent payment was 3 months late and her payment for 6 months previous was 4 months late, so it's logical that these would weigh heavily into the model's prediction for default for this customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yhat[test_yhat['ID'] == int(percentile_dict[99]), :] # helps understand reason codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate reason codes for the model's decision, the locally important variable and its value are used together. If this customer was denied future credit based on this model and data, the top five LOCO-based reason codes for the automated decision would be:\n",
    "\n",
    "1. Most recent payment is 3 months delayed.\n",
    "2. 6th most recent payment is 4 months delayed.\n",
    "3. 3rd most recent payment is 3 months delayed.\n",
    "4. 5th most recent payment is 2 months delayed.\n",
    "5. Customer age is 59. \n",
    "\n",
    "(Of course, in many places, variables like `AGE` and `SEX` cannot, and should not, be used in credit lending or other high-stakes decisions. For a slightly more careful treatment of GBM in a fair lending context, see: https://github.com/jphall663/interpretable_machine_learning_with_python/blob/master/dia.ipynb)\n",
    "\n",
    "## 6. Bonus: Generate ensemble LOCO reason codes for greater explanation stability\n",
    "Just like predictions from high variance, nonlinear models, *explanations* derived from machine learning models can be unstable. One general way to decrease variance is to ensemble the results of many models. The last section of this notebook puts forward a simple approach to creating ensemble explanations.\n",
    "\n",
    "#### Train multiple models\n",
    "To create ensemble explanations, several accurate models are trained. The models and their predictions on the test data are stored in Python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 10 # select number of models\n",
    "\n",
    "# lists for holding models and predictions\n",
    "models = []\n",
    "pred_frames = []\n",
    "\n",
    "for i in range(0, n_models):\n",
    "\n",
    "    # initialize and store models\n",
    "    models.append(H2OGradientBoostingEstimator(ntrees=150,\n",
    "                                               max_depth=4,\n",
    "                                               sample_rate=0.9 - ((i + 1)*0.01),     # perturb sample rate\n",
    "                                               col_sample_rate=0.9 - ((i + 1)*0.01), # perturb column sample rate\n",
    "                                               #balance_classes=True,                # sample to balance 0/1 distribution of target - helps LOCO\n",
    "                                               stopping_rounds=5,                    # stop if validation error does not decrease for 5 iterations (trees)\n",
    "                                               seed=i + 1))                          # new random seed for each model\n",
    "    \n",
    "    # train models\n",
    "    models[i].train(y=y, x=X, training_frame=train, validation_frame=test)\n",
    "    \n",
    "    # store predictions\n",
    "    pred_frames.append(test['ID'].cbind(models[i].predict(test).drop(['predict','p0'])))\n",
    "    pred_frames[i].columns = ['ID', yhat]\n",
    "    \n",
    "    # update progress\n",
    "    print('Training Progress: model %d/%d, AUC = %.4f ...' % (i + 1, n_models, models[i].auc(valid=True)))\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate LOCO for each model\n",
    "LOCO is calculated on the test data for each model, each input, and each row of data in the test set using the stored models and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each new model ...\n",
    "for k, model in enumerate(models):\n",
    "\n",
    "    # calculate LOCO for each input variable \n",
    "    for i in X:\n",
    "\n",
    "        # train and predict with Xi set to missing\n",
    "        test_loco = h2o.deep_copy(test, 'test_loco')\n",
    "        test_loco[i] = np.nan\n",
    "        preds_loco = model.predict(test_loco).drop(['predict','p0'])\n",
    "\n",
    "        # create a new, named column for the LOCO prediction\n",
    "        preds_loco.columns = [i]\n",
    "        pred_frames[k] = pred_frames[k].cbind(preds_loco)\n",
    "\n",
    "        # subtract the LOCO prediction from the original prediction\n",
    "        pred_frames[k][i] = pred_frames[k][yhat] - pred_frames[k][i]\n",
    "        \n",
    "    # update progress    \n",
    "    print('LOCO Progress: model %d/%d ...' % (k + 1, n_models))\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect LOCO values for each model for a risky customer\n",
    "To create ensemble explanations for a single row, the LOCO values for each variable in the row are averaged across all models. Single-model and mean LOCO values for the most risky person in the test set are displayed below. Notice that even slight changes in model specifications can result in different explanations. For example, the local contribution of `PAY_0` for the riskiest customer ranges from 0.13 to 0.23 across the 10 models in the table below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holds predictions for a specific row \n",
    "risky_loco_frames = []\n",
    "\n",
    "# column names for Pandas DataFrame of combined LOCO prediction\n",
    "col_names = ['Loco ' + str(i) for i in range(1, n_models + 1)]\n",
    "\n",
    "# for each new model ...\n",
    "for i in range(0, n_models):\n",
    "    \n",
    "    # collect LOCO for that model and a specific row \n",
    "    # as a column vector in a Pandas DataFrame\n",
    "    preds = pred_frames[i]\n",
    "    risky_loco_frames.append(preds[preds['ID'] == int(percentile_dict[99]), :] # row for risky person\n",
    "                             .as_data_frame()                                  # convert to Pandas\n",
    "                             .drop(['ID', yhat], axis=1)                       # drop predictions and row ID\n",
    "                             .T)                                               # Transpose into column vector\n",
    "\n",
    "# bind LOCO for each row as column vectors \n",
    "# into the same Pandas DataFrame\n",
    "loco_ensemble = pd.concat(risky_loco_frames, axis=1) \n",
    "\n",
    "# update column names\n",
    "loco_ensemble.columns = col_names\n",
    "\n",
    "# mean local importance across models\n",
    "loco_ensemble['Mean Local Importance'] = loco_ensemble.mean(axis=1)\n",
    "\n",
    "# scale contribs\n",
    "scaler = (test_yhat[test_yhat['ID'] == int(percentile_dict[99]), yhat] - y_0) /\\\n",
    "    (loco_ensemble['Mean Local Importance'].sum())\n",
    "loco_ensemble['Scaled Mean Local Importance'] = loco_ensemble['Mean Local Importance'] * scaler[0, 0]\n",
    "\n",
    "# std deviation\n",
    "loco_ensemble['Std. Dev. Local Importance'] = loco_ensemble\\\n",
    "                                              .drop('Scaled Mean Local Importance', axis=1)\\\n",
    "                                              .std(axis=1)\n",
    "        \n",
    "# display\n",
    "loco_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some mean reason codes for a risky customer\n",
    "Taking mean explanations across multiple models leads to reason codes somewhat different from the reason codes produced by a single model. Mean reason codes may be more stable, they represent explanations from several models, and they may take practicioners a step closer to using machine learning models to make inferential conclusions about phenomena represented in the training or test data, instead of simply providing an approximate explanation of a single model's decision processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risky_mean_loco = loco_ensemble['Mean Local Importance'].sort_values(ascending=False)[:5]\n",
    "_ = risky_mean_loco.plot(kind='bar', \n",
    "                         title='Top Five Reason Codes for a Risky Customer\\n', \n",
    "                         color='b',\n",
    "                         legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shutdown H2O\n",
    "After using h2o, it's typically best to shut it down. However, before doing so, users should ensure that they have saved any h2o data structures, such as models and H2OFrames, or scoring artifacts, such as POJOs and MOJOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be careful, this can erase your work!\n",
    "h2o.cluster().shutdown(prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "In this notebook, a complex GBM classifier was trained to predict credit card defaults and explained at a global scale with a decision tree surrogate model and explained at a local scale with LOCO. An ensemble LOCO approach was also introduced to stabilize approximate explanations. The decision tree surrogate creates an overall approximate flowchart for the GBM's decision processes and LOCO can be used to create reason codes for each model prediction. All of these techniques enhance the transparency of the complex model, which in turn enables greater accountability for the model's predictions. These techniques should generalize well for many types of business and research problems, enabling you to train a complex GBM model and explain it to your colleagues, bosses, and potentially, external regulators. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
