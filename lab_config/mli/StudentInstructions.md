---

This lab contains several Jupyter notebooks for training interpretable models, explaining models, and debugging models for accuracy, security, and disparate impact. All of the resources for this lab are freely available here: [https://github.com/jphall663/interpretable_machine_learning_with_python](https://github.com/jphall663/interpretable_machine_learning_with_python) and here: [https://github.com/h2oai/xai_guidelines](https://github.com/h2oai/xai_guidelines).


Once you start the lab by pressing `Start Lab`, you will need a token to open the Jupyter notebooks. That token is `h2o`.


Other related, open resources you may find interesting include: 

- The 2018 ACM FAT* tutorial, Practical Techniques for Interpreting Machine Learning Models:
Introductory Open Source Examples Using Python, H2O, and XGBoost: [https://fatconference.org/static/tutorials/hall_interpretable18.pdf](https://fatconference.org/static/tutorials/hall_interpretable18.pdf)
- The 2018 JSM presentation related to the post-hoc explanation approaches herein: [https://github.com/jphall663/jsm_2018_slides](https://github.com/jphall663/jsm_2018_slides)
- The 2019 KDD XAI Workshop paper and presentation related to the monotonic GBM and post-hoc explanation approaches herein: [https://github.com/jphall663/kdd_2019](https://github.com/jphall663/kdd_2019)
- The 2019 H2O World presentation which puts forward an interpretable machine learning workflow: [https://github.com/jphall663/hc_ml](https://github.com/jphall663/hc_ml)
- The awesome-machine-learning-interpretability metalist that includes many debugging, explanation, fairness, interpretability, privacy, and security resources: [https://github.com/jphall663/awesome-machine-learning-interpretability](https://github.com/jphall663/awesome-machine-learning-interpretability) 
- The proposed 2020 ACM FAT*  tutorial, Responsible Use Guidelines for Explainable AI: [https://github.com/h2oai/xai_guidelines](https://github.com/h2oai/xai_guidelines) 

---